{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code will load the PDF documents and separate them by pages. The content of the PDFs is stored in objects of the Document class that has two elements: page_content and metadata. Metadata is useful for storing data of the data we want to include such as page number, section, document, document summary, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add more metadata, I created a 'library' csv file where I include all the extra information I want the documents to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def extract_folder(doc):\n",
    "    path = Path(doc.metadata['source'])\n",
    "    return path.parent.name\n",
    "\n",
    "def extract_name(doc):\n",
    "    path = Path(doc.metadata['source'])\n",
    "    return path.name\n",
    "\n",
    "def input_metadata(doc, df):\n",
    "    name = extract_name(doc)\n",
    "    #print(name)\n",
    "    df_copy = df[df['file_name'] == name].reset_index(drop=True)\n",
    "    doc.metadata['field'] = df_copy.loc[0,'field']\n",
    "    doc.metadata['link'] = df_copy.loc[0,'link']\n",
    "    doc.metadata['description'] = df_copy.loc[0,'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>field</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDS0 Introduction Data Science in Context.pdf</td>\n",
       "      <td>Theory</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDS1 Ethical Foundations.pdf</td>\n",
       "      <td>Theory</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDS2 Legitimacy, values and decisions.pdf</td>\n",
       "      <td>Theory</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDS3 Fundamental Limits of ML.pdf</td>\n",
       "      <td>Theory</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDS4 Bias and Fairness I.pdf</td>\n",
       "      <td>Theory</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Microsoft_Standards.pdf</td>\n",
       "      <td>Applied</td>\n",
       "      <td>https://github.com/arturofredes/Ethical_AI_RAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_name    field  \\\n",
       "0  EDS0 Introduction Data Science in Context.pdf   Theory   \n",
       "1                   EDS1 Ethical Foundations.pdf   Theory   \n",
       "2      EDS2 Legitimacy, values and decisions.pdf   Theory   \n",
       "3              EDS3 Fundamental Limits of ML.pdf   Theory   \n",
       "4                   EDS4 Bias and Fairness I.pdf   Theory   \n",
       "5                        Microsoft_Standards.pdf  Applied   \n",
       "\n",
       "                                                link  description  \n",
       "0  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  \n",
       "1  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  \n",
       "2  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  \n",
       "3  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  \n",
       "4  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  \n",
       "5  https://github.com/arturofredes/Ethical_AI_RAG...          NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('./library.csv', delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#load multiple documents\n",
    "loaders = [\n",
    "PyPDFLoader(\".\\documents\\EDS0 Introduction Data Science in Context.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\EDS1 Ethical Foundations.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\EDS2 Legitimacy, values and decisions.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\EDS3 Fundamental Limits of ML.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\EDS4 Bias and Fairness I.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\Microsoft_Standards.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Jordi Vitrià\\nIntroduction  +  Data Science in Context', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 0, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='•Data science has the potential to be both beneﬁcial (Improved Decision-Making, Predictive Analytics, Personalized Services, Efﬁciency and Automation, etc.) and detrimental (Privacy Concerns, Bias and Fairness Issues, Security Risks, Loss of Jobs, Data Manipulation, etc.) to individuals (individual harms) and/or to society (sistemic risks).  •To help eliminate/mitigate any adverse effects, we must seek to understand the potential impact of our work for people. •In this course, we will explore the social and ethical ramiﬁcations of the choices we make at the different stages of the data analysis pipeline, from data collection and storage to understand feedback loops in the analysis.  •Through case studies and exercises, students will learn the basics of causal thinking, ethical thinking, understand some tools to check or mitigate undesired effects and study the distinct challenges associated with ethics in modern data science.Introduction\\n2', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 1, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Course Instructors •Jordi Vitrià  (https:/algorismes.github.io),  Departament de Matemàtiques i Informàtica de la UB. •Itziar de Lecuona  (http:/www.bioeticayderecho.ub.edu/ca/itziar-de-lecuona),  Bioethics and Law Observatory at the University of Barcelona. Introduction\\n3\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 2, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Calendar (tentative)\\n4\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 3, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='•Proﬁciency in Python. •Calculus, Linear Algebra. •Basic Probability and Statistics. •Critical Thinking.Prerequisites\\n5Critical thinking is the ability to think clearly and rationally, understanding the logical connection between ideas.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 4, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content=\"•The subject will be evaluated through a combination of both an exam (50%) and practical assignments (50%).  •The exam will test the students' theoretical understanding of the material covered in class.  •The practical assignments/case studies, on the other hand, will give students the opportunity to apply what they have learned in class to real-world scenarios and will be used to evaluate their practical skills and abilities.Grading\\n6Example:  To study the limitations of Machine Learning (ML) algorithms for predicting  juvenile recidivism.  Recividism prediction:  The act of a person committing a crime after they have been convicted of an earlier crime. \", metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 5, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='7DS in Context', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 6, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='While there is no single deﬁnition of data science, it can be broadly thought of as the systematic analysis of the scientiﬁc, computational and analytical methods (methodology) used to process and extract information, knowledge, and insights from data to inform decision-making (or to act in an automatic way).  There is a clear intersection with data-centric AI.  Data-Centric AI is the discipline of systematically engineering the data used to develop AI competences / tools (such as ML, NLP, Vision, etc.). Data Science and AI\\n8\\nDSDLMLAI', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 7, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n9Data is a matter of describing things as they are, and there is no art to it and certainly no fashion!! We want to be objective and should let things speak for themselves!!\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 8, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content=\"In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n10Data is a matter of describing things as they are, and there is no art to it and certainly no fashion!! We want to be objective and should let things speak for themselves!!\\nNaive              Data, in its raw form, consists of numbers, text, or other symbols that represent information. However, without context and analysis, these representations lack meaning. It's the role of data scientists, analysts, and researchers to interpret this data—by analyzing patterns, trends, and anomalies—to derive insights and conclusions.\", metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 9, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n11\\nGenerally, having more data available tends to decrease the number of errors in machine learning applications. However, when it comes to minority groups, having unlimited data can still lead to high error rates.Credit: Moritz Hardt', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 10, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Motivation: harms, unfairness, risks…\\n12\\nCredit: Moritz Hardt', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 11, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content=\"Motivation: harms, unfairness, risks…\\n13\\nMachine Learning (ML) can inadvertently reinforce stereotypes due to biases in the data it's trained on and the way these systems are designed and deployed.\", metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 12, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Motivation: harms, unfairness, risks…\\n14\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 13, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Motivation: harms, unfairness, risks…\\n15\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 14, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Motivation: harms, unfairness, risks…\\n16\\nSyRI (Systeem Risico Indicatie)  Public Administration that has implemented it: Social Protection, at central and municipal level. Description of the innovation:  In 2012, the Dutch Tax Agency began using self-learning algorithms to create fraud risk proﬁles in order to prevent child care beneﬁt fraud. Expected impact:  Enhanced inspection capabilities, improved child welfare, reduction of misuse of public funds', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 15, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Motivation\\n17\\nResult: After a few years of being in operation, this system was withdrawn (2022) due to clearly negative consequences (and the acting government resigned).  The algorithm had been developed in such a way that it categorized as debtors families who had ﬁlled out the application documents incorrectly. At the same time, having dual nationality also inﬂuenced this proﬁling, as well as coming from a low socioeconomic level, being immigrants or belonging to an ethnic minority were characteristics that led the algorithm to disproportionately penalize these population groups.  As a result, more than 10,000 people fell into poverty, others died by suicide after receiving debt bills for impossible amounts, and even more than 1,100 children were separated from their families and put in reception centers. A total of 30,000 families were affected by this algorithm.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 16, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='As data science methods become more common within different ﬁelds, there are both opportunities and challenges for individuals working in data science.  For example, managing privacy, fairness, and bias issues when working with people’s data can be difﬁcult and complex. Approach\\n18', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 17, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Additionally, public perceptions are still developing around many aspects of data-based technology, including the use of artiﬁcial intelligence (AI) in systems and decision making, and ‘big data’ sources about people, such as social media and mobile phone data.  This course is focused on both, giving a theoretical basis and providing the necessary tools to keep up with these challenging ethical issues. Approach\\n19', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 18, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='20ApproachData-empowered algorithms are reshaping our personal, professional, and political realities, and they are likely to have an even larger effect going forward.  However, as with all developing technologies, increases in impact inevitably give rise to unanticipated consequences.  These challenge our norms for how we use technology in ways consistent with our values. Many scholars, educators, and technology companies refer to these as ethical challenges. ', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 19, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Learning outcomes: •Understand the impacts of data/models misuse. •Develop your ability to investigate how data and data-powered algorithms shape, constrain, and manipulate our commercial, civic, and personal experiences. •Develop you ability to identify and mitigate potential risks. •Have a toolkit to implement in your workplaces. Ultimately, to redirect your thinking from what is merely advantageous to what is genuinely good — and be prepared to help you navigate the ethical aspects of DS development and deployment.Approach\\n21', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 20, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='22Data science is the study of extracting value from data – value in the form of insights or conclusions.Data Science in Context\\n○ A hypothesis, testable with more data; ○ An “aha!” that comes from a succinct statistic or an apt visual chart; or ○A plausible relationship among variables of interest, uncovered by examining the data and the implications of different scenarios. ○Etc.○ Prediction of a consequence; ○ Recommendation of a useful action; ○ Clustering that groups similar elements; ○ Classiﬁcation that labels elements in groupings; ○ Transformation that converts data to a more useful form; or ○ Optimization that moves a system to a better state.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 21, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='23Insights and conclusions often arise from models , which are abstractions of the real world. \\nModels that generate these conclusions may be clear box or black box . A clear box model’s logic is available for inspection by others, while an black box model’s logic is not. The “opaque box” term also apply to a model whose operation is not comprehensible. Data Science in Context\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 22, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='24Data Science in ContextResponsible model development refers to the practice of models in a manner that prioritizes ethical, fair, and accountable considerations throughout the lifecycle of the model.  The goal is to ensure that these systems are designed, deployed and maintained in ways that minimize harm, maximize beneﬁts, and adhere to societal norms and values.We must consider all stakeholders!', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 23, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='25Data Science in ContextDataApproachDependabilityUnderstandabilityFocusToleranceELSIConsider whether data of sufﬁcient integrity, size, quality, and manageability exists or could be obtained.Consider whether there is a technical approach grounded in data, such as an analysis, a model, or an interactive visualization, that can achieve the desired result.Does the application meet needed privacy protections?  Is its security sufﬁcient to thwart attackers who try to break it? Does it resist the abuse of malevolent users?  Does it have the resilience to operate correctly in the face of unforeseen circumstances or changes to the world?Will the application need to detail the causal chain underlying its conclusions?  Or will it make its underlying data and associated models, software, and techniques transparent and provide reproducibility?Consider whether the application is trying to achieve well-speciﬁed objectives that align with what we truly want to happen.Consider the application holistically with regard to legality, risk, and ethical considerations. Many of the topics under Dependability or Clear Objectives topics are relevant here.Consider both the possible unintended side effects if the objective is not quite right and the possible damage from failing to meet objectives.Here are some key principles and practices associated with responsible model development:', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 24, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='26ExampleMusic Recommendation Music recommendation has few legal issues and fewer risks than other domains (although, for example, it is crucial to be careful about recommending obscene lyrics to minors).  However, there are many ethical issues relating to the type of recommendations made and their impact on individual listeners, their community, and the creator/artist whose success may be at the mercy of these algorithms.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 25, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='27Why Ethics?   in technology, data science, AI…', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 26, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='“Everything that is not forbidden by laws of nature is achievable,  given the right knowledge”  (Credit: David Deutsch) But that’s the problem.  “Everything” means everything: vaccines and bioweapons,   video on demand and Big Brother on the tele-screen.  Something in addition to science ensured that vaccines were put  to use in eradicating diseases while bioweapons were outlawed.    Fragment de: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”. Apple Books. Scientific point of view', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 27, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Scientific point of view\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 28, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='30Kranzberg’s First Law:  “Technology is neither good nor bad;  nor is it neutral.”  By which he means that, “technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”What was the main (unexpected) consequence of the agricultural revolution? What is the main (unexpected) consequence of the industrial revolution?', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 29, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='31How to manage the unintended  consequences of DS/AI?   ', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 30, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='32\\nIndustry self-regulation is the process whereby members of an industry, trade or sector of the economy monitor their own adherence to legal, ethical, or safety standards, rather than have an outside, independent agency such as a third party entity or governmental regulator monitor and enforce those standards.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 31, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='33\\nChecklists', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 32, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='34\\nThere are hundreds of documents about ethical guidelines!', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 33, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='35\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 34, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='36\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 35, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='37\\nLaw', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 36, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='38Resposible AI Standards\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 37, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='39Data and Ethics  ', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 38, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='40The combination of data analytics, a data-saturated and poorly regulated environment, and the absence of widespread, well-designed standards for data practice in industry, university, non-proﬁt, and government sectors has created a ‘perfect storm’ of ethical risks.  Thus no single set of ethical rules or guidelines will ﬁt all data circumstances; ethical insights in data practice must be adapted to the needs of many kinds of data practitioners operating in different contexts. What does ethics have to do with data?', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 39, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='41What does ethics have to do with data?We can deﬁne a harm or a beneﬁt as ‘ethically signiﬁcant’ when it has a substantial possibility of making a difference to certain individuals’ chances of having a good life, or the chances of a group to live well: that is, to ﬂourish in society together.  Some harms and beneﬁts are not ethically signiﬁcant. Say I prefer Coke to Pepsi. If I ask for a Coke and you hand me a Pepsi, even if I am disappointed, you haven’t impacted my life in any ethically signiﬁcant way.', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 40, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='42In the context of data practice, the potential harms and beneﬁts are real and ethically signiﬁcant.  But due to the more complex, abstract, and often widely distributed nature of data practices, as well as the interplay of technical, social, and individual forces in data contexts, the harms and beneﬁts of data can be harder to see and anticipate. In this respect, then, data has a broader ethical sweep than engineering of bridges and airplanes. Data practitioners must confront a far more complex ethical landscape than many other kinds of technical professionals…What does ethics have to do with data?', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 41, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='43HUMAN UNDERSTANDING:  Because data and its associated practices can uncover previously unrecognized correlations and patterns in the world, data can greatly enrich our understanding of signiﬁcant relationships — in nature, society, and our personal lives.  Ethical Benefits of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 42, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='44SOCIAL, INSTITUTIONAL, AND ECONOMIC EFFICIENCY:  Once we have a more accurate picture of how the world works, we can design or intervene in its systems to improve their functioning.  This reduces wasted effort and resources and improves the alignment between a social system or institution’s policies/processes and our goals. Ethical Benefits of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 43, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='45EFFECTIVENESS AND PERSONALIZATION:  Not only can good data practices help to make social systems work more efﬁciently, but they can also used to more precisely tailor actions to be effective in achieving good outcomes for speciﬁc individuals, groups, and circumstances, and to be more responsive to user input in (approximately) real time. Ethical Benefits of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 44, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='46HARMS TO PRIVACY & SECURITY:  Thanks to the ocean of personal data that humans are generating today (or, to use a better metaphor, the many different lakes, springs, and rivers of personal data that are pooling and ﬂowing across the digital landscape), most of us do not realize how exposed our lives are, or can be, by common data practices. Ethical Harms of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 45, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='47HARMS TO FAIRNESS AND JUSTICE:  We all have a signiﬁcant interest in being judged and treated fairly, whether it involves how we are treated by law enforcement and the criminal and civil court systems, how we are evaluated by our employers and teachers, the quality of health care and other services we receive, or how ﬁnancial institutions and insurers treat us. Ethical Harms of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 46, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='48HARMS TO TRANSPARENCY AND AUTONOMY:  In this context, transparency is the ability to see how a given social system or institution works, and to be able to inquire about the basis of life-affecting decisions made within that system or institution.  So, for example, if your bank denies your application for a home loan, transparency will be served by you having access to information about exactly why you were denied the loan, and by whom. Autonomy is the state that results from being able to make informed free decisions. Ethical Harms of Data Practices', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 47, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='49Europe’s GDPR\\n', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 48, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='50The   GDPR   can   be   summarized   in   the   following points: 1. It   concerns   “Personal   Data”:   Name,   address, localization,  online  identiﬁer,  health  information, income, cultural proﬁle, ...  2. Communication:   Who   gets   the   data,   why, for how long? (No use for other ‘incompatible’ purposes. Use as long as necessary.) 3. Consent: Get clear informed consent. 4. Access: Provide access to my data. 5. Right to be forgotten (not for research). 6. Right to explanation for contracts (& right to have a person decide). 7. Marketing: Right to opt out. 8. Legal: Maintain EU legislation when transferring data out. 9. Need for a “data protection ofﬁcer” in your organisation. 10. Impact   assessment   prior   to   high-risk   processing (new technology, personal information, surveillance, sensitive). Europe’s GDPR', metadata={'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf', 'page': 49, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'description': nan}),\n",
       " Document(page_content='Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science   Foundations  1', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 0, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='2Preliminaries', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 1, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content=\"Is there a common ground to talk about what is right and what is wrong? Ethical relativism is the theory that holds that morality is relative to the norms of one's culture. That is, whether an action is right or wrong depends on the moral norms of the society in which it is practiced.So, to be able to advance, let’s assume a common ground (beign aware of its limitations) based on a revision of the enlightenment, a framework that tries to encompass rationality, science, humanism and progress.\", metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 2, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='Once upon a time…\\n4\\nLet’s see how this book proposes a “common ground”…\\nS.Pinker is a cognitive scientist…..', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 3, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='“…the most arresting question I have ever ﬁelded followed a talk in which I explained the commonplace among scientists that mental life consists of patterns of activity in the tissues of the brain.”  “A student in the audience raised her hand and asked me: “Why should I live?” “What I recall saying … went something like this:…” \\nFragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n5', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 4, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='“In the very act of asking that question, you are seeking reasons for your convictions, and so you are committed to reason as the means to discover and justify what is important to you. (…) \\nFragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n6Proposition 1: The basis\\nA reason is an explanation of a situation or an event that provides a logical basis for a conclusion, belief, or action.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 5, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='As a sentient being, you have the potential to ﬂourish. You can reﬁne your faculty of reason itself by learning and debating. You can seek explanations of the natural world through science, and insight into the human condition through the arts and humanities. You can make the most of your capacity for pleasure and satisfaction, which allowed your ancestors to thrive and thereby allowed you to exist. (…)” “(…) You can appreciate the beauty and richness of the natural and cultural world. As the heir to billions of years of life perpetuating itself, you can perpetuate life in turn. You have been endowed with a sense of sympathy—the ability to like, love, respect, help, and show kindness—and you can enjoy the gift of mutual benevolence with friends, family, and colleagues.” Fragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n7Proposition 2: You as an individual', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 6, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='And because reason tells you that none of this is particular to you, you have the responsibility to provide to others what you expect for yourself. You can foster the welfare of other sentient beings by enhancing life, health, knowledge, freedom, abundance, safety, beauty, and peace. History shows that when we sympathize with others and apply our ingenuity to improving the human condition, we can make progress in doing so, and you can help to continue that progress.” Fragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n8Proposition 3: You as a member of a society', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 7, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='The previous position statement assumes a lot of things about the world that are not self-evident (these are the ideas of the Enlightenment).  Not everybody agree on those statements!   But this is a course on applied ethics, and we need a starting point for the discussion. This will be our provisional starting point. Assumptions\\n9Christians, Jews, and Muslims embrace ethical codes of moral absolutes based on God’s character or moral decree.    Secular Humanists, Marxists, and Postmodernists ground their ethical systems in atheism, naturalism, and evolution. ', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 8, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='10The role of technology in societyMankind has not changed biologically throughout history but human society is undergoing continuous development through the harnessing of information and knowledge in the form of various technologies which have affected our value systems, power structures, everyday routines and environment. History begins with the accounts of the ancient world around the 4th millennium BC, and it coincides with the invention of writing.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 9, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='11The role of technology in societyThe course of human development can be grouped into three time periods separated by \"revolutions\": •The Cognitive Revolution began history about [50,000, 70, 70,000] years ago. •The Agricultural Revolution accelerated it about 12,000 years ago. •The Scientiﬁc Revolution, which began only 500 years ago, has made possible the industrial age and the world as we know it today. A Revolution is associated with a change, often of a technological nature, that causes the human species to change its way of life (organization of work, social organization, cultural practices, etc.). Concepts such as big data, machine learning, artiﬁcial intelligence and data science are making possible a new Revolution, the Digital, which can have as much or deeper consequences than the previous ones.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 10, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='12\\nKranzberg’s Six Laws of Technology The role of technology in societyKranzberg’s First Law:  “Technology is neither good nor bad;  nor is it neutral.”  By which he means that, “technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”Dr. Melvin Kranzberg was a professor of the history of technology at the Georgia Institute of Technology', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 11, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='13Technologies are not ethically ‘neutral’, for they reﬂect the values that we ‘bake in’ to them with our design choices, as well as the values which guide our distribution and use of them.  Technologies both reveal and shape what humans value, what we think is ‘good’ in life and worth seeking.The role of technology in society', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 12, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='14Not only does technology greatly impact our opportunities for living a good life, but its positive and negative impacts are often distributed unevenly among individuals and groups.  Technologies can create widely disparate impacts, creating ‘winners’ and ‘losers’ in the social lottery or magnifying existing inequalities The role of technology in society', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 13, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='15In other cases, technologies can help to create fairer and more just social arrangements, or create new access to means of living well How do we ensure that access to the enormous beneﬁts promised by new technologies, and exposure to their risks, are distributed in the right way? This is a matter of ethics.The role of technology in society', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 14, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='16\\nHealth care organizations, like many other enterprises, face steep challenges in their attempt to maximize operational efﬁciency in the face of resource constraints. Whether it is a hospital’s attempt to optimize stafﬁng or a government trying to fairly allocate and distribute limited doses of Covid-19 vaccines, these tasks can be formidable. A promising way to manage the complexity is to enlist data-driven analytics and artiﬁcial intelligence (AI).However, such techniques, while powerful, can also mask problematic underlying ethical assumptions or lead to morally questionable outcomes. Consider a recently published study about models used by some of the most technologically advanced hospitals in the world to help prioritize which patients with chronic kidney disease should receive kidney transplants. It found that the models discriminated against black patients: “One-third of Black patients … would have been placed into a more severe category of kidney disease if their kidney function had been estimated using the same formula as for white patients.” While it is just the latest of many studies to show the deﬁciencies of such models, it is unlikely to be the last.Cases', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 15, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='17Cases\\nhttps://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/Public services have to be efﬁcient, consistent, objective, etc.  Automatic decision systems can help to this challenge. But…', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 16, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='18Ethics and AlgorithmsWe will specially consider the case of algorithms that are used to  1.turn data into evidence for a given outcome, which is used to,  2.trigger and motivate an action that may have ethical consequences.  Actions (1) and (2) may be performed by data-driven automatic algorithms —such as machine  learning (ML) algorithms— and this complicates the attribution of responsibility for the effects of actions that  an algorithm may trigger. Why?', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 17, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='19Ethics and AlgorithmsThere are, at least, 5 types of ethical concerns: 1.Inconclusive evidence. 2.Inescrutable evidence. 3.Misguided evidence. 4.Unfair outcomes. 5.Transformative effects.\\nEpistemic factors\\nNormative concernsThe epistemic factors in the map highlight the relevance of the quality and accuracy of the data for the justiﬁability of the conclusions that algorithms reach and which, in  turn, may shape morally-loaded decisions affecting individuals, societies, and the environment.  The normative concerns identiﬁed in the map refer explicitly to the ethical impact of algorithmically-driven actions and decisions,  including lack of transparency (opacity) of algorithmic processes, unfair outcomes, and unintended consequences.https://link.springer.com/content/pdf/10.1007/s00146-021-01154-8.pdf', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 18, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='20From another point of view, ethical concerns can be divided in three different time frames/areas: • Short-term/person, organization: What is the impact of [privacy, transparency, fairness] in my application? • Medium-term/society: How the use [military use, medical care, justice, education] of these applications will change the way we are organized as a society? • Long-temr/humans: What are the ethical goals of these technologies?GDPR…Autonomous weapons, pre-pol, AI justice,…Singularity, convergence…Applied Ethics Problems', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 19, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='21Laws are coming in 2024, including the EU AI Act!\\n', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 20, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='22What is Ethics?   ', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 21, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='Definitions\\n23Ethics is the process of questioning, discovering and defending your values, principles and purposes in order to be able of deciding what is right and what is wrong. \\nEthics seeks to answer questions like “what is good or bad”, “what is right or what is wrong”, or “what is justice, well-being or equality”. Applied ethics concerns what a moral agent is obligated or permitted to do in a speciﬁc situation or a particular domain of action.\\nPhilosophySciencePurpose\\nValuesPrinciplesSocietyYour KnowledgeYour Beliefs\\n', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 22, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='24How do we make decisions?\\nDesires NeedsPurpose Principles ValuesKnowledge', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 23, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='25Purpose\\nValuesPrinciplesYour reason for being. Leaving the world better than I found it.\\nThe things that are goodJustice, knowledge, equality,…Lines I’ll never cross.Beliefs, the necessary ingredients of a good individual decision.\\nTreat other people the way you would like to be treated.PhilosophySocietyScienceYour Beliefs', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 24, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='26Beliefs, the necessary ingredients of a good individual decision.\\nNowadays, the most common approach is to consider that, rather than grounding our beliefs on a solid foundation, we assemble a collection of beliefs that hold together under a mutual (maybe unstable) knowledge attraction. PurposeValuesPrinciples\\nTraditional approaches consider that person’s beliefs must be grounded in a solid foundation that cannot be discussed (God, rationality and harm, etc).', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 25, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='27Purpose\\nValuesPrinciplesKnowledge, our vision of the world\\nLaw\\nMoralPhilosophySocietyBeliefs about RealityYour KnowledgeYour Environment', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 26, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='28How do we make decisions?\\nDesires NeedsPurpose Principles ValuesLaw Moral', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 27, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='29LawLaws are formal rules that govern how we behave as members of a society. They specify what we must do, and more frequently, what we must not do. They create an enforceable standard of behavior.  Laws can be just or unjust, because they are subject to ethical assessment. Law cannot be applied to every decision: it cannot say anything about what to do when you hear a friend to make a racist joke…', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 28, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='30In an ideal world, our ethical beliefs shape law and moral systems. We need a toolkit to run our reﬂections!The role of ethics is not to be a soft version of the law, even if laws are based on ethical principles.  The  real  application  of  ethics  lies  in  challenging the status quo, seeking its deﬁcits  and blind spots. N.Kluge Corrêa, Good AI for the Present of Humanity. Democratizing AI Governance How do we take decisions?', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 29, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='31Morality refers to an informal social framework of values, beliefs, principles, customs and ways of living. Examples: christianity, stoicism, buddhism… Moral systems provide a set of answers to general ethical questions.  Morality is, in most of the cases, inherited (unconsciously) from family, community or culture. Morality is applied as a matter of habbit, without having to think.  In most cases, there are moral authorities.How do we take decisions?', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 30, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='32You can take decisions exclusively based on laws and morality, but this should not be enough. Ethics is a process of reﬂection that aims to answer this question: What should I do? The answer is based on our values, principles and purposes rather than social conventions.  An ethical decision is based on conscious, rational reﬂection. How do we take decisions?', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 31, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='33Traditional Normative EthicsThere are three traditional theories of what it means to be ethical: •Utilitarianism (J.Bentham): Does an action maximize happiness and well-being for all affected individuals? (consequences) •Deontology (I.Kant): Does an action follow a moral rule (e.g. the Golden Rule: ‘Treat others how you want to be treated’)? An action should be based on whether that action itself is right or wrong under a series of rules, rather than based on the consequences of the action. (beliefs) •Virtue Ethics (Aristotle): Does an action contribute to virtue? (justice, honesty, responsibility, care, etc.)', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 32, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='34Traditional Normative Ethics\\nAsimov’s Three Laws of Robotics are an example of deontological approach to AI ethics.1.A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2.A robot must obey orders given it by human beings except where such orders would conﬂict with the First Law. 3.A robot must protect its own existence as long as such protection does not conﬂict with the First or Second Law.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 33, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='35Traditional EthicsSuppose it is obvious that someone in need should be helped.  •A utilitarian will point to the fact that the consequences of doing so will maximize well-being.  •A deontologist will point to the fact that, in doing so the agent will be acting in accordance with a moral rule such as “Do unto others as you would be done by”.   •A virtue ethicist will point to the fact that helping the person would be charitable or benevolent. \\nhttps:/ /plato.stanford.edu/entries/ethics-virtue/', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 34, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='36(Political) Philosophy\\n4 theories about what is right and what is wrong in society ', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 35, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='37\\nJohn RawlsJohn Rawls tried to work out how people would construct their society if the choice had to be made behind what he called a “veil of ignorance” about whether they will be rich, poor or somewhere in-between.  Faced with the risk of being the worst off, Rawls posited, humans would not demand total equality, but would need to be assured of the trappings of a modern welfare state. The assurance of basic necessities and the opportunity to do better would form the foundation for social and political justice and provide the ability for people to assert themselves.\\nRawlsians(Political) Philosophy', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 36, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content=\"38\\nJohn LockeA man had a right to live for himself and an individual’s happiness cannot be prescribed by another man or any number of other men. Libertarianism holds that the basic moral concepts are individual rights and that the rights to be respected are noninterference rights. These generally fall under the heading of rights to life, to liberty or to property.  For libertarianism, the only proper limit to one person's enjoyment of these rights is his or her duty to respect the similar rights of others.\\nLibertarians(Political) Philosophy\", metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 37, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='39\\nJohn Stuart MillRulers must be guided to the total happiness, or “utility,” of all the people, and should aim to secure “the greatest good for the greatest number.” Utilitarian calculus opens up the possibility that in situations such as a pandemic, some people might justly be sacriﬁced for the greater good. It would beneﬁt society to accept casualties.\\nUtilitarians\\n(Political) Philosophy', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 38, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='40\\nMichael SandelEveryone derives their identify from the broader community.  Individual rights count, but not more than community norms. Justice cannot be determined in a vacuum or behind a veil of ignorance, but must be rooted in society (common good).\\nCommunitarians(Political) Philosophy', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 39, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='41Only west-centric values?\\nThat most AI ethics guidelines are being written in Western countries means that the ﬁeld is dominated by Western values such as respect for autonomy and the rights of individuals, especially since the few guidelines issued in other countries mostly reﬂect those in the West.Buddhism proposes a way of thinking about ethics based on the assumption that all sentient beings want to avoid pain. Thus, the Buddha teaches that an action is good if it leads to freedom from suffering.Another key concept in Buddhism is compassion, or the desire and commitment to eliminate suffering in others. ', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 40, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='42Canonical views of AI ethics?\\nValue diversity & Pragmatism', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 41, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='43Ethics approachesThe normative approach to ethics focuses on how the world should be.\\nThe positive approach to ethics describes the world as it is.  It is about how humans judge situations and decisions in different scenarios.  \\nhttps://existentialcomics.com/comic/424', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 42, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='44An alternative approach to ethicsIt is about how humans judge situations and decisions in different scenarios.  This is done by focusing our understanding of the world on empirically veriﬁable effects that we can later explore through normative approaches. For instance, empirical work has shown that people exhibit algorithmic aversion, a bias where people tend to reject algorithms even when they are more accurate than humans.Dietvorst BJ, Simmons JP, Massey C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. Journal of Experimental psychology. General. 2015 Feb;144(1):114-126. DOI: 10.1037/xge0000033.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 43, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='45Ethics: positive approach\\n', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 44, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='46Ethics: positive approachIn recent decades, psychologists have discovered ﬁve moral dimensions that humans consider when judging situations: •Harm, which can be both physical or psychological •Fairness/liberty, which is about biases in processes and procedures •Loyalty, which ranges from supporting a group to betraying a country •Authority, which involves disrespecting elders or superiors, or breaking rules •Purity, which involves concepts as varied as the sanctity of religion or personal hygiene. These ﬁve dimensions deﬁne a space where we, humans, decide what is right and what is wrong.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 45, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='47Ethics: positive approachJudgments depend on the intention of agents, not only on the moral dimension, or the outcome, of an action. In which situation would you blame Bob?\\n', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 46, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='48Ethics: positive approachJudging machines/algorithms is not equivalent to judging humans. Humans are judged more positively than machines in autonomous driving scenarios. Humans were judged more harshly (plagiarism). Etc.\\nFindings suggest that people judge machines based on the observed outcome, but judge humans based on a combination of outcome and intention.', metadata={'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf', 'page': 47, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'description': nan}),\n",
       " Document(page_content='Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science  Decision-Making, Values and Legitimacy  2', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 0, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='2\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 1, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='3LegitimacyWhen considering a possible use case of AI, we can ask this question:  Does it make sense to use AI?\\nThe mother of all ethical AI questions', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 2, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='4LegitimacyAccording to institutional theory (politics), legitimacy refers to the congruence between organizational activities and their cultural environment.  The legitimacy of a political system depends on various factors: how well it achieves its goals, whether the subjects of the political system are involved in developing the rules, and whether the decision subject has the ability to challenge decisions. It is the central problem of politics.\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 3, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='5LegitimacyLegitimacy represents an important form of social evaluation, as it is indispensable for the acceptance and diffusion of novel technologies. The legitimacy question should precede other ethical questions such as discrimination or privacy. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 4, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='6LegitimacyIn these scenarios there are “legitimacy issues”: •A student is proud of the creative essay she wrote for a standardized test. She receives a perfect score, but is disappointed to learn that the test had in fact been graded by a computer. •A defendant ﬁnds that a criminal risk prediction system categorized him as high risk for failure to appear in court, based on the behavior of others like him, despite he had every intention of appearing in court on the scheduled date. •An automated system locked out a social media user for violating the platform’s policy on acceptable behavior. The user insists that they did nothing wrong, but the platform won’t provide further details nor any appeal process. Which are the issues?', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 5, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='7LegitimacyHow do we evaluate the legitimacy of AI for taking high-stake decisions? We need to understand which are the critical issues of a high-stake decision and how do they interact with a set of values. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 6, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Values & Technology\\n8', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 7, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content=\"9Values/Principles and technologyOrganizations deﬁne their values regarding AI through a multifaceted process.  Here's a general outline of how this process might unfold: •Mission and Vision Alignment: Companies begin by ensuring that their AI values align with their broader mission and vision. This means considering how AI can help achieve their goals while adhering to the ethical standards they've set for themselves. •Stakeholder Engagement: They engage with various stakeholders, including employees, customers, partners, and potentially affected communities, to gather insights and perspectives on the ethical use of AI. This inclusive approach helps ensure that the company's AI values are considerate of diverse viewpoints and concerns. •Ethical Standards and Principles: Many companies adopt ethical frameworks or principles speciﬁc to AI. These often include commitments to transparency, fairness, accountability, privacy, and ensuring that AI technologies do not cause harm. These principles guide the development and deployment of AI systems. •Regulatory and Industry Standards Compliance: Companies also consider existing and anticipated regulations governing AI in their jurisdictions, as well as industry best practices and standards. This helps ensure that their AI values and practices are not only ethical but also legally compliant.\", metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 8, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='10Example\\nIt is based on ﬁve core principles: justice, autonomy, beneﬁcence, non-maleﬁcence, and transparency. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 9, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='11Example• Justice: • Impartiality • Equality • Proportionality • Autonomy: • Explainability • Privacy • Literacy • Non-Maleﬁcence: • Reliability • Controllability • Accountability • Transparency: • Comprehensibility • Interactivity • Traceability • Beneﬁcence: • Security • Sustainability • Responsibility\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 10, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Decision-making automation\\n12Decision-making automation refers to the use of technology, particularly software and algorithms, to automate the process of making decisions that were traditionally made by humans.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 11, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systemsIn the context of decision-making, automation can be categorized into three distinct types.\\n13\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 12, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systemsConverting human-designed decision-making rules into software. \\n14\\nThe doctors still decided who was medically eligible for dialysis. But then, they established a second committee, a group of seven laypeople\\xa0chosen by the local medical society, who would make the non-medical decision of how to allocate the few available slots among the many eligible patients. The committee members were given some basic education about kidney medicine, but weren’t told how to make their moral choices.1962Rules that have been set down by hand.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 13, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systemsConverting human-designed decision-making rules into software.\\n15\\nRight now, about 100,000 people in the U.S. are waiting for a kidney transplant.The new algorithm’s logic, and the factors that determine each patient’s fate within it, are transparent—not only publicly disclosed, but explained in simple terms. And the system’s performance is subject to annual audits, by an organization that publishes detailed reports.2022', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 14, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systemsLeveraging machine learning to emulate the informal decision-making processes of humans.\\n16\\nDecision makers have primarily reliedon informal judgment rather than formally speciﬁed rules.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 15, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systemsLearning decision-making rules from labeled data (based on a loss function that is a proxy of the policy).\\n17\\nUncovering  patterns in a dataset that predict an outcome or property of policy interest (such as risks of cardiovascular disease, life expectancy, etc.)— and then bases decisions (such as transplant priority) on those predictions.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 16, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systems\\n18\\nLearning decision-making rules from labeled data (and using a proxy loss function).', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 17, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Kinds of automatic decision-making systems\\n19Learning decision-making rules from labeled data (and using a proxy loss function). Example:  To apply a policy for selecting “the student who will beneﬁt the most from studying at your university” you can employ several proxy concepts. Some key proxy concepts include: •Academic Performance (best predicted scores). •Engagement and Participation (best predicted engagement). •Socio-Economic Impact: (best predicted socio-economic impact, either personally (e.g., ﬁrst-generation college students) or on their communities. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 18, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Legitimacy & Decision Making\\n20', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 19, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='21About the processLesson of History Up to now, in our society, critical decisions were often made by bureaucratic systems.  Bureaucracies arose, in part, to counteract the subjectivity, randomness, and inconsistency inherent in human decision-making, that can result in arbitrary decisions.  Its established rules and procedures are designed to reduce the impact of weaknesses found in individual decision makers.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 20, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='22About the processArbitrariness has two sides.  The ﬁrst view of arbitrariness is primarily concerned with procedural regularity: whether a decision-making scheme is executed consistently.  When decision-making is arbitrary in this sense of the term, individuals may ﬁnd that they are subject to different decision-making schemes and receive different decisions simply because they go through the decision-making process at different times.  This principle is based on the belief that people are entitled to similar decisions unless there are reasons to treat them differently.', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 21, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='23About the processThe second view of arbitrariness refers to the basis for making decisions without reasoning, even if decisions are consistently made on that basis.  This principle is based two beliefs:   •the belief  that random decision-making (in general) shows a lack of respect for people &  •the belief that subjective decision-making can lead to unfairness, errors, and lack of quality. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 22, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='24About the decisionsThe results of the decisions must be: •Accurate (the system must provide results that are “correct” in most cases or very close to the ideal result). Correctness of the results must be deﬁned in a way that is compatible with the values of those affected by the decisions. •Reliable (the system offers stable and consistent results in different scenarios, is invariant to some kinds of changes in the environment). •Effective (the system delivers results that affect/impact the real world in the expected way).', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 23, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='25Legitimacy & Decision MakingWe can consider the application of automated decision-making processes in a speciﬁc scenario to be legitimate if it meets several criteria at two different levels: 1. The results of the decisions are: •Accurate & Aligned •Reliable •Effective 2. The decision-making process is: •Well executed •Well justiﬁed', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 24, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='26Legitimacy & Decision Making\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 25, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='27Legitimacy & Decision Making\\nThe ﬁrst form of automation is a direct response to arbitrariness as inconsistency. It allows procedural regularity. However, several problems can arise: policies intended to be automated may lack clarity or speciﬁcity, leading programmers to make subjective decisions and thus overstep their bounds in policy deﬁnition. Also, the software may be prone to errors. Automation also requires an institution to pre-determine all decision-making criteria, leaving no ﬂexibility for unforeseen or unforeseen details. In addition, automation poses a signiﬁcant risk as it potentially reduces accountability and intensiﬁes the impersonal nature of bureaucratic interactions.\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 26, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='28Legitimacy & Decision Making\\nIn the second case, this form of automation could help solve problems of arbitrariness in human decision-making by formalizing and ﬁxing a decision-making scheme similar to what humans might have used in the past .  In this sense, machine learning can be desirable because it can help smooth out any inconsistencies or subjectivities in human decisions.\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 27, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='29Legitimacy & Decision MakingThese decision-making schemes can be considered equivalent to those employed by humans, and are therefore likely to perform similarly, even though the model may make its decisions differently and produce quite different error patterns.  Worse, the models could also learn to base themselves on criteria in ways that humans would ﬁnd troubling or objectionable, even if doing so still produces a set of decisions similar to what humans would make.\\n', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 28, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content=\"30Legitimacy & Decision Making\\nThe third form of automation, which we'll call predictive optimization, speaks directly to concerns with reasoned decision making.  Predictive optimization attempts to provide a more rigorous basis for decision-making based only on criteria to the extent that they demonstrably predict the outcome or quality of interest. (Consistently execute a pre-existing policy through automation does not ensure that the policy itself is reasoned. Nor does relying on past human decisions to induce a decision-making rule guarantee that the basis of automation decision-making will reﬂect reasoned judgments).\", metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 29, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content=\"31Legitimacy & Decision Making\\nBut it has ﬂaws:  •Good predictions may not lead to good decisions (causality),  •It's hard to measure what we really care about (proxy loss functions),  •Training data rarely matches the deployment conﬁguration (drift of distribution),  •Social outcomes are not predictable with precision (predictability), with or without machine learning.\", metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 30, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='32ConclusionsIn consequential applications of AI, to establish legitimacy, the decision-makers must be able to afﬁrmatively justify their scheme according to the dimensions we have set out: the level of accuracy, reliability and effectiveness of their predictions, of their potential ethical problems, and that is also well executed and well justiﬁed.  To these properties we could add a condition of prudence, irreducibility: that there is no comparable solution based on human-designed algorithms and that therefore the decision system cannot be based on converting human-designed decision-making rules into software. ', metadata={'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf', 'page': 31, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'description': nan}),\n",
       " Document(page_content='Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science  Fundamental and Practical Limits of ML3', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 0, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='2ML Aim:  If we model a phenomenon/system as a process by which some input state  is transformed into some output state , we can hope to learn an approximate transformation function  from observed past examples using machine learning/statistics. XYy=f(x)Limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 1, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='3If we model a phenomenon as a process by which some input state  is transformed into some output state , we can hope to learn a transformation function  from observed past examples using machine learning/statistics.  Method: We observe  (i.i.d data) and model  by maximizing the empirical risk of the model (accuracy, likelihood). The interpretation of  is: “given that I have observed , what can I say about ?” XYy=f(x)P(X,Y)𝔼(Y|X)𝔼(Y|X)XYLimits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 2, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='4The term \"i.i.d data\" refers to \"Independent and Identically Distributed\" data. In the context of statistics and machine learning, i.i.d is an assumption about the random variables that make up the dataset being used for analysis or modeling. Here\\'s a breakdown of what this means: Independent: Each data point (or random variable) in the dataset is assumed to be independent of the others. This means that the occurrence of one data point does not inﬂuence or change the probability of occurrence of another data point. For example, in a coin toss, each toss is independent of the previous ones. Identically Distributed: This means that each data point in the dataset is drawn from the same probability distribution and has the same statistical properties (such as mean, variance, etc.). It does not mean that all data points are the same, but rather that they share the same underlying distribution.Limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 3, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='5Limits to prediction\\nA seven-day forecast can accurately predict the weather about 80 percent of the time and a ﬁve-day forecast can accurately predict the weather approximately 90 percent of the time.  However, a 10-day—or longer—forecast is only right about half the time.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 4, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='6Weather data is typically not i.i.d because it often violates both the independence and identical distribution assumptions for several reasons: •Temporal Dependence: Weather observations are strongly dependent on preceding conditions. For example, the weather today is likely to be similar to the weather yesterday to some extent, especially in terms of temperature, precipitation, and atmospheric pressure. This sequential dependence means weather data points are not independent. •Seasonal Variations: Weather data exhibits seasonal patterns, which means that its distribution can change over different times of the year. For instance, temperatures are generally higher in summer and lower in winter in many places. This seasonal effect means that the distribution of weather data is not identical throughout the year. Limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 5, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='7Getting truly i.i.d weather data is challenging due to the inherent temporal and spatial dependencies in weather phenomena. However, you can approximate i.i.d conditions in weather data for speciﬁc types of analyses: •If your analysis can tolerate it, randomly sampling weather data from a wide range of locations and times might reduce dependencies. •Instead of using raw weather data, you can use anomalies or deviations from a long-term mean. For example, calculating the deviation of daily temperatures from the 30-year average for that day can help to remove some of the seasonal and longer-term trends, making the data more homogenous. •Etc.Limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 6, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='8Let’s suppose we get iid data. Is everything predictable given enough data and powerful algorithms?  • Are there fundamental limits? • Which are the practical limits?Limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 7, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Fundamental Limits of ML\\n9', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 8, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='10Fundamental limits to prediction•The nondeterminism of phenomena of interest  •Inscrutability of of the world •Computational limits. •Limits to collecting sufﬁcient training examples (volume, independence, etc.) •Etc.impossible to know, to understandIf there were a vast intelligence — Laplace’s Demon — that knew the exact state of the universe at any one moment, and knew all the laws of physics, and had arbitrarily large computational capacity, it could both predict the future and reconstruct the past with perfect accuracy.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 9, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='11Fundamental limits to predictionNote that: •Determinism of the universe at the most fundamental level is compatible with non-determinism at higher levels of description (chemistry, biology, psychology, sociology, etc.)! •The cause of this paradox is that higher levels of description are deﬁned by states that correspond to multiple fundamental level states.The laws of physics (Core Theory, QFT) are sufﬁcient to predict the future state of the universe (at least the part of the universe that matters for humans) at any one moment given a complete representation of the current state.Carroll, Sean M. \"The Quantum Field Theory on Which the Everyday World Supervenes.\" arXiv preprint arXiv:2101.07884 (2021).', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 10, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Practical Limits of ML\\n12', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 11, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='13Practical limits: •Sensitive dependence on inputs (butterﬂy’s effect, ill-posed problems). This is possible even in linear models. •Effects of unexpected/unpredictable events (a lottery jackpot; an accident). This corresponds to variables that interact with very low probability (the real problem of autonomous driving). •Feedback loops (predicting  causes changes in ). •Drift: the statistical relationship between the input variables and the target may change over time. •Unobservable/latent input features (intelligence, people’s thoughts).  All these issues can cause failures..YXPractical limits to prediction', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 12, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='14Sometimes, what is incorrectly framed as a prediction problem can be better understood as a problem of explanation, intervention, or decision making. •Explanation is about generating scientiﬁc insight into how a process works rather than simply predicting its input-output behavior. We need a generative model of , their statistical relationships are not sufﬁcient (causality). •Intervention is about ﬁguring out how to change a process for the better rather than treating it as a given and conﬁning oneself to making predictions. We need a generative model of  (causality). •Decision making recognizes that many considerations go into making good decisions beyond maximizing predictive accuracy (fairness, diversity, etc.).P(X,Y)P(X,Y)When shouldn’t be used prediction?', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 13, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='15Explanation is about generating scientiﬁc insight into how a process works rather than simply predicting its input-output behavior.  Example: the multicollinearity problem. Take the ﬁctional toy example of predicting a child’s reading ability (y) as a function of its age (a) and height (h). Let’s assume age and height are perfectly correlated in our data, as in the example below. Now we can express y equivalently as: When shouldn’t be used prediction?\\nThis model is not identiﬁable (existence of one unique value for each parameter) DataModels', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 14, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='16Intervention is about ﬁguring out how to change a process for the better rather than treating it as a given and conﬁning oneself to making predictions. When shouldn’t be used prediction?\\nObserving  does not determine the effect of an intervention . In general .P(Wet,Rain,Sprinkler)P(Rain|do(Wet))P(Rain|do(Wet))≠P(Rain|Wet)P(Wet,Rain,Sprinkler)Generative ModelDataSprinklerRainWetTFTTTTTTFFFFTFTFTFFTTTFT………', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 15, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='17Decision making recognizes that many considerations go into making good decisions beyond maximizing predictive accuracy, especially because the decisions themselves have causal effects. When shouldn’t be used prediction?When training a model: •I want to minimize the Empirical Risk. •I want to maximize robustness against changes in data distribution. •I want to be able of explaining my predictions. •I want to measure and mitigate unwanted biases (discrimination). •Etc.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 16, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='ML failures from a  data-centric point of view\\n18', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 17, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='ML failures\\n19ML fails when we are dealing with a predictive problem but, at inference time,  does not correspond to what happens in the real world.𝔼(Y|X)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 18, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='ML failures\\n20• Data distribution shifts: the model learns from a distribution that does not represent the world at inference time. ', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 19, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='ML failures\\n21• Data distribution shifts: the model learns from a distribution that does not represent the world at inference time.   Causes: •External changes in the  data generation process. •Degenerate feedback loops:  system’s outputs cause  changes in the inputs. \\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 20, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='ML failures\\n22•Edge Cases: a ML learning can fail in a number of edge cases, making catastrophic mistakes.\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 21, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Data Distribution Shifts\\n23• The distribution of the data the model is trained on, , is called source distribution.  • The distribution of the data the model runs inference on is called the target distribution. • can be decomposed in two ways: •  •P(X,Y)P(X,Y)P(X,Y)=P(X)P(Y|X)P(X,Y)=P(Y)P(X|Y)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 22, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Data Distribution Shifts\\n24Data distribution shifts are: • Covariate shift is when  changes, but  remains the same.  • Label Shift is when  changes, but  remains the same. • Concep drift is when  changes, but  remains the same.P(X)P(Y|X)P(Y)P(X|Y)P(Y|X)P(X)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 23, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Covariate Shift\\n25Statistics: a covariate is a variable that can inﬂuence the outcome of a given statistical trial. Supervised ML: input features are covariates. Covariate shift: Input distribution changes, but for a given input, output is the same.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 24, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Covariate Shift\\n26 changes, but for a given input,  is the same. P(X)P(Y|X)\\nNew incoming data can invalidate the current model.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 25, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Covariate Shift\\n27 changes, but for a given input,  is the same. P(X)P(Y|X)Training\\nTest\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 26, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Covariate Shift\\n28 changes, but for a given input,  is the same. Example: • Predicts (cancer | patient_data) • (age > 40) > (age > 40) • (cancer | age > 40) = (cancer | age > 40) There are several causes. E.g. women >40 are encouraged by doctors to get check-ups.P(X)P(Y|X)PPtrainingPinferencePtrainingPinference', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 27, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Covariate Shift\\n29 changes, but for a given input,  is the same. Example: • Predicts (cancer | patient_data) • (age > 40) > (age > 40) • (cancer | age > 40) = (cancer | age > 40) Training: If knowing in advance how the production data will differ from training data, use importance weighting. Production: unlikely to know how a distribution will change in advance.P(X)P(Y|X)PPtrainingPinferencePtrainingPinference', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 28, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Importace weighting\\n30In supervised machine learning, it is important to train an estimator on balanced data so the model is equally informed on all classes. To balance the classes, we can inform the estimator to adjust how it calculates loss. Using weights, we can force as estimator to learn based on more or less importance (‘weight’) given to a particular class. Weights scale the loss function. As the model trains on each point, the error will be multiplied by the weight of the point. The estimator will try to minimize error on the more heavily weighted classes, because they will have a greater effect on error, sending a stronger signal. Without weights set, the model treats each point as equally important. Example: Logistic regression Loss=1NN∑i=1(−(yilog(̂yi))+(1−yi)log(1−̂yi))WeightedLoss=1NN∑i=1(−w0(yilog(̂yi))+w1(1−yi)log(1−̂yi))\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 29, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Importace weighting\\n31\\nIn supervised machine learning, it is important to train an estimator on balanced data so the model is equally informed on all classes. To balance the classes, we can inform the estimator to adjust how it calculates loss. Using weights, we can force as estimator to learn based on more or less importance (‘weight’) given to a particular class. Weights scale the loss function. As the model trains on each point, the error will be multiplied by the weight of the point. The estimator will try to minimize error on the more heavily weighted classes, because they will have a greater effect on error, sending a stronger signal. Without weights set, the model treats each point as equally important. Example: Multiclass ', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 30, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Importace weighting\\n32\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 31, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Label Shift\\n33 changes, but for a given output,  is the same. Output distribution changes but for a given output, input distribution stays the same.P(Y)P(X|Y)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 32, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Label Shift\\n34 changes, but for a given output,  is the same. Output distribution changes but for a given output, input distribution stays the same. Exemple: •Predicts  •The prevalence of diseases,  , are changing over time.P(Y)P(X|Y)P(Y=disease|X=symptoms)P(Y)P(Y|X)=P(X|Y)P(Y)P(X)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 33, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Concept Drift\\n35 remains the same, but  changes. Same input, expecting different output.P(X)P(Y|X)\\nConcept shift on soft drink names in the United States.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 34, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Concept Drift\\n36 remains the same, but  changes. Same input, expecting different output. Example (non stationary distribution): •Predicts P(€|features of a house in BCN) •P(features of a house in BCN) remains the same. •Covid causes people to leave BCN, housing prices drop. •P(€1M | features of a house in BCN):  •Pre-covid: high •During-covid: lowP(X)P(Y|X)', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 35, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Other drifts: Bergson’s paradox\\n37US Universities pick students based on a number of attributes.  Two commonly considered attributes are high school GPA and SAT scores.  We want to measure the correlation GPA-SAT by using data from a random school. The prior hypothesis is that there is a positive correlation… The SAT is a standardized test widely used for college admissions in the United States.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 36, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Other: Bergson’s paradox\\n38The admissions committee accepts students who have either a sufﬁciently high GPA, a sufﬁciently high SAT score, or some combination of the two.  However, applicants who have both high GPAs and high SAT scores will likely get into a higher-tier school and not attend, even if they are accepted. \\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 37, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Other: Bergson’s paradox\\n39Data show a downward trend (negative correlation) even though the overall population (red and blue dots) show an upward trend (positive correlation). This trend reversal is the \"paradox,\" though there is nothing truly paradoxical about it.\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 38, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Other: Bergson’s paradox\\n40 change!P(X),P(Y),P(Y|X),P(X|Y)\\n', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 39, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='How to detect Data Distribution Shifts?\\n41Data distribution shifts are only a problem if they cause your model’s performance to degrade. You have to monitor your model’s accuracy related metrics! ', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 40, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='How to detect Data Distribution Shifts?\\n42How to determine that two distributions are different? 1.Compare statistics: mean, median, variance, quantiles, skewness, kurtosis,…  How: Compute mean & variance of a feature during training and compare them to the same values computed in production. Not universal: only useful for distributions where these statistics are meaningful. Inconclusive:  if statistics differ, distributions differ. If statistics are the same, distributions can still differ. 2.Two-sample hypothesis test.  How: Determine whether the difference between two populations is statistically signiﬁcant (using the Kolmogorov-Smirnov test). Doesn’t make assumptions about distribution. Only works with one-dimensional data.', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 41, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='How to address Data Distribution Shifts?\\n431.Train model using a massive dataset (hopefully including diverse data distributions). 2.Retrain model with new data from new distribution (ﬁne-tuning).  Need to ﬁgure out not just when to retrain models, but also how and what data. ', metadata={'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf', 'page': 42, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf', 'description': nan}),\n",
       " Document(page_content='Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science   Bias and Discrimination I 4', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 0, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Index 1. Bias and Discrimination. 2. The human factor. 3. Automated Discrimination. 4. Case Analysis: Recividism risk.\\n2', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 1, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Bias and discrimination\\n3', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 2, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='What do we mean by “data bias”? \\n4The common deﬁnition of data bias is that the available data is not representative of the population or phenomenon of study.  But bias also denotes: •Data includes content which may contain bias against speciﬁc groups of people.Except for data acquired by a carefully designed randomized sampling process, most organically produced datasets are biased.\\nEthical Issue!', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 3, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='What do we mean by “algorithmic bias”? \\n5Algorithmic bias describes systematic deviation in output/performance or impact, relative to some norm or standard.  Example: many universities use data from past students to build models for predicting student success, where those models can support informed changes in policies and practices.World observationOutputImpact', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 4, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='What do we mean by “algorithmic bias”? \\n6\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 5, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='What do we mean by “algorithmic bias”? \\n7Algorithmic bias describes systematic deviation in output/performance or impact, relative to some norm or standard.  An algorithm can be statistically XOR ethically biased.  Example:  - Our algorithm will be statistically biased if predictions differ systematically from previously observed data. - Our algorithm will be ethically biased if predictions depend on the gender of the student.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 6, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='What do we mean by “algorithmic bias”? \\n8Not all statistically biased behaviors are ethically problematic, while not all statistically unbiased behaviors are ethically acceptable. Example: An AI system for hiring, trained with historical data, can be statistically unbiased and ethically problematic.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 7, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n9https://pair.withgoogle.com/explorables/hidden-bias/\\nX\\nYThe Dataset (x,y)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 8, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n10https://pair.withgoogle.com/explorables/hidden-bias/\\nNaive Predictor  ̂y=x\\nŶY', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 9, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n11https://pair.withgoogle.com/explorables/hidden-bias/\\nY\\nPredictor  ̂y=𝔼(y|x)=ax+b\\n̂Y\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 10, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n12https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x1,x2,…,xK)\\n̂Y\\nY', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 11, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n13https://pair.withgoogle.com/explorables/hidden-bias/\\n̂YPredictor  ̂y=𝔼(y|x1,x2,…,xK,…,xn)\\nY', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 12, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n14https://pair.withgoogle.com/explorables/hidden-bias/\\nOverpredicting outcomes for men and underpredicting for women can have signiﬁcant ethical implications, but it is not the only unethical bias we can found. Higher outcome variance for women can also be an issue from the perspective of “quality of service”.̂Y\\nY', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 13, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n15https://pair.withgoogle.com/explorables/hidden-bias/\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 14, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n16https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x)\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 15, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n17https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x)\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 16, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n18https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x1,x2,…,xK)\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 17, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Sources of Bias\\n19\\nData Generation\\nSampleWorld as it isSampleDataset\\nModelLearningEvaluationModel deﬁnition\\nDecisionsML model life cycle\\nIdeal and Possible World\\nMeasurement\\nWorld as it isFeedback\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 18, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content=\"20Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDecisionsTraining & Test\\nThe features we use do not represent the phenomenon we are studying,Sources of Bias\\nThe data we use do not represent the population,Inductive bias describes the tendency for a system to prefer a certain set of generalizations over others that are equally consistent with the observed data.Aggregation bias occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group.Simpson’s ParadoxStructural bias refers to the social/institutional patterns and practices that confer advantage to some and disadvantage to others based on identity.When evaluating a model, metrics calculated against an entire test or validation set don't always give an accurate picture of how the model works. Deployment bias refers, generally, to any bias that arises during deployment, where a system is used or interpreted in inappropriate ways, perhaps not intended by the designers or developers.This type of bias occurs when a model itself inﬂuences the generation of data that is used to train it.\", metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 19, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='21Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasSexism, racism, socio-economic status, etc.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 20, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='22Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasSexism, racism, socio-economic status, etc.An optimal predictor can be unfair!', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 21, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='23Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIndirect effect: Unbalanced Dataset\\nAggregation  BiasSexism, racism, socio-economic status, etc.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 22, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='24Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasDirect effect:  If there is systemic racism in a university that impacts student success,  there is label bias.Sexism, racism, socio-economic status, etc.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 23, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='25Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 24, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='26What about applications that aren’t about people?  Consider “Street Bump,” a project by the city of Boston to crowdsource data on potholes.  The smartphone app automatically detects pot holes using data from the smartphone’s sensors and sends the data to the city. Infrastructure seems like a comfortably boring application of data-driven decision-making, far removed from the ethical quandaries we’ve been discussing. But the data reﬂects terms of smartphone ownership, which are higher in wealthier parts of the city compared to lower-income areas and areas with large elderly populations.  \\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 25, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='27Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasStudent success can be speciﬁed in terms of many different variables that do not represent in a fair way all groups: grades, employer prestige, post-graduate salary, etc.\\nAggregation  Bias', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 26, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='28Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasStudent success can be speciﬁed in terms of many different variables that do not represent in a fair way all groups: grades, employer prestige, post-graduate salary, etc.\\nAggregation  BiasReality Gap: “real” data are not necessarily equivalent to “reality”The features we use determine what real patterns can be detected!', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 27, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='29Recommended Reading:  Measurement and Fairness, by Abigail Z. Jacobs, Hanna Wallach https:/ /arxiv.org/abs/1912.05511Measuring almost any attribute about people is similarly subjective and challenging: teacher effectiveness, economic status, etc.\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 28, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='30Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasThe choice of the objective function is not ethically neutral: we could minimize “prediction errors” or minimize “predictions of no-help-needed for students who truly need help”.\\nAggregation  Bias', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 29, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='31Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasSometimes, there is no “ground truth” to label data. In many applications the system can do little more than mimic the agreement amongst domain “experts”.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 30, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='32Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIf users care about students who are most likely to have low grades but the algorithm is optimized to identify those likely to drop-out, the output will not provide the right information. \\nAggregation  Bias', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 31, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='33Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIf users care about students who are most likely to have low grades but the algorithm is optimized to identify those likely to drop-out, the output will not provide the right information. \\nAggregation  BiasDistribution Shifts', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 32, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='34Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasPopularity bias is an undesirable phenomenon associated with recommendation algorithms where popular items tend to be over-recommended over long-tail ones.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 33, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='35We have seen bad biases, biases that are problematic from an ethical point of view because they conﬁgure the distribution of goods, services, risks, and opportunities, or even access to information in ways that are problematic. But there are biases that are inevitable, that enable ML.Every Bias Is Not a Bad Bias', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 34, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Every Bias Is Not a Bad Bias\\n36\\n1980:  Bias in ML does help us generalize better and make our model less sensitive to some single data point.Bias is a need to generalize!', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 35, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Every Bias Is Not a Bad Bias\\n37Deﬁnition: a hypothesis space is the set of mathematical functions  (hypotheses) that are tested against the training data, based on the assumption that relevant (real) patters can be expressed by way of a mathematical function, called the target function. The learning algorithm cannot uncover patterns that are not described in one of the hypotheses. fW', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 36, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Every Bias Is Not Necessarily a Bad Bias\\n38\\nA quadratic pattern cannot be seen by a linear model.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 37, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='39The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions (in terms of the hypothesis space, ) that the learner uses to predict outputs of given inputs that it has not encountered.  fW\\nEvery Bias Is Not Necessarily a Bad Bias', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 38, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='40The inductive bias  is inevitable and, though neither good nor bad in itself, but it is not neutral in real world settings: pattern blindness can result in winners and losers!Every Bias Is Not Necessarily a Bad Bias\\nAgeOverpredictionOverpredictionUnderpredictionModel', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 39, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='41We’ve seen that training data reﬂects the disparities, distortions, and biases from the real world and the measurement process.  Some patterns in the training data (“smoking is associated with cancer”) represent knowledge that we wish to mine using machine learning, while other patterns (“girls like pink and boys like blue”) represent stereotypes or bad habits that we might wish to avoid learning. Algorithm ethical assessment', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 40, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='42But learning algorithms have no general way to distinguish between these two types of patterns, because they are the result of social norms and moral judgments.  This leads to an obvious question: when we learn a model from such data, are these disparities preserved, mitigated, or exacerbated?Algorithm ethical assessment', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 41, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Impact of bad biases\\n43', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 42, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='We ﬁnd that lenders charge Latin/African-American borrowers 7.9 and 3.6 basis points more for purchase and reﬁnance mortgages respectively, costing them $765M in aggregate per year in extra interest.   FinTech algorithms also discriminate, but 40% less than face-to-face lenders.  The lower levels of price discrimination by algorithms suggests that removing face-to-face interactions can reduce discrimination.  44\\nDiscrimination: Unjustiﬁed basis of differentiation between individuals\\nBad News!Bias ampliﬁcationImpact of bad biases', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 43, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='45Impact of bad biases', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 44, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='46X-ray image datasets used to diagnose various thoracic diseasesImpact of bad biases', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 45, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Measuing bias is difficult\\n47\\nProblem: police discrimination analysisData:and sometimes impossible', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 46, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Measuing bias is difficult\\n48\\nProblem: police discrimination analysisData:If the rate of using force against stopped Black people and the rate of using force against stopped white people are the same, can we conclude that we are observing a fair behavior? ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 47, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='49Measuing bias is difficult', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 48, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='50Measuing bias is difficult\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 49, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='51Measuing bias is difficult\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 50, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='52Measuing bias is difficult\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 51, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='53Measuing bias is difficult\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 52, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Discrimination\\n54\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 53, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='55Under the most advanced law systems, everyone is protected from unlawful behavior (discrimination) when the cause of this behavior is that they have or are perceived to have a “protected  characteristic” or are associated with someone who has a protected characteristic: •Age •Disability •Gender •Civil state •Pregnancy and maternity •Race •Religion and belief •Sex •Sexual orientationLaw & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 54, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='56https://fairmlbook.org/Be careful! In many classiﬁcation tasks, available data contain protected characteristics of an individual.  Some have hoped that removing or ignoring protected attributes would somehow ensure the impartiality of the resulting classiﬁer. Unfortunately, this practice is usually somewhere on the spectrum between ineffective and harmful. In a typical data set, we have many features that are slightly correlated with the sensitive attribute.  However, if numerous such features are available, as is the case in a typical browsing history, the task of predicting gender becomes feasible at high accuracy levels.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 55, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='But, isn’t discrimination the very point of machine learning? Yes, but it is not admissible when this discrimination/differentiation is based on unjustiﬁed causes, is practically irrelevant or is morally wrong.  Discrimination is not a general concept, it’s domain and feature speciﬁc! \\n57ENGINEER POINT OF VIEW: THAT’S NOT MY BUSINESS!\\nWE NEED A CASE BY CASE ANALYSIS FAIRNESS CANNOT BE AUTOMATED', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 56, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='58There are several types of discrimination: 1.Direct discrimination . This means treating someone less favorably than someone else because of a protected characteristic.  2.Direct discrimination by perception. This means treating one person less favorably than someone else, because you incorrectly think they have a protected characteristic. 3.Discrimination arising from disability.  This means treating a disabled person unfavorably because of something connected with their disability when this cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favorably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behavior related to a protected characteristic which has the purpose or effect of violating  someone’s dignity or which creates a hostile, degrading, humiliating or offensive environment.https://www.equalityhumanrights.com/sites/default/ﬁles/ea_legal_deﬁnitions_0.pdfLaw & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 57, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='59Disparate treatment or direct discrimination:  Treatment depends on class membershipDisparate impact or indirect discrimination: Outcome depends on class membershipLaw & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 58, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='60https://www.equalityhumanrights.com/sites/default/ﬁles/ea_legal_deﬁnitions_0.pdfAn employer does not interview a job applicant because of the  applicant’s ethnic backgroundA hair salon owner has a policy of not employing stylists who  cover their hair, believing it is important for them to exhibit their  ﬂamboyant haircuts.An employer dismisses a worker because she has had three  months’ sick leave. The employer is aware that the worker  has multiple sclerosis and most of her sick leave is disability- related.An employer offers ﬂexible working to all staff. Requests are  supposed to be considered based on business need. A manager  allows a man’s request to work ﬂexibly to train for a qualiﬁcation  but does not allow another man’s request to work ﬂexibly to care  for his disabled child.An employer has a policy that designated car parking spaces are  only offered to senior managers. A worker who is not a manager,  but has a mobility impairment is not given a designated car parking space. A builder addresses abusive and hostile remarks to a customer  because of her race after their business relationship has ended.1  1.Direct discrimination . This means treating someone less favourably than someone else because of a protected characteristic.  2.Direct discrimination by perception. This means treating one person less favourably than someone else, because you incorrectly think they have a protected characteristic. 3.Discrimination arising from disability.  This means treating a disabled person unfavourably because of something connected with their disability when this cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favourably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behaviour related to a protected characteristic which has the purpose or effect of violating  someone’s dignity or which creates a hostile, degrading, humiliating or offensive environment.2  3  4  5  6  Law & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 59, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='61Algorithmic discrimination scenarios: •Access to employment •Access to education •Access to government/companies beneﬁts •Access to penitentiary alternatives •Etc. Anti-discrimination legislation typically seeks equal access/treatment (mitigation of direct discrimination) to employment, working conditions, education, social protection, goods, and services, but in some cases, equal outcome is also sought (mitigation of indirect discrimination).  Law & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 60, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='In general, anti-discrimination laws aim to achieve equality of opportunity. Narrow notions of equality of opportunity are concerned with ensuring that decision-making treats similar people similarly on the basis of relevant features, given their current degree of similarity. Broader notions of equality of opportunity are concerned with organizing society in such a way that people of equal talents and ambition can achieve equal outcomes over the course of their live. Somewhere in between is a notion of equality of opportunity that forces decision-making to treat seemingly dissimilar people similarly, on the belief that their current dissimilarity is the result of past injustice.62Law & Discrimination', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 61, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n63\\nhttps://www.cs.cmu.edu/~mtschant/ife/', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 62, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Example\\n64\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 63, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content=' The human factor ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 64, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Our brains are evolved to help us survive. That means they take a lot of shortcuts to help us get through the day. These shortcuts, or heuristics, are vital. But they come at a cost. Our world is much more complex than the world our brains developed these heuristics. Unconscious brains can be unreliable in this environment. Our unconscious can helps us in some situations, but it is not always the right tool. We must be sure that it will not hurt others.Human Biases\\n66The halo effect People who looks healthy or attractive are also competent and good. Reading:  Physiognomy’s New Clothes, by Blaise Agüera y Arcas, Margaret Mitchell and Alexander Todorov. ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 65, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Unconscious Human Biases\\n67\\nhttps://www.visualcapitalist.com/wp-content/uploads/2017/09/cognitive-bias-infographic.html', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 66, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='We know that human decision-making is affected by: •Unconscious thoughts, biases, etc. •Unthinking custom and practice, or unconsciously absorving beliefs of our friends, family, society, etc. •Personal ethical decision making proﬁle. F.e. you prioritize relationships in your decicion-making. •Reﬂective practice, to consider context and the people who will be affected by your decisions. Human decision making\\n68The role of ethics is to have a toolkit to do reﬂective practice, and to able of making and justiﬁying our decisionsMINDPERSONAL HISTORYDEFAULT SETTING', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 67, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content=' Automated Discrimination ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 68, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Algorithmic Fairness\\n70Algorithm fairness is the ﬁeld of research aimed at understanding and correcting unwanted biases. Speciﬁcally, it includes: •Researching the causes of bias in data and algorithms •Deﬁning and applying measurements of fairness •Developing data collection and modelling methodologies aimed at creating fair algorithms. ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 69, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='71We can distinguish between two approaches to formalizing fairness: – Individual fairness deﬁnitions are based on the premise that similar entities should be treated similarly. – Group fairness deﬁnitions are based on the deﬁnition of group entities and ask that all groups are treated similarly. To operationalize both approaches to fairness, we need to deﬁne similarity for the input and the output of an algorithm.   For group fairness, the challenge lies in determining how to partition entities into groups (protected attributes)How to measure fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 70, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='72Individual fairness.  discriminates against  in relation to  if: • has property  and  does not have . • treats worse  than she treats  and this is because  has  and  does not have . XYZYPZPXYZYPZPHow to measure fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 71, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='73Group fairness.  group-discriminates against someone, , in relation to another, , by -ing (e.g., hiring  rather than ), if: •There is a property, , such that  has  or  believes that  has , and  does not have  or  believes that  does not have , •  treats  worse than he treats or would treat  by  -ing. •It is because ( believes that)  has  and ( believes that)  does not have  that  treats  worse than  by -ing” and, •  is the property of being a member of a certain socially salient group (to which  does not belong). XYZΦZYPYPXYPZPXZPXYZΦXYPXZPXYZΦPZHow to measure fairness\\nLippert-Rasmussen K (2014) Born free and equal?: a philosophical inquiry into the nature of discrimination. In: Oxford university press, Oxford, New York', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 72, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='74One way of formulating individual fairness is a distance-based one.  Given a distance measure d between two entities and a distance measure D between the outputs of an algorithm, we would like the distance between the output of the algorithm for two entities to be small, when the entities are similar. Fairness Definitions', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 73, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='75Another form of individual fairness is counterfactual fairness.  An output is fair toward an entity if it is the same in both the actual world and a counterfactual world where the entity belonged to a different group.  Given that Alice did not get promoted in her job, and given that she is a woman, and given everything else we can observe about her circumstances and performance, what is the probability of her getting a promotion if she was a man instead? Causal inference is used to formalize this notion of fairness.Fairness Definitions', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 74, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='76For simplicity, let us assume two groups, namely the protected group  (f.e. women) and the non-protected (or, privileged) group  (f.e. men) and a binary classiﬁer.  We will start by presenting statistical approaches commonly used in classiﬁcation. Assume that  is the actual and  the predicted output of the binary classiﬁer, that is,  is the “ground truth”, and  the output of the algorithm.  Let 1 be the positive class that leads to a favorable decision, e.g., someone getting a loan, or being admitted at a competitive school, and  be the predicted probability for a certain classiﬁcation.G+G−ŶYŶYSFairness Definitions\\nThere are equivalent frameworks for regression and ranking.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 75, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='77Statistical approaches to group fairness can be distinguished as: •Base rates approaches: that use only the output  of the algorithm, •Accuracy-based approaches: that use both the output  of the algorithm and the ground truth , and •Calibration approaches: that use the predicted probability  and the ground truth . ̂ŶYYSYFairness Definitions', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 76, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='78Base rate fairness compares (ratio or difference)  •the probability  that an entity  receives the favorable outcome when  belongs to the protected group  •with the corresponding probability  that  receives the favorable outcome when  belongs to the non-protected group.  When the probabilities of a favorable outcome are equal for the two groups, we have a special type of fairness termed demographic, or statistical parity:  P(̂Y=1|X∈G+)XXP(̂Y=1|X∈G−)XXP(̂Y=1|X∈G+)∼P(̂Y=1|X∈G−)Base rate fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 77, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='79In a more general setting we can deﬁne demographic parity in terms of statistical independence: the protected characteristic must be statistically independent of the outcome.  independent of the protected characteristic for all groups  and all values : ̂Ya,bdBase rate fairness\\np(̂Y=d|X∈a)=p(̂Y=d|X∈b)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 78, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='80Base rate fairness ignores the actual output. For example, assume that the classiﬁcation task is getting or not a job and the protected group  is based on gender.  Statistical parity asks for a speciﬁc ratio of women in the positive class, even when there are not that many women in the input who are well qualiﬁed for the job. G+Base rate fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 79, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='81Base rate fairness\\nWhat is the 80% Rule?  The rule states that companies should be hiring protected groups at a rate that is at least 80% of that of white men. The 80% rule was created to help companies determine if they have been unwittingly discriminatory in their hiring process. \\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 80, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Let’s assume we’re building an application to select promising candidates for a job. Our model will aim to learn the typical proﬁle of those who can be hired. In this example we get demographic parity: We must take into account that: •Demographic parity can reject the optimal classiﬁer. Base rate fairness\\n82Dataset, Positive OutcomêY=1Acceptance rate: 6/24Class A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | NegativeAcceptance rate: 4/16Reminder: P(̂Y=1|X∈G)=P(̂Y=1,X∈G)P(X∈G)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 81, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Decisions based on a classiﬁer that satisﬁes independence can have undesirable properties (and similar arguments apply to other statistical criteria). Imagine a company that in group  hire diligently selected applicants at some rate .  In group , the company hires carelessly selected applicants at the same rate .  Eventhough the acceptance rates in both groups are identical, it is far more likely that unqualiﬁed applicants are selected in one group than in the other.  As a result, it will appear in hindsight that members of group B performed worse than members of group A, thus establishing a negative track record for group B.Ap>0BpWarning!\\n83https://fairmlbook.org/', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 82, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='84Accuracy-based fairness warrants that various types of classiﬁcation errors (e.g., true positives, false positives) are equal across groups. Depending on the type of classiﬁcation errors considered, the achieved type of fairness takes different names. Accuracy-based fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 83, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='85Accuracy-based fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 84, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='86Accuracy-based fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 85, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='87The case in which we ask that   (same True Positive Rate) is called equal opportunity. Comparing equal opportunity with statistical parity, again the members of the two groups have the same chance of getting the favorable outcome, but only when these members qualify. P(̂Y=1|Y=1,X∈G+)=P(̂Y=1|Y=1,X∈G−)Accuracy-based fairness\\n Reminder: P(̂Y=1|Y=1,X∈G)=P(̂Y=1,Y=1,X∈G)P(Y=1,X∈G)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 86, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='88In the general case, this method can be called separation:  must be independent of  the protected characteristic, conditional on . Separation acknowledges that in many scenarios, the sensitive characteristic may be correlated with the target variable.   A bank might argue that it is a matter of business necessity  to therefore have different lending rates for these groups. For example, one group might have a higher  default rate on loans than another. Roughly speaking, the separation criterion allows correlation between the score and the sensitive attribute to the extent that it is justiﬁed by the target variable. ̂YYAccuracy-based fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 87, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='89Accuracy-based fairnessThe case in which we ask that is called equalized odds. All groups experience the same true positive rate and the same false positive rate.p(̂Y=1|Y=1,X∈G+)=p(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=0,X∈G+)=p(̂Y=1|Y=0,X∈G−)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 88, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='90Equalized odds requires both the fraction of non-defaulters  that qualify for loans and the fraction of defaulters that qualify for loans to be constant across groups. Accuracy-based fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 89, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='91Class A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | NegativeDatasetFPFPFPFPFNFNFN\\nFNFNFNFNFNFNFNTPRA=58FPRA=116Accuracy-based fairnesŝY=1̂Y=0TPRB=18FPRB=38', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 90, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='In many applications (e.g. hiring), people care more about the true positive rate than false positive rate so many works focus on equal of opportunity:\\n92p(̂Y=1|Y=1,X∈G+)=p(̂Y=1|Y=1,X∈G−)\\nFPFPFPFPFNFNFN\\nFNFNFNFNFNFNFNTPRA=58TPRB=18Accuracy-based fairness\\n̂Y=1̂Y=0', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 91, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Separation may not help closing the gap between two groups in the real world.  For example, imagine group A has 100 applicants and 58 of them are qualiﬁed while group B also have 100 applicants but only 2 of them are qualiﬁed.  If the company decides to accept 30 applicants and satisﬁes equality of opportunities, 29 offers will be conferred to group A while only 1 offer will be conferred to group B.  If the job is a well-paid job, group A tends to have a better living condition and affords better education for their kids, and thus enable them to be qualiﬁed for such well-paid jobs when they grow up. The gap between group A and group B will tend to be enlarged over time.93Accuracy-based fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 92, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='The criteria we reviewed constrain the joint distribution  in non-trivial ways. We should therefore suspect that imposing any two of them simultaneously over-constrains the space to the point where only degenerate solutions remain.  It can be shown that if we assume that  is binary, the protected feature is not independent of , and  is not independent of , then, independence and separation cannot both hold. It is impossible to satisfy all deﬁnitions of group fairness, meaning that the data scientists need to choose one to refer to when starting a fairness analysis.P(X,Y,̂Y)YŶYYRelationships between criteria\\n94', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 93, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Relationships between criteria\\n95Incompatibility of fairness metrics doesn’t imply that fairness efforts are fruitless.  Instead, it suggests that fairness must be deﬁned contextually for a given ML problem, with the goal of preventing harms speciﬁc to its use cases.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 94, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='For binary decision procedures, we can summarize a procedure with the confusion matrix, which illustrates match and mismatch between decision  and true status .̂YYFairness for decisions\\n96\\nConfusion MatrixDemographic Parity:p(̂Y=1|X∈G+)=p(̂Y=1|X∈G−)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 95, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='For binary decision procedures, we can summarize a procedure with the confusion matrix, which illustrates match and mismatch between decision  and true status .̂YYFairness for decisions\\n97\\nConfusion MatrixFor any box in the confusion matrix involving the decision , we can deﬁne fairness as equality across groups. For example, Equal False Omission Rates: dp(Y=1|̂Y=0,X∈G+)=p(Y=1|̂Y=0,X∈G−)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 96, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Some machine learning systems produce scores instead of labels, f.e. probabilistic classiﬁers, recommenders, etc. Some of the measures we have seen can be generalized to scores.Fairness for scores\\n98', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 97, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='For score outputs, we can consider the following initial deﬁnitions of fairness based on equal metrics across groups: •Balance for the Positive Class: the average score assigned to positive members, , should be the same across groups.  •Balance for the Negative Class: the average score assigned to negative members, , should be the same across groups.  •Calibration: the fraction of those marked with a given score who are actually positive, , should be the same across groups.  •AUC (Area Under Curve) Parity: the area under the receiver operating characteristic (ROC) curve should be the same across groups. The AUC can be interpreted as the probability that a randomly chosen positive individual  is scored higher than a randomly chosen negative individual. 𝔼(S|Y=1)𝔼(S|Y=0)𝔼(Y=1|S=d)Y=1Fairness for scores\\n99', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 98, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='100Calibration-based fairness considers probabilistic classiﬁers that predict a probability  for each class. In general, a classiﬁcation algorithm is considered to be well-calibrated if:  when the algorithm predicts a set of individuals as having probability  of belonging to the positive class, then approximately a  fraction of this set is actual members of the positive class.  In terms of fairness, intuitively, we would like the classiﬁer to be equally well calibrated for both groups.pppFairness for scores', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 99, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='101CalibrationTo get an intuitive understanding of how well a speciﬁc model performs in this regard, Realiability Diagramms are often used.\\nAccuracyConﬁdence', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 100, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='102CalibrationThe Expected Calibration Error (ECE) simply takes a weighted average over the absolute accuracy/conﬁdence difference.\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 101, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='103Calibration-based fairness is asking that for any predicted probability score , the probability of positives among those with a given score is equal for both groups, i.e., p∈[0,1]Calibration-based fairnessP(Y=1|S=p,X∈G+)=P(Y=1|S=p,X∈G−)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 102, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='104P(Y=1|S=p,X∈G+)=P(Y=1|S=p,X∈G−)\\nCalibration by gender on UCI adult data. A straight diagonal line would correspond to perfect calibration.\\nCalibration by race on UCI adult data.The fraction of those marked with a given score who are actually positive should be the same across groups.Calibration-based fairness', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 103, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='105Online Example\\nhttps://research.google.com/bigpicture/attacking-discrimination-in-ml/Attacking discrimination with  smarter machine learning or why fairness is part of a multi-objective task.  ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 104, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='106Group-based measures in general tend to ignore the merits of each individual in the group.  Some individuals in a group may be better for a given task than other individuals in the group, which is not captured by some group-based fairness deﬁnitions. Limitations', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 105, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='107This issue may lead to two problematic behaviors, namely,  (a) the self-fulﬁlling prophecy where by deliberately choosing the less qualiﬁed members of the protected group we aim at building a bad track record for the group, and  (b) reverse tokenism where by not choosing a well qualiﬁed member of the non-protected group we aim at creating convincing refutations for the members of the protected group that are also not selected.Limitations', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 106, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='‘Bias preserving’ fairness metrics seek to reproduce historic performance in the outputs of the target model with equivalent error rates for each group as reﬂected in the training data (or status quo).  F.e. Equal FPR In contrast, ‘bias transforming’ metrics do not blindly accept social bias as a given or neutral starting point that should be preserved, but instead require people to make an explicit decision as to which biases the system should exhibit.   F.e. Demographic parity  Bias preservation or transformation?\\n108p(̂Y=1|Y=0,X∈G+)=p(̂Y=1|Y=0,X∈G−)\\np(̂Y=1|X∈G+)=p(̂Y=1|X∈G−)', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 107, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Bias preserving criteria are always satisﬁed by a perfect classiﬁer that exactly predicts its target labels with zero error, replicating bias present in the data. Bias transforming metrics are not necessarily satisﬁed by a  perfect classiﬁer. \\n109Bias preservation or transformation?', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 108, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='110\\nBias preservation or transformation?', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 109, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='111DEMOGRAPHIC DISPARITY (DD) Is the disadvantaged class a bigger proportion of the rejected outcomes than the proportion of accepted outcomes for the same class? For example, in the case of college admissions, if women applicants comprised 40% of the rejected applicants and comprised only 30% of the accepted applicants, we say that there is demographic disparity because the rate at which women were rejected exceeds the rate at which they were accepted. DD=P(X∈G+|̂Y=0)−P(X∈G+|̂Y=1)Bias transforming\\nThis applies to cases were we can accept different a priori preferences between groups.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 110, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='112DD=P(X∈a|̂Y=0)−P(X∈a|̂Y=1)Rejected = 8 /20 = 40%  Accepted = 3/10 = 30%DD=0.4−0.3=0.1Bias transformingWomanMan', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 111, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='113CONDITIONAL DEMOGRAPHIC DISPARITY (CDD) We can condition DD on attributes that deﬁne a strata of subgroups on the dataset.  This is necessary to rule out Simpson’s paradox (a problem that appears when aggregating data). Example: Graduate school admissions to University of California, Berkeley.\\n44% of the male applicants were accepted compared to only 35% of female applicants… Is there discrimination?Bias transforming', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 112, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='114Graduate school admissions to University of California, Berkeley.\\nAccepted women: 35% of 4231 = 1481 Accepted applicants: 44% of 8442 + 35% of 4231 = 5195 Non accepted women: 65% of 4231 = 2750 Non accepted applicants: 56% of 8442 + 65% of 4231 = 7478 DD = 2750/7478 - 1481/5195 = 0.08 There is evidence of (small) demographic disparity.Bias transforming', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 113, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='115However, when examining the individual departments, it appeared that 6 out of 85 departments were signiﬁcantly biased against men, while 4 were signiﬁcantly biased against women. The issue was that women were much more likely to apply to more competitive departments (such as English) that were much more likely to reject graduates of any gender, whereas other departments (such as Engineering) were more lenient. In the language of demographic parity: although Berkeley’s pattern of admission exhibited evidence of demographic disparity, once we condition according to “department applied for”, the apparent bias disappears.Bias transforming', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 114, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='116Let’s consider these dataset: \\nDD=0.46−0.32=0.14Bias against women.Bias transforming', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 115, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='117Let’s consider these tables: \\nBias in favour of menBias transforming\\nBias in favour of women\\nSimpson’s Paradox  is a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations. ', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 116, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='118The Conditional Demographic Disparity metric gives a single measure for all the disparities found in the subgroups deﬁned by an attribute (f.e. department) by averaging (each subgroup weighted in proportion to the number of observations it contains) them. \\nSmall bias in favour of women!CDD=1n∑iniDDini:Number of observations for each subgroupn:Total number of observationsBias transforming\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 117, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='119Individual Fairness: Distance, Counterfactual. Demographic parity: • Equal opportunity,  • Equalized odds: • • Calibration: • (Conditional) Demographic disparity: •P(̂Y=1|X∈G+)∼P(̂Y=1|X∈G−)P(̂Y=1|Y=1,X∈G+)∼P(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=1,X∈G+)∼p(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=0,X∈G+)∼p(̂Y=1|Y=0,X∈G−)P(̂Y=1|S=p,X∈G+)∼P(̂Y=1|S=p,X∈G−)P(X∈G+|̂Y=0)∼P(X∈G+|̂Y=1)Fairness Metrics Summary : Protected groupG+ : Non-protected groupG−', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 118, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='The criminal justice system needs to evaluate a diverse set of risks:  •The risk of committing a new crime after an arrest (recidivism), •The risk of committing a new violent crime (violent recidivism), •The risk of committing an act of violence against another inmate or penitentiary personnel in jail (intra-penitentiary violence), •The risk of committing an administrative violation such as breaking the conditions of a permit. •Etc.Case Analysis: Recidivism risk\\n120', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 119, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Structured risk assessment corresponds to a family of methodologies for evaluating these risks using a systematic process, typically in which a number of different items are evaluated.  We can train a ML system to make automatic decisions based on the scores in each item, but most often, a professional makes a decision based on his/her own evaluation of a defendant and the result of a series of items.\\n121Measuring recidivism risk', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 120, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='COMPAS (Correctional Offender Management Proﬁling for Alternative Sanctions) is an automated tool that outputs numerical scores, which are labeled, for example, “risk of recidivism”, “risk of violent recidivism”, or “risk of failure to appear”.  These scores are then used in an unspeciﬁed way to make decisions of jail, bail, home arrest, release, etc. Measuring recidivism risk\\n122Bail: the temporary release of an accused person awaiting trial.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 121, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='In May 2016, ProPublica, an investigative journal, published a piece called \"Machine Bias\", in which COMPAS, was found to be biased against blacks. \\n123\\nMeasuring recidivism risk', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 122, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='124\\nMeasuring recidivism risk\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 123, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='125Measuring recidivism risk: the debateTwo of their ﬁndings of ProPublica can be phrased in our language as follows: •COMPAS does not satisfy equal false negative rates, in fact, white defendants who did get rearrested () were nearly twice as likely to be misclassiﬁed as low risk ().  •COMPAS does not satisfy equal false positive rates, in fact, black defendants who did not get rearrested () were nearly twice as likely to be misclassiﬁed as higher risk ().  Y=1̂Y=0Y=0̂Y=1', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 124, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='126Accuracy-based fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 125, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='127Measuring recidivism riskBut the developers did not agree…\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 126, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='128Measuring recidivism risk\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 127, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='129Measuring recidivism risk: the debateIn their response, Equivant/Northpointe, the developers of COMPAS, cited two articles ﬁnding that: •COMPAS satisﬁes calibration: scores mean the same thing regardless of the defendant’s race. For example, among defendants with a score of 7, 60 percent of white defendants were rearrested and 61 percent of black defendants were rearrested.  •It can be shown that calibration implies equal (positive and negative) predictive values (but not the other way around):  •among those labeled higher risk (), the proportion of defendants who got rearrested () is approximately the same regardless of race. •among those labeled lower risk (), the proportion of defendants who did not get rearrested () is approximately the same regardless of race.̂Y=1Y=1̂Y=0Y=0', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 128, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='130Accuracy-based fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 129, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='131Accuracy-based fairness\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 130, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='132Measuring recidivism risk: analysis\\nhttps://allendowney.github.io/RecidivismCaseStudy/', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 131, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='133Bias MitigationThe ﬁeld of bias mitigation strategies can be categorised into three types:  •Pre-processing methods manipulate the data to eliminate bias before a machine learning (ML) model is able to incorporate these biases based on the data.  •In-processing bias mitigation strategies manipulate the model to mitigate bias that appears during the training process.  •Post-processing methods alter the outcomes of a model, preying on bias present in the output.\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 132, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='134Bias MitigationPre-processing techniques •Reweighing Pre-Processing: Generates weights for the training samples in each (group, label) combination  differently  to  ensure  fairness  before  classiﬁcation. It  does  not  change  any  feature  or  label  values,  so  this  is  ideal  if you are unable to make value changes. •Optimized Pre-Processing:  Learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data ﬁdelity constraints and objectives. •Learning Fair Representations:  Finds  a  latent  representation  that  encodes  the  data  well  but obfuscates information about protected attributes.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 133, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='135Bias MitigationIn-processing techniques •Adversarial Debiasing: Learns a classiﬁer to maximize prediction accuracy and simultaneously  reduces  an  adversary’s  ability  to  determine  the  protected  attribute  from  the  predictions.  This  approach  leads  to  a fair  classiﬁer  because  the  predictions  can’t  carry  any  group  discrimination information that the adversary can exploit. •Prejudice Remover: Adds a discrimination-aware regularization term to the learning objective. •Meta Fair Classiﬁer: Takes the fairness metric as part of the input and returns a classiﬁer optimized for the metric.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 134, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='136Bias MitigationPost-processing techniques •Equalized Odds: Solves  a  linear  program  to  ﬁnd  probabilities  with  which  to change output labels to optimize equalized odds. •Calibrated Equalized Odds: Optimizes over calibrated classiﬁer score outputs to ﬁnd probabilities  with  which  to  change  output  labels  with  an  equalized odds objective. •Reject Option Classiﬁcation: Gives favorable outcomes to unprivileged groups and unfavorable outcomes to privileged groups in a conﬁdence band aroundthe decision boundary with the highest uncertainty.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 135, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='137Bias Mitigation\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 136, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='138Bias Mitigation\\n', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 137, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='139Bias Mitigation\\nhttp://aif360.mybluemix.net/', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 138, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='140ReweightingSampling, massaging, reweighing and suppression are among different pre-processing bias mitigation techniques proposed from academic literature. The advantage of reweighting is, instead of modifying the labels, it assigns different weights to the examples based upon their categories of protected attribute and outcome such that bias is removed from the training dataset. The weights are based on frequency counts.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 139, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='141ReweightingReweighting works by postulating that a fair data set  would show no conditional dependence of the outcome on a protected attribute.  Hence, it postulates  group membership and outcome should be statistically independent.   Reweighting adjusts the data point weights to make this so.DP(Y=a,X∈G)=P(Y=a)P(X∈G)=|{Y=a}||D|×|{X∈G}||D|', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 140, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='142Reweighting: Adult datasetThe binary target in our example is whether an individual has an income higher or lower than $50k.  It contains several features that are protected by the law, but for simplicity, we will focus on sex.  As can be seen in the table, Male is the privileged group with a 31% probability of having a positive outcome (>$50k) compared to an 11% probability of having a positive outcome for the Female group.\\n+p−p−np+npNegative non privilegedPositive non privilegedNegative privilegedPositive privileged', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 141, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='143Reweighting: Adult datasetUsing the frequency counts in the table, the reweighing technique will assign weights as follows:w+p=np×n+n×n+pw−p=np×n−n×n−pw−np=nnp×n−n×n−npw+np=nnp×n+n×n+np', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 142, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='144Assignment:  Recividism Analysis in CataloniaThe dataset corresponds to a set of juvenile offenders in Catalonia who were evaluated using SAVRY, a structured risk assessment tool. The data on recidivism indicates if the same people committed a new offence in 2013-2015. Objectives of the assignment: •To compare the performance of SAVRY and ML-based methods, in terms of both accuracy and fairness metrics.  •To analyze the causes of unfairness. •To explore a mitigation strategy.', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 143, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='145Equalized Base RatesLet’s suppose we have a binary decision problem  and my protected feature is . My dataset  is: I have Equal Base Rates if  and , which is not the case.  In this case I need to oversample class  in order to get  additional samples! Two of these samples will be oversampled from the positive pool. The other one must be sampled from the negative pool. The result is a new dataset : Now  and  .Y∈{−1,1}X∈{A,B}DPD(A)=PD(B)PD′ (Y|X)=PD(Y|X)B6−3=3D′ PD′ (A)=PD′ (B)PD′ (Y|X)=PD(Y|X)DatasetClass A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | Negative\\nDatasetrandomly resample the training dataset', metadata={'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf', 'page': 144, 'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'description': nan}),\n",
       " Document(page_content='Preview  – Microsoft Responsible AI Standar d v2 – Introduction  \\n \\n1 \\n \\n Microsoft \\nResponsible AI \\nStandard , v2 \\n \\nGENERAL REQUIREMENTS  \\n \\n \\nFOR EXTERNAL RELEASE  \\n \\nJune 2022  \\n \\n \\n  \\n \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 0, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n2  \\nIndex  \\n \\n \\n \\n \\n \\nAbout this release  ................................ ................................ ................................ ................................ ................................ ................................ ....... 3 \\nAccountability Goals  ................................ ................................ ................................ ................................ ................................ ................................ .. 4 \\nTransparency Goals  ................................ ................................ ................................ ................................ ................................ ................................ .... 9 \\nFairness Goals  ................................ ................................ ................................ ................................ ................................ ................................ .............  13 \\nReliability & Safety Goals  ................................ ................................ ................................ ................................ ................................ .......................  21 \\nPrivacy & Security Goals  ................................ ................................ ................................ ................................ ................................ ........................  26 \\nInclusiveness Goal  ................................ ................................ ................................ ................................ ................................ ................................ ..... 27 \\n \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 1, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n3  \\nAbout this release  \\n \\nWhen we embarked on our effort to operationalize Microsoft’s six AI principles, we knew there was a policy \\ngap. Laws and norms had not caught up with AI’s unique risks or society’s needs. Yet, our product development \\nteams needed concrete and actionable guidance as to what our principles meant and how they could uphold \\nthem . We leveraged the expertise on our research, policy, and engineering teams to develop guidance on how  \\nto fill that gap.  \\n \\nThe Responsible AI Standard is the product of a multi -year effort  to define product development requirements \\nfor responsible AI.  We are making available th is second version of the Responsible AI Standard to share what  \\nwe have learned, invite feedback from others,  and contribute to the discussion  about building better norms  \\nand practices around AI . \\n \\nWhile our Standard is an important step in Microsoft’s responsible AI journey, it is just one step. As we make \\nprogress with implementation,  we expect to encounter challenges that require us to pause, reflect, and adjust. \\nOur Standard will remain a living document, evolving to address new research, technologies, laws, and \\nlearnings from within and outside the company.  \\n \\nThere is a rich and active global dialog about how to create principled and actionable norms to ensure \\norganizations develop and deploy AI responsibly. We have benefited from this discussion and will continue to \\ncontribute to it. We believe that industry, academia, civil soci ety, and government need to collaborate to \\nadvance the state -of-the-art and learn from one another. Together, we  need to answer open research \\nquestions, close  measurement gaps, and design  new practices, patterns, resources, and tools . \\n \\nAs we continue our journey, we welcome feedback on our approach and insights on other ways forward : \\nhttps://aka.ms/ResponsibleAIQuestions  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 2, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n4 Accountability  Goals  \\nGoal A1: Impact  assessment  \\nMicrosoft AI systems are assessed using Impact Assessments.   \\nApplies to:  All AI systems . \\nRequirements   \\nA1.1  Assess the impact of the system on people, organizations, and society by completing an Impact Assessment  \\nearly in the system’s development, typically when defining the product vision and requirements. Document the \\neffort using the Impact Assessment template  provided by the Office of Responsible AI . \\nTags:  Impact Assessment . \\nA1.2  Review the completed Impact Assessment  with the reviewers identified according to your organization’s \\ncompliance process before development starts . Secure all required approvals from those reviewers.  \\nTags:  Impact Assessment . \\nA1.3  Update and r eview the Impact Assessment  at least annually , when new intended uses  are added,  and before \\nadvancing to a new release stage.    \\nTags:  Impact Assessment . \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 3, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n5 Goal A2: Oversight of significant adverse impacts  \\nMicrosoft AI systems are reviewed to identify systems that may have a significant adverse impact on people, \\norganizations, and society, and additional oversight and requirements are applied to those systems.   \\nApplies to:  All AI systems . \\nRequirements  \\nA2.1  Review defined Restricted Uses  to determine whether the system meets the definition of any Restricted Use . \\nIf it does , document this in the Impact Assessment , and follow the requirements for the  Restricted Use . \\nTags:  Impact Assessment.  \\nA2.2  Answer prompts in the Impact Assessment template  to determine whether the system meets the definition of \\na Sensitive Use . If it does, report it to the Office of Responsible AI, and follow any additional requirements resulting \\nfrom a Sensitive Uses review.  \\nTags:  Impact Assessment.  \\nA2.3  Review your systems at least annually against the definitions for Sensitive Uses and  Restricted Uses. If there \\nare systems that meet the criteria for Sensitive Uses, report them  to the Office of Responsible AI . If there are \\nsystems that meet the criteria for Restricted Uses, notify the Office of Responsible AI . \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 4, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n6 Goal A3: Fit for purpose  \\nMicrosoft AI systems are fit for purpose in the sense that they provide valid solutions for the problems they are \\ndesigned to solve . \\nApplies to: All AI systems . \\nRequirements  \\nA3.1 Document in the Impact Assessment  how the system’s use will solve the problem posed by each  intended use , \\nrecognizing that there may be multiple valid ways in which to solve the problem.  \\nTags:  Impact Assessment . \\nA3.2 Define and document for each model in the AI system:  \\n1) the model’s proposed  inputs and how well they represent the concepts they are intended to represent ; include \\nanalysis of the limitations of this representation , \\n2) the model’s proposed  output  and how well it represents the concept it is intended to represent ; include analysis \\nof the limitations of this representation,  and \\n3) limitations to the generalizability of the resulting model based on the training and testing data that will be used.  \\nA3.3 Define and document  Responsible R elease Criteria  for this Goal . Include:  \\n1) a concise definition of the problem being solved in the  intended use , \\n2) performance metrics and their Responsible R elease Criteria , and  \\n3) error types and their Responsible R elease Criteria . \\nA3.4 Document an evaluation plan for each of the performance metrics and error types .  \\nTags:  Ongoing Evaluation Checkpoint.  \\nA3.5 Use the methods defined in requirement A3.4 to conduct evaluations . Document the pre-release results of the \\nevaluations . Determine and document how often ongoing evaluation should be conducted to continue supporting this \\nGoal. \\nTags:  Ongoing Evaluation Checkpoint . \\nA3.6 Provide documentation to customers which describes the system’s:   \\n1) intended uses , and  \\n2) evidence that the system is fit for purpose for each intended use . \\nWhen the system is a platform service  made available to  external customers or partners, include this information in the \\nrequired Transparency Note . \\nTags:  Transparency Note.  \\nA3.7 If an intended use  is not supported by evidence, or if evidence comes to light that refutes that the system is fit for \\npurpose for the intended use  at any point in the system’s use:  \\n1) remove the  intended use from customer -facing materials and make current customers aware of the issue,  take \\naction to close the identified gap , or discontinue the system,  \\n2) revise documentation related to the intended use , and  \\n3) publish the revised documentation to customers.  \\nWhen the syst em is a platform service made available to  external customers or partners, include this information in the \\nrequired Transparency Note.  \\nA3.8 Communicate with care about system benefits ; follow any applicable guidance from  your attorney .  \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 5, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n7 Goal A 4: Data governance and management  \\nMicrosoft AI systems are subject to appropriate data governance and management practices.  \\nApplies to:  All AI systems . \\nRequirements  \\nA4.1 Define and document data requirements with respect to the system’s intended uses , stakeholders , and the \\ngeographic areas where the system will be deployed. Document these requirements in the Impact Assessment .  \\nTags:  Impact Assessment.  \\nA4.2  Define and doc ument procedures for the collection and processing of data, to include annotation, labelling, \\ncleaning, enrichment, and aggregation, where relevant.  \\nA4.3 If you plan to use existing data sets to train the system, assess the quantity and suitability of available data \\nsets that will be needed by the system  in relation to the data requirements defined in A4.1.  Document this \\nassessment in the Impact Assessmen t.  \\nTags:  Impact Assessment.  \\nA4.4 Define and document methods for evaluating data to be used by the system against the requirements \\ndefined in A 4.1. \\nA4.5 Evaluate all data sets using the methods defined in requirement A 4.4. Document the results of the evaluation.  \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 6, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n8 Goal A5: Human oversight and control  \\nMicrosoft AI systems include capabilities that support informed human oversight and control.  \\nApplies to:  All AI systems . \\nRequirements  \\nA5.1 Identify the stakeholders  who are responsible for troubleshooting, managing, operating, overseeing, and \\ncontrolling the system during and after deployment. Document these stakeholders and their oversight and control \\nresponsibilities using the Impact Assessment template .  \\nTags:  Impact Assessment.  \\nA5.2 Identify the system elements (including system UX, features, alerting and reporting functions, and \\neducational materials) necessary for stakeholders identified in requirement A5.1 to effectively understand their \\noversight responsibilities and carry them out.  Stakeholders must be able to understand :  \\n1) the system’s intended uses ,  \\n2) how to effectively execute interactions with the system,  \\n3) how to interpret system behavior ,  \\n4) when and how to override, intervene, or interrupt the system , and  \\n5) how to remain aware of the possible tendency of over -relying on outputs produced by the system  \\n(“automation  bias”) .  \\nDocument the system design elements that will support relevant stakeholders for each oversight and control \\nfunction.  \\nA5.3 When possible, design the system elements identified in A 5.2. When this is not possible (for example, when \\nMicrosoft is not responsible for the system UX), provide guidance on human oversight considerations to the third \\nparty responsible for implementing the system elements identif ied in A 5.2.  \\nA5.4 Define and document the method to be used to evaluate whether each oversight or control function can be \\naccomplished by stakeholders in realistic conditions  of system use . Include the metrics or rubrics that will be used \\nin the evaluations. When this is n ot possible (for example, when Microsoft is not responsible for oversight and \\ncontrol functions), provide guidance on evaluating oversight and control functions to the third party responsible \\nfor evaluating oversight or control functions.  \\nA5.5 Define and document Responsible Release Criteria  to achieve this Goal.  \\nA5.6 Conduct evaluations defined by requirement A5.4 using a near -release version of the system. Document the \\nresults.  \\nA5.7 If there are Responsible Release Criteria for metrics or rubrics that have not been met, consult with the \\nreviewers named in the Impact Assessment , and in the case of Sensitive Uses , with the Office of Responsible AI, to \\ndevelop a plan detailing how the g ap will be managed until it can be closed. Document that plan.   \\n \\nTools and practices  \\nRecommendation A 5.3.1 Follow the Guidelines for Human -AI Interaction  when designing the system.  \\nRecommendation A 5.4.1 Assign user researchers to design these evaluations.  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 7, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n9  \\nTransparency  Goals  \\nGoal T 1: System intelligibility for decision making  \\nMicrosoft AI systems that inform decision making by or about people are designed to support stakeholder needs for \\nintelligibility of system behavior.  \\nApplies to: All AI systems  when the intended use of the generated outputs is to inform decision making by  or \\nabout people.  \\n \\n Requirements  \\nT1.1 Identify:  \\n1) stakeholders who will use the outputs of the system to make decisions, and  \\n2) stakeholders who are subject to decisions informed by the system.  \\nDocument these stakeholders using the Impact Assessment template . \\nTags:  Impact Assessment.  \\nT1.2 Design the system, including, when possible, the system UX, features, reporting functions, and educational \\nmaterials, so that stakeholders identified in requirement T1.1 can:  \\n1) understand the system’s intended uses,  \\n2) interpret relevant system behavior effectively (i.e., in a way that supports informed decision making), and  \\n3) remain aware of the possible tendency of over -relying on outputs produced by the system (\"automation \\nbias\").   \\nFor the two categories of stakeholders identified in requirement T1.1, document:  \\n1) how the system design will support their understanding of the system’s intended uses, and  \\n2) how the system aids their ability to interpret relevant system responses, and  \\n3) how the system design discourages automation bias.  \\nT1.3 Define and document the method to be used to evaluate whether each stakeholder who will make decisions \\nor be subject to decisions based on the behavior of the system can interpret the relevant system res ponses \\nreasonably well. Include the metrics or rubrics that will be used in the evaluations.  \\nTags:  Ongoing Evaluation Checkpoint.  \\nT1.4 Define and document a Responsible Release Plan, to include  Responsible Release Criteria to achieve this \\nGoal.  \\nTags:  Ongoing Evaluation Checkpoint.  \\nT1.5 Conduct evaluations defined by requirement T1.3. Document the pre -release results of the evaluations. \\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this Goal.  \\nTags:  Ongoing Evaluation Checkpoint.  \\nT1.6 If there are Responsible Release Criteria for metrics or rubrics that that have not been met, consult with the \\nreviewers named in the Impact Assessment, and in the ca se of Sensitive Uses, with the Office of Responsible AI, to \\ndevelop a plan detailing how the gap will be managed until it can be closed. Document that plan.  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 8, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n10 Tools and practices  \\nRecommendation T 1.2.1 Follow the Guidelines for Human -AI Interaction when designing the system.  \\n \\nRecommendation  T1.2.2 Use one or more techniques available as part of the Interpret ML toolkit to understand \\nthe impact of features on system behavior. This may help stakeholders who need to understand model predicti ons. \\nRecommendation  T1.3.1 Assign user researchers to define, design, and prioritize evaluations in appropriately \\nrealistic contexts  of use .  \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 9, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n11 Goal T2: Communication to stakeholders   \\nMicrosoft provides information about the capabilities and limitations of our AI systems to support stakeholders in \\nmaking informed choices about those systems.   \\nApplies to:  All AI systems . \\n \\n  Requirements  \\nT2.1 Identify : \\n1) stakeholders who make decisions about whether to employ a system for particular tasks, and  \\n2) stakeholders who develop or deploy systems that integrate with this system .  \\nDocument these stakeholders in the Impact Assessment  template . \\nTags: Impact Assessment . \\nT2.2 Publish documentation for the system so that  stakeholders defined in T2.1 can understand the system . \\nInclude :  \\n1) capabilities,  \\n2) intended  uses, \\n3) uses that require extra care or guidance,  \\n4) operational factors and settings that allow for effective and responsible system use,  \\n5) limitations, including uses for which the system was not designed  or evaluated , and  \\n6) evidence of system accuracy and performance  as well as a description of the extent to which these results \\nare generalizable across use cases that we re not part of the evaluation.  \\nWhen the system is a platform service made available to external customers or partners, a Transparency Note  is \\nrequired . \\nTags:  Transparency Note.  \\nT2.3 Review and update documentation annually or when any of the following events occur:  \\n1) new uses are added,  \\n2) functionality changes,  \\n3) the product moves to a new release stage , \\n4) new information about reliable and safe performance becomes known as defined by requirement RS 3.3, or \\n5) new information about system accuracy and performance becomes available.  \\nWhen the system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Transparency Note.  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 10, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n12 Goal T3: Disclosure of AI interaction  \\n \\nMicrosoft AI systems  are designed to inform  people that they are interacting with an AI system  or are using a system \\nthat generates or manipulates image, audio, or video content that could falsely appear to be  authentic.   \\nApplies to: AI systems  that impersonate interactions with humans , unless it is obvious from the circumstances \\nor context of use that an AI system is in use.   AI systems that generate or manipulate image, audio, or video \\ncontent  that could falsely appear to be authentic . \\n \\nRequirements  \\nT3.1 Identify stakeholders who will use or be exposed to the system, in accordance with the Impact Assessment \\nrequirements. Document these stakeholders using the Impact Assessment template.  \\nTags:  Impact Assessment.  \\nT3.2 Design the system, including system UX, features, reporting functions, educational materials, and outputs so \\nthat stakeholders identified in T3.1 will be informed of the type of AI system they are interacting with or exposed \\nto. Ensure that any image, audi o, or video outputs that are intended to be used outside the system  are labelled as \\nbeing produced by AI.  \\nT3.3 Define and document the method to be used to evaluate whether each stakeholder identified in T 3.1 is \\ninformed of the type of AI system they are interacting with or exposed to.  \\nTags:  Ongoing Evaluation Checkpoint . \\nT3.4 Define and document Responsible Rele ase Criteria  to achieve this Goal.  \\nTags:  Ongoing Evaluation Checkpoint . \\nT3.5 Conduct evaluations defined by requirement T 3.3. Document the pre -release results of the evaluations.  \\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this goal.  \\nTags:  Ongoing Evaluation Checkpoint . \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 11, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n13 Fairness  Goals  \\nGoal F1: Quality of service  \\nMicrosoft AI systems are designed to provide a similar quality of service for identified demographic groups, \\nincluding marginalized groups.   \\n \\nApplies to:  AI systems  when system users or people impacted by the system  with different demographic \\ncharacteristics might experience differences in quality of service t hat Microsoft can remedy by building the system \\ndifferently.  \\nRequirements  \\nF1.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of \\nexperiencing worse quality of service based on intended uses and geographic areas where the system will be \\ndeployed. Include:  \\n1) groups  defined by a single facto r, and  \\n2) groups defined by a combination of factors.  \\nDocument the prioritized identified demographic groups using the Impact Assessment template . \\nTags:  Impact Assessment.  \\nF1.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close gaps.  \\nDocument this process and its results.  \\nF1.3 Define and document the evaluation that you will perform  to support this Goal . Include:  \\n1) any system components to be evaluated, in addition to the whole system , \\n2) the metrics to be used to evaluate the system components and the whole system , and \\n3) a description of the data set to be used for this evaluation.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF1.4 Define and document Responsible Release Criteria  to achieve this Goal , as follows:   \\nFor each metric, document : \\n1) any target minimum performance level for all groups, and  \\n2) the target maximum (absolute or relative) performance difference between groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF1.5 Evaluate the system according to the defined Responsible Release Criteria.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF1.6 Reassess the system design, including the choice of training data, features, objective function, and training \\nalgorithm, to pursue the goals of :  \\n1) improving performance for any identified demographic group that does not meet any target minimum \\nperformance level, and  \\n2) minimizing performance differences between identified demographic groups, paying particular attention \\nto those that exceed the target maximum, while recognizing that doing so may appear to affect system \\nperformance and that it is seldom clear how to make such tradeoffs.  \\nConsult with your attorney to determine your appro ach to this, including how you will identify and document \\ntradeoffs.   \\nTags:  Ongoing Evaluation Checkpoint . ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 12, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n14 F1.7 Identify and document any justifiable factors , such as circumstantial and other operational factors  (e.g., \\n“background noise” for speech recognition systems or “image resolution” for facial recognition systems), that \\naccount for :  \\n1) any inability to meet any target minimum performance level for any identified demographic group, and  \\n2) any remaining performa nce differences between identified demographic groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF1.8 Document the pre -release results from requirements F1. 4, F1.5, and F1. 6. Determine and document how often \\nongoing evaluation should be conducted to continue supporting this Goal. \\nTags:  Ongoing Evaluation Checkpoint . \\nF1.9 Publish information for customers about :  \\n1) identified demographic groups for which performance may not meet any target minimum performance \\nlevel,  \\n2) any remaining performance disparities between identified demographic groups that may exceed the target \\nmaximum, and  \\n3) any justifiable factors that accoun t for these performance levels and differences.  \\nWhen the system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Transparency Note.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 13, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n15 Tools and practices  \\nRecommendation F1.1.1 For identifying people by age, gender identity, and ancestry in North America, use Best \\nPractices for Age, Gender Identity, and Ancestry .  \\n \\nRecommendation F1.1.2 Work with user researchers to understand variations in demographic group s across \\nintended uses and geographic areas.  \\n \\nRecommendation F1.1.3  Work with domain -specific subject matter experts  to understand the factors that impact \\nperformance of your system and how they vary across identified demographic groups in this domain.  \\n \\nRecommendation F1.1.4 Work with members of identified demographic groups to understand the risks of and \\nimpacts associated with differences in quality of service. Consider using the Community Jury technique to conduct \\nthese discussions.  \\nRecommendation F1.2.1 Use Analysis Platform to understand the representation of identified demographic \\ngroups in data sets that you plan to use for training and evaluating your system, respecting privacy controls for \\nworking with sensitive data.  \\n \\nRecommend ation F1.2.2 Document the representation of identified demographic groups in a Datasheet.  \\nRecommendation F1. 5.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate \\nfor the system.  \\n \\nRecommendation F1. 5.2 Use Error Analysis to help understand factors that may account for performance levels \\nand differences, if appropriate for the system.  \\n \\nRecommendation F1. 5.3 Use one or more techniques available as part of the Interpret ML toolkit to help \\nunderstand factors th at may account for performance levels and differences, if appropriate for the system.  \\nRecommendation F1. 6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate \\nfor the system.  \\n \\nRecommendation F1. 6.2 Be prepared to collect additional training data for identified demographic groups.  \\nRecommendation F1. 7.1 Use Error Analysis to help understand factors that may account for performance levels \\nand differences, if appropriate for the system.   \\n \\nRecommendation F1 .7.2 Use one or more techniques available as part of the Interpret ML toolkit to help \\nunderstand factors that may account for performance levels and differences, if appropriate for the system.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 14, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n16 Goal F2: Allocation of resources and opportunities  \\n \\nMicrosoft AI systems that allocate resources or opportunities in essential domains are designed to do so in a manner \\nthat minimizes disparities in outcomes for identified demographic groups, including marginalized groups.   \\nApplies to: AI sys tems  that generate outputs that directly affect the allocation of resources or opportunities relating \\nto finance, education, employment, healthcare, housing, insurance, or social welfare.  \\nRequirements  \\nF2.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of being \\ndifferentially impacted by the system based on intended uses and geographic areas where the system will be \\ndeployed. Include:  \\n1) groups defined by a single  factor, and  \\n2) groups defined by a combination of factors.  \\nDocument the prioritized identified demographic groups using the Impact Assessment template .  \\nTags: Impact Assessment . \\nF2.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close any \\ngaps.  Document this process and its results.  \\nF2.3 Define and document the evaluation that you will perform to support this Goal . Include:  \\n1) any system components to be evaluated, in addition to the whole syst em, \\n2) the metrics to be used to evaluate the system components and the whole system, and  \\n3) the data set to be used for this evaluation.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF2.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:  \\nFor each metric, document the target maximum difference (absolute or relative) between the rates at which \\nresources and opportunities are allocated to groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF2.5 Evaluate the system according to the defined Responsible Release Criteria.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF2.6 Reassess the system design, including the choice of training data, features, objective function, and training \\nalgorithm, to pursue the goal of minimizing differences between the rates at which resources and opportunities \\nare allocated to identified demogra phic groups, paying particular attention to those that exceed the target \\nmaximum difference, while recognizing that doing so may appear to affect system performance and it is seldom \\nclear how to make such trade -offs.  \\nConsult with your attorney to determine your approach to this, including how you will identify and document \\ntrade -offs. \\nTags:  Ongoing Evaluation Checkpoint . \\n \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 15, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n17 F2.7 Identify and document any justifiable factors that account for any remaining differences between the rates at \\nwhich resources and opportunities are allocated to identified demographic groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF2.8 Document the pre -release results for the evaluation described by requirements F2. 4, F2.5, and F2. 6. \\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this goal.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF2.9 Publish information for customers about : \\n1) any remaining differences between the rates at which resources and opportunities are allocated to \\nidentified demographic groups , and  \\n2) any justifiable factors that account for these differences. When the system is a platform service made \\navailable to external customers or partners, include this information in the required Transparency Note.  \\nTags:  Transparency Note.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 16, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n18 Tools and practices  \\nRecommendation F2.1.1 For North America, use Best Practices for Age, Gender Ide ntity, and Ancestry to help \\nidentify demographic groups and methods for collecting demographic information.  \\n \\nRecommendation F2. 1.2 Work with user researchers to understand variations in demographic groups across \\nintended uses and geographic areas.  \\n \\nRecommendation F2. 1.3 Work with domain -specific subject matter experts to understand the facts that impact \\nperformance of your system and how they vary across identified demographic groups in this domain.  \\n \\nRecommendation F2. 1.4 Work with members of identif ied demographic groups to understand risks of and \\nimpacts associated with differences between the rates at which resources and opportunities are allocated.  \\nRecommendation F2. 2.1 Use Analysis Platform to understand the representation of identified demograp hic \\ngroups, respecting privacy requirements for using sensitive data.  \\n \\nRecommendation F2. 2.2 Document the representation of identified demographic groups in a Datasheet.  \\nRecommendation F2. 5.1 Use the Fairlearn Python toolkit ’s assessment and mitigation capabilities, if appropriate \\nfor the system .  \\n \\nRecommendation F2.5.2 Use Error Analysis to help understand factors that may account for differences between \\nthe rates at which resources and opportunities are allocated to the ide ntified demographic groups, if appropriate \\nfor the system.   \\n \\nRecommendation F2. 5.3 Use one or more techniques available as part of the Interpret ML toolkit to help \\nunderstand factors that may account for differences between the rates at which resources and  opportunities are \\nallocated to the identified demographic groups, if appropriate for the system.  \\nRecommendation F2. 6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate \\nfor the system.   \\nRecommendation F2. 7.1 Use Error Analysis to help understand factors that may account for differences between \\nthe rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate \\nfor the system.   \\n \\nRecommendation F2.7.2 Use Interpret ML to help understand factors that may account for differences between \\nthe rates at which resources and opportunities are allocated to the identified demographic groups , if appropriate \\nfor the system.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 17, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n19 Goal F3: Minimization of stereotyping, demeaning, and erasing outputs  \\n \\nMicrosoft AI systems that describe, depict, or otherwise represent people, cultures, or society are designed to \\nminimize the potential for stereotyping, de meaning , or erasing identified demographic groups, including \\nmarginalized groups.  \\nApplies to: AI systems  when system outputs include descriptions, depictions, or other representations of people, \\ncultures, or society.  \\nRequirements  \\nF3.1 Identify and prioritize demographic grou ps, including marginalized groups, that may be at risk of being \\nsubject to stereotyping, demeaning , or erasing outputs  of the system . Include:  \\n1) groups defined by a single factor, and  \\n2) groups defined by a combination of factors.   \\nDocument the prioritized identified demographic groups using the Impact Assessment template .  \\nTags:  Impact Assessment.  \\nF3.2 Define and document any system components to be evaluated, in addition to the whole system.  \\nF3.3 Define and document a plan to evaluate the system components and the whole system for risks of \\nstereotyping, demeaning , and erasing  the prioritized identifie d demographic groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF3.4 Evaluate the system according to the plan defined  in requirement F3. 3.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF3.5 Reassess the system design, including the choice of training data, features, objective function, and training \\nalgorithm, to pursue the goal of minimizing the potential for  stereotyping, demeaning , and erasing  the identified \\ndemographic groups.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF3.6 Document the pre -release results from requirements F3.4 and F3. 5. Determine and document how often \\nongoing evaluation should be conducted to continue supporting this goal.  \\nTags:  Ongoing Evaluation Checkpoint . \\nF3.7 Publish information for customers about these risks involving identified demographic groups. When the \\nsystem is a platform service  made available to external customers or partners, include this information in the \\nrequired Transparency Note . \\nTags:  Transparency Note.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 18, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n20 Tools and practices  \\nRecommendation F3. 1.1 Work with user researchers, subject matter experts, and members of identified \\ndemographic groups to understand these risks and their impacts.  \\nRecommendation F3. 4.1 Use CheckList to help evaluate these risks involving identified demographic groups, if \\nappropriate for the system.  \\nRecommendation  F3.4.2 Use red teaming exercises to evaluate these risks involving identified demographic \\ngroups.   \\nRecommendation  F3.5.1 Mitigate any risks of these types of harms that you can. In addition, establish feedback \\nmechanisms and a plan for addressing problems, in alignment with Reliability and Safety Goal RS3. Note that this \\napproach is recommended in ackno wledgment of the fact that the state -of-the-art in mitigating these risks is less \\nadvanced than the state -of-the-art in mitigating differences in quality of service or allocative harms.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 19, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n21 Reliability & Safety  Goals  \\nGoal RS1: Reliability and safety guidance  \\nMicrosoft evaluates  the operational factors and ranges within which AI systems are expected to perform reliably and \\nsafely , remediates issues, and provides related information to customers . \\nApplies to: All AI systems . \\nRequirements  \\nRS1.1  Document how:  \\n1) reliable and safe behavior  is defined for this system and,  \\n2) what acceptable error rates are for overall system performance in the context of intended uses.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.2  Evaluate training and test data sets to ensure that they include representation of the intended uses, \\noperational factors, and an appropriate range of settings for each factor. Document the evaluation.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.3  Determine  and document the operational factors, including quality of system input, use, and operational \\ncontext that are critical to manage for reliable and safe use of the system in its deployed context.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.4  Define and docu ment acceptable ranges for each operational factor important to support reliable and safe \\nsystem use. Define and document an acceptable error rate for the system when operating within these ranges.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.5  Define intende d uses, if any, where additional operational factors, more narrow or different acceptable \\nranges, or lower acceptable error rates (including false positive and false negative error rates), are advised to \\nensure reliability and safety. Document your conclus ions. \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.6  Define and document  an evaluation plan based on requirements RS1.1, RS1.3, RS1.4, and RS1.5, to include \\nthe environment in which the system will be evaluated.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.7  Evaluate the system according to the evaluation plan defined in requirem ent RS1.6 to ensure reliable and \\nsafe system behavior.  Document the pre -release results of the evaluation.  Determine and document how often \\nongoing evaluation should be conducted to continue supporting this goal.    \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.8 In the event of failure cases within operational factors and defined ranges, work to resolve the issues. If the \\nResponsible Release Criteria established in requirements RS1.1, RS1.3, RS1.4, and RS1 .5 cannot be met, a \\nreassessment of intended uses and updated documentation is required.  \\nTags:  Ongoing Evaluation Checkpoint . \\nRS1.9  Provide documentation to customers and potential customers of the system that includes the outputs of \\nrequirements RS1. 2, RS1.7 and RS1.8, and any unsupported uses defined i n the Impact Assessment and in RS1.8. \\nWhen the system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Impact Assessment , Transparency Note . ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 20, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n22  \\nTools and practices  \\nRecommendation RS1. 1.1 Interview safety experts and review relevant literature for domains where the system \\nmay impact the safety of people.  \\nRecommendation RS1. 4.1 Interview customers to understand operational factors and their variations.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 21, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n23 Goal RS2:  Failures and remediations  \\nMicrosoft AI systems are designed to minimize the time to remediation of predictable or known failures.  \\nApplies to: All AI systems . \\nRequirements  \\nRS2.1 Define predictable failures, including false positive  and false negative  results for the system as a whole \\nand how they would impact stakeholders  for each intended use . Use the Impact Assessment template  to \\ndocument any adverse impacts  of these failures on stakeholders.  \\nTags:  Impact Assessment . \\nRS2.2 For each case of a predictable failure likely to have an adverse impact on a stakeholder, document the \\nfailure management approach : \\n1) When possible, design and build the system to avoid this  failure. Describe the design solution. \\nEstimate the time range for resolving predic table failures for each designed solution or indicate that \\nthe failure will be prevented by design.  \\n2) When a failure cannot be prevented by design, build a fallback option t hat may be used when this \\nfailure occurs. Describe the fallback option and document the estimated time required to invoke and \\nuse the fallback option.  \\n3) Provide training and documentation for stakeholders accountable for system oversight that supports \\ntheir resolution of the failure. Describe the documentation and training.  \\nRS2.3 Document your plan for managing previously unknown failures that come to light once the system is \\nin use:  \\n1) Describe the system’s rollback plan and document the time that may elapse until the entire system, \\nacross all endpoints can be rolled back.   \\n2) Describ e support for turning features off and document the time that may elapse until the feature can \\nbe turned off across all endpoints.   \\n3) Describe the process for updating and releasing updates to each model and document the time that \\nmay elapse until the syste m has been updated across all endpoints.   \\n4) Describe how customers, partners, and end users will be notified of changes to the system, updated \\nunderstandings of failures, and their best mitigations.  \\nRS2.4 Provide training and documentation for system owners, developers, customer support and other \\nstakeholders responsible for managing the system to support their remediation and mitigation of predictable \\nfailures identified in requirement RS2.1. Document the training and document ation provided.  \\n \\nTools and practices  \\nRecommendation RS2. 1.1 Conduct Failure Mode and Effects Analysis.  \\nRecommendation RS2. 2.1 Follow the Guidelines for Human -AI Interaction when designing the system to help \\nmanage failures.  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 22, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n24 Goal RS 3: Ongoing monitoring , feedback , and evaluation  \\n \\nMicrosoft AI systems are subject to ongoing monitoring,  feedback, and evaluation  so that we can identify and review \\nnew uses, identify and troubleshoot issues, manage and maintain the systems, and improve them over tim e.  \\n \\nApplies to:  All AI systems . \\nRequirements  \\nRS3.1 Establish and document a detailed inventory of the system health monitoring methods to be used, to \\ninclude:  \\n1) data and insights generated from data repositories, system analytics, and associated alerts,  \\n2) processes by which customers can submit information about failures and concerns, and  \\n3) processes by which the general public can submit feedback.  \\nRS3.2  Define and document a standard operating procedure and system health monitoring action plan for e ach \\nmonitoring channel for the system, to include:  \\n1) processes for reproducing system failures to support troubleshooting and prevention of future failures,  \\n2) which events will be monitored,  \\n3) how events will be prioritized for review,  \\n4) the expected frequency o f those reviews,  \\n5) how events will be prioritized for response and timing to resolution,  \\n6) how high priority issues related to supporting the Standard and its goals will be escalated to the Office of \\nResponsible AI, and  \\n7) engaging customer service to ensure that they are aware of how to respond to issues for the system.  \\nRS3.3  When new uses, critical operational factors, or changes in the supported range of an operational factor are \\nidentified, determine whether any new use or operational factor can be supported with the existing system, will be \\nsupported but require additional  work, or will not be supported.   \\n• When new uses or operational  factors  identified are to be supported , evaluate the updated system in \\naccordance with requirement RS1.6 , add the new intended use to the Impact Assessment,  and publish \\nupdated communication in  accordance with requirement RS1.9 . \\n• When these new uses or operational factor range changes cannot or will not be accommodated to ensure \\nreliable and safe performance of the system update customer documentation described in RS1.9 to include \\nthe new use as an unsupported use.  \\nWhen the system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Impact Assessment, Transparency Note.  \\nRS3.4 When a system is to be used for a Sensitive Use that imposes qualification or quality control requirements \\nbeyond the intended uses and/or operational factor ranges, conduct an evaluation specific to this use. If the \\nrequired Responsible Release Criteria cannot be met, the Office of Responsible AI will review the results and decide \\nhow to proceed. Document any changes to the Responsible Release Criteria and document the results of \\nevaluation.  \\nRS3.5 Conduct all evaluations tagged as Ongoing Evaluation Checkpoints in other Goals on an ongoing basis.  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 23, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n25 RS3.6 If there are targets in Ongoing Evaluation Checkpoints that are no longer satisfied, consult with named \\nreviewers, and in the case of Sensitive Uses, with the Office of Responsible AI, to de velop and implement a plan to \\nclose any gaps. Document the process, its results, and conclusions.  \\nRS3.7 If evidence comes to light that refutes the system is fit for purpose for an intended use  at any point in the \\nsystem’s use:  \\n1) remove the intended use from customer -facing materials  and make current customers aware of the issue, \\ntake action to close the identified gap , or discontinue the system,  \\n2) revise documentation related to the intended use , and  \\n3) publish the revised documentation to customers.  \\nWhen the  system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Transparency Note.  \\nRS3.8: Review and update documentation required by Goal T2 when any of the following events occur:  \\n1) new uses are added,  \\n2) functionality changes,  \\n3) new information about reliable and safe performance becomes known as defined by requirement RS3. 3, or \\n4) new information abo ut system accuracy and performance becomes available . \\nWhen the system is a platform service made available to external customers or partners, include this information in \\nthe required Transparency Note.  \\nTags:  Transparency Note.  \\nRS3.9: Escalate unresolved  issues related to supporting the Standard and its requirements to the Office of \\nResponsible AI.  \\n \\n \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 24, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n26 Privacy & Security  Goals  \\nGoal PS1: Privacy Standard compliance  \\nMicrosoft AI systems are designed to protect privacy in accordance with the Microsoft Privacy Standard.   \\n \\nApplies when:  Microsoft Privacy Standard applies.  \\n \\n \\nGoal PS2: Security Policy compliance  \\n \\nMicrosoft AI systems are designed to be secure in accordance with the Microsoft Security Policy.   \\nApplies when: Microsoft Security Policy applies.  \\n \\n  ', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 25, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan}),\n",
       " Document(page_content='Microsoft Responsible AI Standard v2  \\n \\n27 Inclusiveness  Goal  \\nGoal I1: Accessibility Standards compliance  \\nMicrosoft AI systems  are designed to be inclusive in accordance with the Microsoft Accessibility Standards.   \\nApplies when:  Microsoft Accessibility Standards apply.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nScan this code to access responsible AI resources  from Microsoft : \\n \\n \\n \\n \\n© 2022 Microsoft Corporation. All rights reserved. This document  is provided “as -is.” It has been edited for external release to \\nremove internal links, references, and examples.  Information and views expressed in this document, including URL and other \\nInternet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only \\nand are fictitious. No real association is intended or inferred. This document does not provide you with any legal rights to any \\nintellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.  \\n', metadata={'source': '.\\\\documents\\\\Microsoft_Standards.pdf', 'page': 26, 'field': 'Applied', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/Microsoft_Standards.pdf', 'description': nan})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#añadir metadata departamento\n",
    "for doc in docs:\n",
    "    input_metadata(doc,df)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text splitter\n",
    "In this step, we will divide the text into chunks, which are the pieces of informations that will be embedded and stored in the data base. We wil later retrieve them and give them as context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf',\n",
       " 'page': 3,\n",
       " 'field': 'Theory',\n",
       " 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       " 'description': nan}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings y Vector store (chroma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating database\n",
    "We will use Chroma, an opensource vector store included in langchain to store our chunks and embeddings. When we create the database, the chunks we created will be embedded using the selected model, in our case OpenAi's ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will persist the database in memory, so next time we will only have to load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating metadata\n",
    "In some cases we would like to update the metadata once the database was created, we would do so with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vectordb.get()['ids']\n",
    "docs = vectordb.get()['documents']\n",
    "meta = vectordb.get()['metadatas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def extract_folder(metadata):\n",
    "    path = Path(metadata['source'])\n",
    "    return path.parent.name\n",
    "\n",
    "def extract_name(metadata):\n",
    "    path = Path(metadata['source'])\n",
    "    return path.name\n",
    "\n",
    "def input_metadata(doc, df):\n",
    "    name = meta['source']\n",
    "    #print(name)\n",
    "    df_copy = df[df['file_name'] == name].reset_index(drop=True)\n",
    "    doc.metadata['field'] = df_copy.loc[0,'field']\n",
    "    doc.metadata['link'] = df_copy.loc[0,'link']\n",
    "    doc.metadata['description'] = df_copy.loc[0,'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('.library.csv',delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors =[]\n",
    "for i in range(len(ids)):\n",
    "    try:\n",
    "        input_metadata(df,meta[i])\n",
    "        doc = Document(page_content=docs[i], metadata=meta[i])\n",
    "        vectordb.update_document(ids[i],doc)\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KeyError(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(errors))\n",
    "errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.get(ids[820])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "We can filter the database before retrieving. This should help us to get a more specific response and get the programme running faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['a4ebdddc-e2db-11ee-bd0e-38fc98f3447b',\n",
       "  'a4ebdddd-e2db-11ee-9fb0-38fc98f3447b',\n",
       "  'a4ebddde-e2db-11ee-b583-38fc98f3447b',\n",
       "  'a4ebdddf-e2db-11ee-adc4-38fc98f3447b',\n",
       "  'a4ebdde0-e2db-11ee-99f8-38fc98f3447b',\n",
       "  'a4ebdde1-e2db-11ee-b148-38fc98f3447b',\n",
       "  'a4ebdde2-e2db-11ee-abb6-38fc98f3447b',\n",
       "  'a4ebdde3-e2db-11ee-b9d3-38fc98f3447b',\n",
       "  'a4ebdde4-e2db-11ee-a615-38fc98f3447b',\n",
       "  'a4ebdde5-e2db-11ee-9415-38fc98f3447b',\n",
       "  'a4ebdde6-e2db-11ee-8b06-38fc98f3447b',\n",
       "  'a4ebdde7-e2db-11ee-bcb2-38fc98f3447b',\n",
       "  'a4ebdde8-e2db-11ee-bef6-38fc98f3447b',\n",
       "  'a4ebdde9-e2db-11ee-a9f4-38fc98f3447b',\n",
       "  'a4ebddea-e2db-11ee-80d9-38fc98f3447b',\n",
       "  'a4ebddeb-e2db-11ee-9102-38fc98f3447b',\n",
       "  'a4ebddec-e2db-11ee-828e-38fc98f3447b',\n",
       "  'a4ebdded-e2db-11ee-a630-38fc98f3447b',\n",
       "  'a4ebddee-e2db-11ee-9210-38fc98f3447b',\n",
       "  'a4ebddef-e2db-11ee-a930-38fc98f3447b',\n",
       "  'a4ebddf0-e2db-11ee-b353-38fc98f3447b',\n",
       "  'a4ebddf1-e2db-11ee-8ba3-38fc98f3447b',\n",
       "  'a4ebddf2-e2db-11ee-aec7-38fc98f3447b',\n",
       "  'a4ebddf3-e2db-11ee-b310-38fc98f3447b',\n",
       "  'a4ebddf4-e2db-11ee-897c-38fc98f3447b',\n",
       "  'a4ebddf5-e2db-11ee-a6c5-38fc98f3447b',\n",
       "  'a4ebddf6-e2db-11ee-b8d2-38fc98f3447b',\n",
       "  'a4ebddf7-e2db-11ee-bb03-38fc98f3447b',\n",
       "  'a4ebddf8-e2db-11ee-9fb2-38fc98f3447b',\n",
       "  'a4ebddf9-e2db-11ee-9515-38fc98f3447b',\n",
       "  'a4ebddfa-e2db-11ee-b01e-38fc98f3447b',\n",
       "  'a4ebddfb-e2db-11ee-8180-38fc98f3447b',\n",
       "  'a4ebddfc-e2db-11ee-b251-38fc98f3447b',\n",
       "  'a4ebddfd-e2db-11ee-b52e-38fc98f3447b',\n",
       "  'a4ebddfe-e2db-11ee-92ee-38fc98f3447b',\n",
       "  'a4ebddff-e2db-11ee-b0b4-38fc98f3447b',\n",
       "  'a4ebde00-e2db-11ee-ac44-38fc98f3447b',\n",
       "  'a4ebde01-e2db-11ee-9bdf-38fc98f3447b',\n",
       "  'a4ebde02-e2db-11ee-844b-38fc98f3447b',\n",
       "  'a4ebde03-e2db-11ee-9f94-38fc98f3447b',\n",
       "  'a4ebde04-e2db-11ee-b3fb-38fc98f3447b',\n",
       "  'a4ebde05-e2db-11ee-bcaf-38fc98f3447b',\n",
       "  'a4ebde06-e2db-11ee-9ce4-38fc98f3447b',\n",
       "  'a4ebde07-e2db-11ee-b4ea-38fc98f3447b',\n",
       "  'a4ebde08-e2db-11ee-9757-38fc98f3447b',\n",
       "  'a4ebde09-e2db-11ee-a0ac-38fc98f3447b',\n",
       "  'a4ebde0a-e2db-11ee-a011-38fc98f3447b',\n",
       "  'a4ebde0b-e2db-11ee-8bcc-38fc98f3447b',\n",
       "  'a4ebde0c-e2db-11ee-81d2-38fc98f3447b',\n",
       "  'a4ebde0d-e2db-11ee-a428-38fc98f3447b',\n",
       "  'a4ebde0e-e2db-11ee-86ce-38fc98f3447b',\n",
       "  'a4ebde0f-e2db-11ee-b7ec-38fc98f3447b',\n",
       "  'a4ebde10-e2db-11ee-8e64-38fc98f3447b',\n",
       "  'a4ebde11-e2db-11ee-ab87-38fc98f3447b',\n",
       "  'a4ebde12-e2db-11ee-8cd4-38fc98f3447b',\n",
       "  'a4ebde13-e2db-11ee-a099-38fc98f3447b',\n",
       "  'a4ebde14-e2db-11ee-afcc-38fc98f3447b',\n",
       "  'a4ebde15-e2db-11ee-84f0-38fc98f3447b',\n",
       "  'a4ebde16-e2db-11ee-af6a-38fc98f3447b',\n",
       "  'a4ebde17-e2db-11ee-9cc2-38fc98f3447b',\n",
       "  'a4ebde18-e2db-11ee-bfda-38fc98f3447b',\n",
       "  'a4ebde19-e2db-11ee-998d-38fc98f3447b',\n",
       "  'a4ebde1a-e2db-11ee-a260-38fc98f3447b',\n",
       "  'a4ebde1b-e2db-11ee-9567-38fc98f3447b',\n",
       "  'a4ebde1c-e2db-11ee-b073-38fc98f3447b',\n",
       "  'a4ebde1d-e2db-11ee-87e5-38fc98f3447b',\n",
       "  'a4ebde1e-e2db-11ee-b964-38fc98f3447b',\n",
       "  'a4ebde1f-e2db-11ee-83b3-38fc98f3447b',\n",
       "  'a4ebde20-e2db-11ee-993c-38fc98f3447b',\n",
       "  'a4ebde21-e2db-11ee-8fcc-38fc98f3447b',\n",
       "  'a4ebde22-e2db-11ee-aa99-38fc98f3447b',\n",
       "  'a4ebde23-e2db-11ee-b0c6-38fc98f3447b',\n",
       "  'a4ebde24-e2db-11ee-8565-38fc98f3447b',\n",
       "  'a4ebde25-e2db-11ee-a65c-38fc98f3447b',\n",
       "  'a4ebde26-e2db-11ee-ab84-38fc98f3447b',\n",
       "  'a4ebde27-e2db-11ee-b2a5-38fc98f3447b',\n",
       "  'a4ebde28-e2db-11ee-9848-38fc98f3447b',\n",
       "  'a4ebde29-e2db-11ee-975f-38fc98f3447b',\n",
       "  'a4ebde2a-e2db-11ee-8aad-38fc98f3447b',\n",
       "  'a4ebde2b-e2db-11ee-a96f-38fc98f3447b',\n",
       "  'a4ebde2c-e2db-11ee-aa8d-38fc98f3447b',\n",
       "  'a4ebde2d-e2db-11ee-b73d-38fc98f3447b',\n",
       "  'a4ebde2e-e2db-11ee-b683-38fc98f3447b',\n",
       "  'a4ebde2f-e2db-11ee-9ef1-38fc98f3447b',\n",
       "  'a4ebde30-e2db-11ee-841b-38fc98f3447b',\n",
       "  'a4ebde31-e2db-11ee-9b27-38fc98f3447b',\n",
       "  'a4ebde32-e2db-11ee-a14c-38fc98f3447b',\n",
       "  'a4ebde33-e2db-11ee-9bd3-38fc98f3447b',\n",
       "  'a4ebde34-e2db-11ee-be4b-38fc98f3447b',\n",
       "  'a4ebde35-e2db-11ee-bc7c-38fc98f3447b',\n",
       "  'a4ebde36-e2db-11ee-9641-38fc98f3447b',\n",
       "  'a4ebde37-e2db-11ee-b6f8-38fc98f3447b',\n",
       "  'a4ebde38-e2db-11ee-baad-38fc98f3447b',\n",
       "  'a4ebde39-e2db-11ee-8b6d-38fc98f3447b',\n",
       "  'a4ebde3a-e2db-11ee-9726-38fc98f3447b',\n",
       "  'a4ebde3b-e2db-11ee-add0-38fc98f3447b',\n",
       "  'a4ebde3c-e2db-11ee-a5eb-38fc98f3447b',\n",
       "  'a4ebde3d-e2db-11ee-a7d9-38fc98f3447b',\n",
       "  'a4ebde3e-e2db-11ee-b53e-38fc98f3447b',\n",
       "  'a4ebde3f-e2db-11ee-a866-38fc98f3447b',\n",
       "  'a4ebde40-e2db-11ee-9bbc-38fc98f3447b',\n",
       "  'a4ebde41-e2db-11ee-9835-38fc98f3447b',\n",
       "  'a4ebde42-e2db-11ee-8811-38fc98f3447b',\n",
       "  'a4ebde43-e2db-11ee-b7fb-38fc98f3447b',\n",
       "  'a4ebde44-e2db-11ee-b108-38fc98f3447b',\n",
       "  'a4ebde45-e2db-11ee-9791-38fc98f3447b',\n",
       "  'a4ebde46-e2db-11ee-89ad-38fc98f3447b',\n",
       "  'a4ebde47-e2db-11ee-8949-38fc98f3447b',\n",
       "  'a4ebde48-e2db-11ee-99b4-38fc98f3447b',\n",
       "  'a4ebde49-e2db-11ee-b168-38fc98f3447b',\n",
       "  'a4ebde4a-e2db-11ee-b2dc-38fc98f3447b',\n",
       "  'a4ebde4b-e2db-11ee-a086-38fc98f3447b',\n",
       "  'a4ebde4c-e2db-11ee-ab93-38fc98f3447b',\n",
       "  'a4ebde4d-e2db-11ee-9ed3-38fc98f3447b',\n",
       "  'a4ebde4e-e2db-11ee-a581-38fc98f3447b',\n",
       "  'a4ebde4f-e2db-11ee-a3ce-38fc98f3447b',\n",
       "  'a4ebde50-e2db-11ee-b549-38fc98f3447b',\n",
       "  'a4ebde51-e2db-11ee-8e57-38fc98f3447b',\n",
       "  'a4ebde52-e2db-11ee-93f2-38fc98f3447b',\n",
       "  'a4ebde53-e2db-11ee-aef7-38fc98f3447b',\n",
       "  'a4ebde54-e2db-11ee-8826-38fc98f3447b',\n",
       "  'a4ebde55-e2db-11ee-a7bf-38fc98f3447b',\n",
       "  'a4ebde56-e2db-11ee-bb8b-38fc98f3447b',\n",
       "  'a4ebde57-e2db-11ee-b154-38fc98f3447b',\n",
       "  'a4ebde58-e2db-11ee-abca-38fc98f3447b',\n",
       "  'a4ebde59-e2db-11ee-8db2-38fc98f3447b',\n",
       "  'a4ebde5a-e2db-11ee-bb23-38fc98f3447b',\n",
       "  'a4ebde5b-e2db-11ee-97cd-38fc98f3447b',\n",
       "  'a4ebde5c-e2db-11ee-bf67-38fc98f3447b',\n",
       "  'a4ebde5d-e2db-11ee-b991-38fc98f3447b',\n",
       "  'a4ebde5e-e2db-11ee-82b4-38fc98f3447b',\n",
       "  'a4ebde5f-e2db-11ee-bf4b-38fc98f3447b',\n",
       "  'a4ebde60-e2db-11ee-baa3-38fc98f3447b',\n",
       "  'a4ebde61-e2db-11ee-be98-38fc98f3447b',\n",
       "  'a4ebde62-e2db-11ee-9e69-38fc98f3447b',\n",
       "  'a4ebde63-e2db-11ee-b12f-38fc98f3447b',\n",
       "  'a4ebde64-e2db-11ee-8c61-38fc98f3447b',\n",
       "  'a4ebde65-e2db-11ee-bdf1-38fc98f3447b',\n",
       "  'a4ebde66-e2db-11ee-a31e-38fc98f3447b',\n",
       "  'a4ebde67-e2db-11ee-bfb3-38fc98f3447b',\n",
       "  'a4ebde68-e2db-11ee-8ec4-38fc98f3447b',\n",
       "  'a4ebde69-e2db-11ee-90ff-38fc98f3447b',\n",
       "  'a4ebde6a-e2db-11ee-a852-38fc98f3447b',\n",
       "  'a4ebde6b-e2db-11ee-b63e-38fc98f3447b',\n",
       "  'a4ebde6c-e2db-11ee-8294-38fc98f3447b',\n",
       "  'a4ebde6d-e2db-11ee-9f5e-38fc98f3447b',\n",
       "  'a4ebde6e-e2db-11ee-a1a5-38fc98f3447b',\n",
       "  'a4ebde6f-e2db-11ee-a1c8-38fc98f3447b',\n",
       "  'a4ebde70-e2db-11ee-92a2-38fc98f3447b',\n",
       "  'a4ebde71-e2db-11ee-893b-38fc98f3447b',\n",
       "  'a4ebde72-e2db-11ee-8e3e-38fc98f3447b',\n",
       "  'a4ebde73-e2db-11ee-806e-38fc98f3447b',\n",
       "  'a4ebde74-e2db-11ee-b5e3-38fc98f3447b',\n",
       "  'a4ebde75-e2db-11ee-9121-38fc98f3447b',\n",
       "  'a4ebde76-e2db-11ee-b7d7-38fc98f3447b',\n",
       "  'a4ebde77-e2db-11ee-b06c-38fc98f3447b',\n",
       "  'a4ebde78-e2db-11ee-960b-38fc98f3447b',\n",
       "  'a4ebde79-e2db-11ee-922c-38fc98f3447b',\n",
       "  'a4ebde7a-e2db-11ee-8dfe-38fc98f3447b',\n",
       "  'a4ebde7b-e2db-11ee-9bec-38fc98f3447b',\n",
       "  'a4ebde7c-e2db-11ee-bba1-38fc98f3447b',\n",
       "  'a4ebde7d-e2db-11ee-9795-38fc98f3447b',\n",
       "  'a4ebde7e-e2db-11ee-800b-38fc98f3447b',\n",
       "  'a4ebde7f-e2db-11ee-a4e4-38fc98f3447b',\n",
       "  'a4ebde80-e2db-11ee-89f2-38fc98f3447b',\n",
       "  'a4ebde81-e2db-11ee-8812-38fc98f3447b',\n",
       "  'a4ebde82-e2db-11ee-9fb7-38fc98f3447b',\n",
       "  'a4ebde83-e2db-11ee-8d23-38fc98f3447b',\n",
       "  'a4ebde84-e2db-11ee-8e78-38fc98f3447b',\n",
       "  'a4ebde85-e2db-11ee-81d8-38fc98f3447b',\n",
       "  'a4ebde86-e2db-11ee-9833-38fc98f3447b',\n",
       "  'a4ebde87-e2db-11ee-94ce-38fc98f3447b',\n",
       "  'a4ebde88-e2db-11ee-85fa-38fc98f3447b',\n",
       "  'a4ebde89-e2db-11ee-9621-38fc98f3447b',\n",
       "  'a4ebde8a-e2db-11ee-8c7d-38fc98f3447b',\n",
       "  'a4ebde8b-e2db-11ee-beb6-38fc98f3447b',\n",
       "  'a4ebde8c-e2db-11ee-8115-38fc98f3447b',\n",
       "  'a4ebde8d-e2db-11ee-92f9-38fc98f3447b',\n",
       "  'a4ebde8e-e2db-11ee-aaf8-38fc98f3447b',\n",
       "  'a4ebde8f-e2db-11ee-818d-38fc98f3447b',\n",
       "  'a4ebde90-e2db-11ee-92ed-38fc98f3447b',\n",
       "  'a4ebde91-e2db-11ee-b453-38fc98f3447b',\n",
       "  'a4ebde92-e2db-11ee-83b2-38fc98f3447b',\n",
       "  'a4ebde93-e2db-11ee-af7b-38fc98f3447b',\n",
       "  'a4ebde94-e2db-11ee-839e-38fc98f3447b',\n",
       "  'a4ebde95-e2db-11ee-9ed9-38fc98f3447b',\n",
       "  'a4ebde96-e2db-11ee-a9ae-38fc98f3447b',\n",
       "  'a4ebde97-e2db-11ee-ac1f-38fc98f3447b',\n",
       "  'a4ebde98-e2db-11ee-85b4-38fc98f3447b',\n",
       "  'a4ebde99-e2db-11ee-a1d5-38fc98f3447b',\n",
       "  'a4ebde9a-e2db-11ee-bfe2-38fc98f3447b',\n",
       "  'a4ebde9b-e2db-11ee-850d-38fc98f3447b',\n",
       "  'a4ebde9c-e2db-11ee-aac6-38fc98f3447b',\n",
       "  'a4ebde9d-e2db-11ee-96d1-38fc98f3447b',\n",
       "  'a4ebde9e-e2db-11ee-b25b-38fc98f3447b',\n",
       "  'a4ebde9f-e2db-11ee-86d2-38fc98f3447b',\n",
       "  'a4ebdea0-e2db-11ee-94c6-38fc98f3447b',\n",
       "  'a4ebdea1-e2db-11ee-a17f-38fc98f3447b',\n",
       "  'a4ebdea2-e2db-11ee-935e-38fc98f3447b',\n",
       "  'a4ebdea3-e2db-11ee-80cd-38fc98f3447b',\n",
       "  'a4ebdea4-e2db-11ee-8b67-38fc98f3447b',\n",
       "  'a4ebdea5-e2db-11ee-aae3-38fc98f3447b',\n",
       "  'a4ebdea6-e2db-11ee-9c52-38fc98f3447b',\n",
       "  'a4ebdea7-e2db-11ee-96fa-38fc98f3447b',\n",
       "  'a4ebdea8-e2db-11ee-9fcd-38fc98f3447b',\n",
       "  'a4ebdea9-e2db-11ee-9548-38fc98f3447b',\n",
       "  'a4ebdeaa-e2db-11ee-b107-38fc98f3447b',\n",
       "  'a4ebdeab-e2db-11ee-9a37-38fc98f3447b',\n",
       "  'a4ebdeac-e2db-11ee-aee7-38fc98f3447b',\n",
       "  'a4ebdead-e2db-11ee-917e-38fc98f3447b',\n",
       "  'a4ebdeae-e2db-11ee-bfa3-38fc98f3447b',\n",
       "  'a4ebdeaf-e2db-11ee-9667-38fc98f3447b',\n",
       "  'a4ebdeb0-e2db-11ee-976b-38fc98f3447b',\n",
       "  'a4ebdeb1-e2db-11ee-8207-38fc98f3447b',\n",
       "  'a4ebdeb2-e2db-11ee-8ad1-38fc98f3447b',\n",
       "  'a4ebdeb3-e2db-11ee-81e8-38fc98f3447b',\n",
       "  'a4ebdeb4-e2db-11ee-a8ab-38fc98f3447b',\n",
       "  'a4ebdeb5-e2db-11ee-a446-38fc98f3447b',\n",
       "  'a4ebdeb6-e2db-11ee-9518-38fc98f3447b',\n",
       "  'a4ebdeb7-e2db-11ee-9e65-38fc98f3447b',\n",
       "  'a4ebdeb8-e2db-11ee-bb63-38fc98f3447b',\n",
       "  'a4ebdeb9-e2db-11ee-9452-38fc98f3447b',\n",
       "  'a4ebdeba-e2db-11ee-b40e-38fc98f3447b',\n",
       "  'a4ebdebb-e2db-11ee-b54f-38fc98f3447b',\n",
       "  'a4ebdebc-e2db-11ee-87d0-38fc98f3447b',\n",
       "  'a4ebdebd-e2db-11ee-b865-38fc98f3447b',\n",
       "  'a4ebdebe-e2db-11ee-83af-38fc98f3447b',\n",
       "  'a4ebdebf-e2db-11ee-8f40-38fc98f3447b',\n",
       "  'a4ebdec0-e2db-11ee-883f-38fc98f3447b',\n",
       "  'a4ebdec1-e2db-11ee-a95a-38fc98f3447b',\n",
       "  'a4ebdec2-e2db-11ee-bdd2-38fc98f3447b',\n",
       "  'a4ebdec3-e2db-11ee-b510-38fc98f3447b',\n",
       "  'a4ebdec4-e2db-11ee-9f35-38fc98f3447b',\n",
       "  'a4ebdec5-e2db-11ee-afb7-38fc98f3447b',\n",
       "  'a4ebdec6-e2db-11ee-8201-38fc98f3447b',\n",
       "  'a4ebdec7-e2db-11ee-98df-38fc98f3447b',\n",
       "  'a4ebdec8-e2db-11ee-8c11-38fc98f3447b',\n",
       "  'a4ebdec9-e2db-11ee-b761-38fc98f3447b',\n",
       "  'a4ebdeca-e2db-11ee-adec-38fc98f3447b',\n",
       "  'a4ebdecb-e2db-11ee-9de3-38fc98f3447b',\n",
       "  'a4ebdecc-e2db-11ee-891c-38fc98f3447b',\n",
       "  'a4ebdecd-e2db-11ee-b170-38fc98f3447b',\n",
       "  'a4ebdece-e2db-11ee-946e-38fc98f3447b',\n",
       "  'a4ebdecf-e2db-11ee-a7b5-38fc98f3447b',\n",
       "  'a4ebded0-e2db-11ee-8979-38fc98f3447b',\n",
       "  'a4ebded1-e2db-11ee-b0c7-38fc98f3447b',\n",
       "  'a4ebded2-e2db-11ee-8e6e-38fc98f3447b',\n",
       "  'a4ebded3-e2db-11ee-91d7-38fc98f3447b',\n",
       "  'a4ebded4-e2db-11ee-832c-38fc98f3447b',\n",
       "  'a4ebded5-e2db-11ee-92e3-38fc98f3447b',\n",
       "  'a4ebded6-e2db-11ee-932d-38fc98f3447b',\n",
       "  'a4ebded7-e2db-11ee-a3aa-38fc98f3447b',\n",
       "  'a4ebded8-e2db-11ee-8282-38fc98f3447b',\n",
       "  'a4ebded9-e2db-11ee-abb2-38fc98f3447b',\n",
       "  'a4ebdeda-e2db-11ee-aa0d-38fc98f3447b',\n",
       "  'a4ebdedb-e2db-11ee-8be7-38fc98f3447b',\n",
       "  'a4ebdedc-e2db-11ee-bfc9-38fc98f3447b',\n",
       "  'a4ebdedd-e2db-11ee-b788-38fc98f3447b',\n",
       "  'a4ebdede-e2db-11ee-ae18-38fc98f3447b',\n",
       "  'a4ebdedf-e2db-11ee-880e-38fc98f3447b',\n",
       "  'a4ebdee0-e2db-11ee-b9a9-38fc98f3447b',\n",
       "  'a4ebdee1-e2db-11ee-b595-38fc98f3447b',\n",
       "  'a4ebdee2-e2db-11ee-a504-38fc98f3447b',\n",
       "  'a4ebdee3-e2db-11ee-b272-38fc98f3447b',\n",
       "  'a4ebdee4-e2db-11ee-99ed-38fc98f3447b',\n",
       "  'a4ebdee5-e2db-11ee-8528-38fc98f3447b',\n",
       "  'a4ebdee6-e2db-11ee-b274-38fc98f3447b',\n",
       "  'a4ebdee7-e2db-11ee-a254-38fc98f3447b',\n",
       "  'a4ebdee8-e2db-11ee-a53d-38fc98f3447b',\n",
       "  'a4ebdee9-e2db-11ee-9bc8-38fc98f3447b',\n",
       "  'a4ebdeea-e2db-11ee-ada0-38fc98f3447b',\n",
       "  'a4ebdeeb-e2db-11ee-bf30-38fc98f3447b',\n",
       "  'a4ebdeec-e2db-11ee-9458-38fc98f3447b',\n",
       "  'a4ebdeed-e2db-11ee-8296-38fc98f3447b',\n",
       "  'a4ebdeee-e2db-11ee-888c-38fc98f3447b',\n",
       "  'a4ebdeef-e2db-11ee-a819-38fc98f3447b',\n",
       "  'a4ebdef0-e2db-11ee-a22c-38fc98f3447b',\n",
       "  'a4ebdef1-e2db-11ee-a1ec-38fc98f3447b',\n",
       "  'a4ebdef2-e2db-11ee-99b1-38fc98f3447b',\n",
       "  'a4ebdef3-e2db-11ee-b67f-38fc98f3447b',\n",
       "  'a4ebdef4-e2db-11ee-be07-38fc98f3447b',\n",
       "  'a4ebdef5-e2db-11ee-97dd-38fc98f3447b',\n",
       "  'a4ebdef6-e2db-11ee-ab00-38fc98f3447b',\n",
       "  'a4ebdef7-e2db-11ee-a674-38fc98f3447b',\n",
       "  'a4ebdef8-e2db-11ee-9985-38fc98f3447b',\n",
       "  'a4ebdef9-e2db-11ee-b805-38fc98f3447b',\n",
       "  'a4ebdefa-e2db-11ee-8276-38fc98f3447b',\n",
       "  'a4ebdefb-e2db-11ee-a3cb-38fc98f3447b',\n",
       "  'a4ebdefc-e2db-11ee-81ef-38fc98f3447b',\n",
       "  'a4ebdefd-e2db-11ee-9de4-38fc98f3447b',\n",
       "  'a4ebdefe-e2db-11ee-9707-38fc98f3447b',\n",
       "  'a4ebdeff-e2db-11ee-83e0-38fc98f3447b',\n",
       "  'a4ebdf00-e2db-11ee-b501-38fc98f3447b',\n",
       "  'a4ebdf01-e2db-11ee-a834-38fc98f3447b',\n",
       "  'a4ebdf02-e2db-11ee-a7b3-38fc98f3447b',\n",
       "  'a4ebdf03-e2db-11ee-ae8f-38fc98f3447b',\n",
       "  'a4ebdf04-e2db-11ee-847d-38fc98f3447b',\n",
       "  'a4ebdf05-e2db-11ee-8009-38fc98f3447b',\n",
       "  'a4ebdf06-e2db-11ee-afef-38fc98f3447b',\n",
       "  'a4ebdf07-e2db-11ee-9496-38fc98f3447b',\n",
       "  'a4ebdf08-e2db-11ee-a081-38fc98f3447b',\n",
       "  'a4ebdf09-e2db-11ee-a8cf-38fc98f3447b',\n",
       "  'a4ebdf0a-e2db-11ee-bfa2-38fc98f3447b',\n",
       "  'a4ebdf0b-e2db-11ee-b77d-38fc98f3447b',\n",
       "  'a4ebdf0c-e2db-11ee-850c-38fc98f3447b',\n",
       "  'a4ebdf0d-e2db-11ee-b373-38fc98f3447b',\n",
       "  'a4ebdf0e-e2db-11ee-befc-38fc98f3447b',\n",
       "  'a4ebdf0f-e2db-11ee-a932-38fc98f3447b',\n",
       "  'a4ebdf10-e2db-11ee-91f9-38fc98f3447b',\n",
       "  'a4ebdf11-e2db-11ee-8b47-38fc98f3447b',\n",
       "  'a4ebdf12-e2db-11ee-998a-38fc98f3447b',\n",
       "  'a4ebdf13-e2db-11ee-baf2-38fc98f3447b',\n",
       "  'a4ebdf14-e2db-11ee-ba85-38fc98f3447b',\n",
       "  'a4ebdf15-e2db-11ee-8e75-38fc98f3447b',\n",
       "  'a4ebdf16-e2db-11ee-89c1-38fc98f3447b',\n",
       "  'a4ebdf17-e2db-11ee-a38a-38fc98f3447b',\n",
       "  'a4ebdf18-e2db-11ee-a33d-38fc98f3447b',\n",
       "  'a4ebdf19-e2db-11ee-bf62-38fc98f3447b',\n",
       "  'a4ebdf1a-e2db-11ee-b540-38fc98f3447b',\n",
       "  'a4ebdf1b-e2db-11ee-a8f8-38fc98f3447b',\n",
       "  'a4ebdf1c-e2db-11ee-a748-38fc98f3447b',\n",
       "  'a4ebdf1d-e2db-11ee-9e54-38fc98f3447b',\n",
       "  'a4ebdf1e-e2db-11ee-ae65-38fc98f3447b',\n",
       "  'a4ebdf1f-e2db-11ee-8ead-38fc98f3447b',\n",
       "  'a4ebdf20-e2db-11ee-bfda-38fc98f3447b',\n",
       "  'a4ebdf21-e2db-11ee-a18b-38fc98f3447b',\n",
       "  'a4ebdf22-e2db-11ee-878b-38fc98f3447b'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 0,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 1,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 2,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 3,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 4,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 5,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 6,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 7,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 9,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 10,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 11,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 12,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 13,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 14,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 16,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 17,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 18,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 20,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 21,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 22,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 23,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 25,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 26,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 27,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 28,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 29,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 30,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 31,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 32,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 33,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 34,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 35,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 36,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 37,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 38,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 39,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 40,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 41,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 42,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 43,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 44,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 45,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 46,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 47,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 48,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf',\n",
       "   'page': 49,\n",
       "   'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 0,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 1,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 2,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 3,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 4,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 5,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 6,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 7,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 9,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 10,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 11,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 12,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 13,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 14,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 16,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 17,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 18,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 20,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 21,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 22,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 23,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 25,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 26,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 27,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 28,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 29,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 30,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 31,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 32,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 33,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 34,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 35,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 36,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 37,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 38,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 39,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 40,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 41,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 42,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 43,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 44,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 45,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 46,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf',\n",
       "   'page': 47,\n",
       "   'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 0,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 1,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 2,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 3,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 4,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 5,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 6,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 7,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 9,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 10,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 11,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 12,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 13,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 14,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 16,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 17,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 18,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 20,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 21,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 22,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 23,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 25,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 26,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 27,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 28,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 29,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 30,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf',\n",
       "   'page': 31,\n",
       "   'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 0,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 1,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 2,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 3,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 4,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 5,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 6,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 7,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 9,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 10,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 11,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 12,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 13,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 14,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 16,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 17,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 18,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 20,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 21,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 22,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 23,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 25,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 26,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 27,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 28,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 29,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 30,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 31,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 32,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 33,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 34,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 35,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 36,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 37,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 38,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 39,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 40,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 41,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS3%20Fundamental%20Limits%20of%20ML.pdf',\n",
       "   'page': 42,\n",
       "   'source': '.\\\\documents\\\\EDS3 Fundamental Limits of ML.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 0,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 1,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 2,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 3,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 4,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 5,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 6,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 7,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 8,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 9,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 10,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 11,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 12,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 13,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 14,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 15,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 16,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 17,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 18,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 19,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 20,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 21,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 22,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 23,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 24,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 25,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 26,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 27,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 28,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 29,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 30,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 31,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 32,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 33,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 34,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 35,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 36,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 37,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 38,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 39,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 40,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 41,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 42,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 43,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 44,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 45,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 46,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 47,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 48,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 49,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 50,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 51,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 52,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 53,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 54,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 55,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 56,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 57,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 57,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 58,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 59,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 59,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 59,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 59,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 60,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 61,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 62,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 63,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 64,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 65,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 66,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 67,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 68,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 69,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 70,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 71,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 72,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 73,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 74,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 75,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 76,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 77,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 78,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 79,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 80,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 81,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 82,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 83,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 84,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 85,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 86,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 87,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 88,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 89,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 90,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 91,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 92,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 93,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 94,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 95,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 96,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 97,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 98,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 99,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 100,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 101,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 102,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 103,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 104,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 105,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 106,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 107,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 108,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 109,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 110,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 111,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 112,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 113,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 114,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 115,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 116,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 117,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 118,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 119,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 120,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 121,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 122,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 123,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 124,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 125,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 126,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 127,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 128,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 129,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 130,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 131,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 132,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 133,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 134,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 135,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 136,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 137,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 138,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 139,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 140,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 141,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 142,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 143,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'},\n",
       "  {'field': 'Theory',\n",
       "   'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf',\n",
       "   'page': 144,\n",
       "   'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'}],\n",
       " 'documents': ['Jordi Vitrià\\nIntroduction  +  Data Science in Context',\n",
       "  '•Data science has the potential to be both beneﬁcial (Improved Decision-Making, Predictive Analytics, Personalized Services, Efﬁciency and Automation, etc.) and detrimental (Privacy Concerns, Bias and Fairness Issues, Security Risks, Loss of Jobs, Data Manipulation, etc.) to individuals (individual harms) and/or to society (sistemic risks).  •To help eliminate/mitigate any adverse effects, we must seek to understand the potential impact of our work for people. •In this course, we will explore the social and ethical ramiﬁcations of the choices we make at the different stages of the data analysis pipeline, from data collection and storage to understand feedback loops in the analysis.  •Through case studies and exercises, students will learn the basics of causal thinking, ethical thinking, understand some tools to check or mitigate undesired effects and study the distinct challenges associated with ethics in modern data science.Introduction\\n2',\n",
       "  'Course Instructors •Jordi Vitrià  (https:/algorismes.github.io),  Departament de Matemàtiques i Informàtica de la UB. •Itziar de Lecuona  (http:/www.bioeticayderecho.ub.edu/ca/itziar-de-lecuona),  Bioethics and Law Observatory at the University of Barcelona. Introduction\\n3',\n",
       "  'Calendar (tentative)\\n4',\n",
       "  '•Proﬁciency in Python. •Calculus, Linear Algebra. •Basic Probability and Statistics. •Critical Thinking.Prerequisites\\n5Critical thinking is the ability to think clearly and rationally, understanding the logical connection between ideas.',\n",
       "  \"•The subject will be evaluated through a combination of both an exam (50%) and practical assignments (50%).  •The exam will test the students' theoretical understanding of the material covered in class.  •The practical assignments/case studies, on the other hand, will give students the opportunity to apply what they have learned in class to real-world scenarios and will be used to evaluate their practical skills and abilities.Grading\\n6Example:  To study the limitations of Machine Learning (ML) algorithms for predicting  juvenile recidivism.  Recividism prediction:  The act of a person committing a crime after they have been convicted of an earlier crime.\",\n",
       "  '7DS in Context',\n",
       "  'While there is no single deﬁnition of data science, it can be broadly thought of as the systematic analysis of the scientiﬁc, computational and analytical methods (methodology) used to process and extract information, knowledge, and insights from data to inform decision-making (or to act in an automatic way).  There is a clear intersection with data-centric AI.  Data-Centric AI is the discipline of systematically engineering the data used to develop AI competences / tools (such as ML, NLP, Vision, etc.). Data Science and AI\\n8\\nDSDLMLAI',\n",
       "  'In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n9Data is a matter of describing things as they are, and there is no art to it and certainly no fashion!! We want to be objective and should let things speak for themselves!!',\n",
       "  \"In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n10Data is a matter of describing things as they are, and there is no art to it and certainly no fashion!! We want to be objective and should let things speak for themselves!!\\nNaive              Data, in its raw form, consists of numbers, text, or other symbols that represent information. However, without context and analysis, these representations lack meaning. It's the role of data scientists, analysts, and researchers to interpret this data—by analyzing patterns, trends, and anomalies—to derive insights and conclusions.\",\n",
       "  'In what ways can machine learning become unfair without any intentional wrongdoing?! Motivation: harms, unfairness, risks…\\n11\\nGenerally, having more data available tends to decrease the number of errors in machine learning applications. However, when it comes to minority groups, having unlimited data can still lead to high error rates.Credit: Moritz Hardt',\n",
       "  'Motivation: harms, unfairness, risks…\\n12\\nCredit: Moritz Hardt',\n",
       "  \"Motivation: harms, unfairness, risks…\\n13\\nMachine Learning (ML) can inadvertently reinforce stereotypes due to biases in the data it's trained on and the way these systems are designed and deployed.\",\n",
       "  'Motivation: harms, unfairness, risks…\\n14',\n",
       "  'Motivation: harms, unfairness, risks…\\n15',\n",
       "  'Motivation: harms, unfairness, risks…\\n16\\nSyRI (Systeem Risico Indicatie)  Public Administration that has implemented it: Social Protection, at central and municipal level. Description of the innovation:  In 2012, the Dutch Tax Agency began using self-learning algorithms to create fraud risk proﬁles in order to prevent child care beneﬁt fraud. Expected impact:  Enhanced inspection capabilities, improved child welfare, reduction of misuse of public funds',\n",
       "  'Motivation\\n17\\nResult: After a few years of being in operation, this system was withdrawn (2022) due to clearly negative consequences (and the acting government resigned).  The algorithm had been developed in such a way that it categorized as debtors families who had ﬁlled out the application documents incorrectly. At the same time, having dual nationality also inﬂuenced this proﬁling, as well as coming from a low socioeconomic level, being immigrants or belonging to an ethnic minority were characteristics that led the algorithm to disproportionately penalize these population groups.  As a result, more than 10,000 people fell into poverty, others died by suicide after receiving debt bills for impossible amounts, and even more than 1,100 children were separated from their families and put in reception centers. A total of 30,000 families were affected by this algorithm.',\n",
       "  'As data science methods become more common within different ﬁelds, there are both opportunities and challenges for individuals working in data science.  For example, managing privacy, fairness, and bias issues when working with people’s data can be difﬁcult and complex. Approach\\n18',\n",
       "  'Additionally, public perceptions are still developing around many aspects of data-based technology, including the use of artiﬁcial intelligence (AI) in systems and decision making, and ‘big data’ sources about people, such as social media and mobile phone data.  This course is focused on both, giving a theoretical basis and providing the necessary tools to keep up with these challenging ethical issues. Approach\\n19',\n",
       "  '20ApproachData-empowered algorithms are reshaping our personal, professional, and political realities, and they are likely to have an even larger effect going forward.  However, as with all developing technologies, increases in impact inevitably give rise to unanticipated consequences.  These challenge our norms for how we use technology in ways consistent with our values. Many scholars, educators, and technology companies refer to these as ethical challenges.',\n",
       "  'Learning outcomes: •Understand the impacts of data/models misuse. •Develop your ability to investigate how data and data-powered algorithms shape, constrain, and manipulate our commercial, civic, and personal experiences. •Develop you ability to identify and mitigate potential risks. •Have a toolkit to implement in your workplaces. Ultimately, to redirect your thinking from what is merely advantageous to what is genuinely good — and be prepared to help you navigate the ethical aspects of DS development and deployment.Approach\\n21',\n",
       "  '22Data science is the study of extracting value from data – value in the form of insights or conclusions.Data Science in Context\\n○ A hypothesis, testable with more data; ○ An “aha!” that comes from a succinct statistic or an apt visual chart; or ○A plausible relationship among variables of interest, uncovered by examining the data and the implications of different scenarios. ○Etc.○ Prediction of a consequence; ○ Recommendation of a useful action; ○ Clustering that groups similar elements; ○ Classiﬁcation that labels elements in groupings; ○ Transformation that converts data to a more useful form; or ○ Optimization that moves a system to a better state.',\n",
       "  '23Insights and conclusions often arise from models , which are abstractions of the real world. \\nModels that generate these conclusions may be clear box or black box . A clear box model’s logic is available for inspection by others, while an black box model’s logic is not. The “opaque box” term also apply to a model whose operation is not comprehensible. Data Science in Context',\n",
       "  '24Data Science in ContextResponsible model development refers to the practice of models in a manner that prioritizes ethical, fair, and accountable considerations throughout the lifecycle of the model.  The goal is to ensure that these systems are designed, deployed and maintained in ways that minimize harm, maximize beneﬁts, and adhere to societal norms and values.We must consider all stakeholders!',\n",
       "  '25Data Science in ContextDataApproachDependabilityUnderstandabilityFocusToleranceELSIConsider whether data of sufﬁcient integrity, size, quality, and manageability exists or could be obtained.Consider whether there is a technical approach grounded in data, such as an analysis, a model, or an interactive visualization, that can achieve the desired result.Does the application meet needed privacy protections?  Is its security sufﬁcient to thwart attackers who try to break it? Does it resist the abuse of malevolent users?  Does it have the resilience to operate correctly in the face of unforeseen circumstances or changes to the world?Will the application need to detail the causal chain underlying its conclusions?  Or will it make its underlying data and associated models, software, and techniques transparent and provide reproducibility?Consider whether the application is trying to achieve well-speciﬁed objectives that align with what we truly want to happen.Consider the application',\n",
       "  'the abuse of malevolent users?  Does it have the resilience to operate correctly in the face of unforeseen circumstances or changes to the world?Will the application need to detail the causal chain underlying its conclusions?  Or will it make its underlying data and associated models, software, and techniques transparent and provide reproducibility?Consider whether the application is trying to achieve well-speciﬁed objectives that align with what we truly want to happen.Consider the application holistically with regard to legality, risk, and ethical considerations. Many of the topics under Dependability or Clear Objectives topics are relevant here.Consider both the possible unintended side effects if the objective is not quite right and the possible damage from failing to meet objectives.Here are some key principles and practices associated with responsible model development:',\n",
       "  '26ExampleMusic Recommendation Music recommendation has few legal issues and fewer risks than other domains (although, for example, it is crucial to be careful about recommending obscene lyrics to minors).  However, there are many ethical issues relating to the type of recommendations made and their impact on individual listeners, their community, and the creator/artist whose success may be at the mercy of these algorithms.',\n",
       "  '27Why Ethics?   in technology, data science, AI…',\n",
       "  '“Everything that is not forbidden by laws of nature is achievable,  given the right knowledge”  (Credit: David Deutsch) But that’s the problem.  “Everything” means everything: vaccines and bioweapons,   video on demand and Big Brother on the tele-screen.  Something in addition to science ensured that vaccines were put  to use in eradicating diseases while bioweapons were outlawed.    Fragment de: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”. Apple Books. Scientific point of view',\n",
       "  'Scientific point of view',\n",
       "  '30Kranzberg’s First Law:  “Technology is neither good nor bad;  nor is it neutral.”  By which he means that, “technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”What was the main (unexpected) consequence of the agricultural revolution? What is the main (unexpected) consequence of the industrial revolution?',\n",
       "  '31How to manage the unintended  consequences of DS/AI?',\n",
       "  '32\\nIndustry self-regulation is the process whereby members of an industry, trade or sector of the economy monitor their own adherence to legal, ethical, or safety standards, rather than have an outside, independent agency such as a third party entity or governmental regulator monitor and enforce those standards.',\n",
       "  '33\\nChecklists',\n",
       "  '34\\nThere are hundreds of documents about ethical guidelines!',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37\\nLaw',\n",
       "  '38Resposible AI Standards',\n",
       "  '39Data and Ethics',\n",
       "  '40The combination of data analytics, a data-saturated and poorly regulated environment, and the absence of widespread, well-designed standards for data practice in industry, university, non-proﬁt, and government sectors has created a ‘perfect storm’ of ethical risks.  Thus no single set of ethical rules or guidelines will ﬁt all data circumstances; ethical insights in data practice must be adapted to the needs of many kinds of data practitioners operating in different contexts. What does ethics have to do with data?',\n",
       "  '41What does ethics have to do with data?We can deﬁne a harm or a beneﬁt as ‘ethically signiﬁcant’ when it has a substantial possibility of making a difference to certain individuals’ chances of having a good life, or the chances of a group to live well: that is, to ﬂourish in society together.  Some harms and beneﬁts are not ethically signiﬁcant. Say I prefer Coke to Pepsi. If I ask for a Coke and you hand me a Pepsi, even if I am disappointed, you haven’t impacted my life in any ethically signiﬁcant way.',\n",
       "  '42In the context of data practice, the potential harms and beneﬁts are real and ethically signiﬁcant.  But due to the more complex, abstract, and often widely distributed nature of data practices, as well as the interplay of technical, social, and individual forces in data contexts, the harms and beneﬁts of data can be harder to see and anticipate. In this respect, then, data has a broader ethical sweep than engineering of bridges and airplanes. Data practitioners must confront a far more complex ethical landscape than many other kinds of technical professionals…What does ethics have to do with data?',\n",
       "  '43HUMAN UNDERSTANDING:  Because data and its associated practices can uncover previously unrecognized correlations and patterns in the world, data can greatly enrich our understanding of signiﬁcant relationships — in nature, society, and our personal lives.  Ethical Benefits of Data Practices',\n",
       "  '44SOCIAL, INSTITUTIONAL, AND ECONOMIC EFFICIENCY:  Once we have a more accurate picture of how the world works, we can design or intervene in its systems to improve their functioning.  This reduces wasted effort and resources and improves the alignment between a social system or institution’s policies/processes and our goals. Ethical Benefits of Data Practices',\n",
       "  '45EFFECTIVENESS AND PERSONALIZATION:  Not only can good data practices help to make social systems work more efﬁciently, but they can also used to more precisely tailor actions to be effective in achieving good outcomes for speciﬁc individuals, groups, and circumstances, and to be more responsive to user input in (approximately) real time. Ethical Benefits of Data Practices',\n",
       "  '46HARMS TO PRIVACY & SECURITY:  Thanks to the ocean of personal data that humans are generating today (or, to use a better metaphor, the many different lakes, springs, and rivers of personal data that are pooling and ﬂowing across the digital landscape), most of us do not realize how exposed our lives are, or can be, by common data practices. Ethical Harms of Data Practices',\n",
       "  '47HARMS TO FAIRNESS AND JUSTICE:  We all have a signiﬁcant interest in being judged and treated fairly, whether it involves how we are treated by law enforcement and the criminal and civil court systems, how we are evaluated by our employers and teachers, the quality of health care and other services we receive, or how ﬁnancial institutions and insurers treat us. Ethical Harms of Data Practices',\n",
       "  '48HARMS TO TRANSPARENCY AND AUTONOMY:  In this context, transparency is the ability to see how a given social system or institution works, and to be able to inquire about the basis of life-affecting decisions made within that system or institution.  So, for example, if your bank denies your application for a home loan, transparency will be served by you having access to information about exactly why you were denied the loan, and by whom. Autonomy is the state that results from being able to make informed free decisions. Ethical Harms of Data Practices',\n",
       "  '49Europe’s GDPR',\n",
       "  '50The   GDPR   can   be   summarized   in   the   following points: 1. It   concerns   “Personal   Data”:   Name,   address, localization,  online  identiﬁer,  health  information, income, cultural proﬁle, ...  2. Communication:   Who   gets   the   data,   why, for how long? (No use for other ‘incompatible’ purposes. Use as long as necessary.) 3. Consent: Get clear informed consent. 4. Access: Provide access to my data. 5. Right to be forgotten (not for research). 6. Right to explanation for contracts (& right to have a person decide). 7. Marketing: Right to opt out. 8. Legal: Maintain EU legislation when transferring data out. 9. Need for a “data protection ofﬁcer” in your organisation. 10. Impact   assessment   prior   to   high-risk   processing (new technology, personal information, surveillance, sensitive). Europe’s GDPR',\n",
       "  'Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science   Foundations  1',\n",
       "  '2Preliminaries',\n",
       "  \"Is there a common ground to talk about what is right and what is wrong? Ethical relativism is the theory that holds that morality is relative to the norms of one's culture. That is, whether an action is right or wrong depends on the moral norms of the society in which it is practiced.So, to be able to advance, let’s assume a common ground (beign aware of its limitations) based on a revision of the enlightenment, a framework that tries to encompass rationality, science, humanism and progress.\",\n",
       "  'Once upon a time…\\n4\\nLet’s see how this book proposes a “common ground”…\\nS.Pinker is a cognitive scientist…..',\n",
       "  '“…the most arresting question I have ever ﬁelded followed a talk in which I explained the commonplace among scientists that mental life consists of patterns of activity in the tissues of the brain.”  “A student in the audience raised her hand and asked me: “Why should I live?” “What I recall saying … went something like this:…” \\nFragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n5',\n",
       "  '“In the very act of asking that question, you are seeking reasons for your convictions, and so you are committed to reason as the means to discover and justify what is important to you. (…) \\nFragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n6Proposition 1: The basis\\nA reason is an explanation of a situation or an event that provides a logical basis for a conclusion, belief, or action.',\n",
       "  'As a sentient being, you have the potential to ﬂourish. You can reﬁne your faculty of reason itself by learning and debating. You can seek explanations of the natural world through science, and insight into the human condition through the arts and humanities. You can make the most of your capacity for pleasure and satisfaction, which allowed your ancestors to thrive and thereby allowed you to exist. (…)” “(…) You can appreciate the beauty and richness of the natural and cultural world. As the heir to billions of years of life perpetuating itself, you can perpetuate life in turn. You have been endowed with a sense of sympathy—the ability to like, love, respect, help, and show kindness—and you can enjoy the gift of mutual benevolence with friends, family, and colleagues.” Fragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n7Proposition 2: You as an individual',\n",
       "  'And because reason tells you that none of this is particular to you, you have the responsibility to provide to others what you expect for yourself. You can foster the welfare of other sentient beings by enhancing life, health, knowledge, freedom, abundance, safety, beauty, and peace. History shows that when we sympathize with others and apply our ingenuity to improving the human condition, we can make progress in doing so, and you can help to continue that progress.” Fragment from: Steven Pinker. “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress”.Once upon a time…\\n8Proposition 3: You as a member of a society',\n",
       "  'The previous position statement assumes a lot of things about the world that are not self-evident (these are the ideas of the Enlightenment).  Not everybody agree on those statements!   But this is a course on applied ethics, and we need a starting point for the discussion. This will be our provisional starting point. Assumptions\\n9Christians, Jews, and Muslims embrace ethical codes of moral absolutes based on God’s character or moral decree.    Secular Humanists, Marxists, and Postmodernists ground their ethical systems in atheism, naturalism, and evolution.',\n",
       "  '10The role of technology in societyMankind has not changed biologically throughout history but human society is undergoing continuous development through the harnessing of information and knowledge in the form of various technologies which have affected our value systems, power structures, everyday routines and environment. History begins with the accounts of the ancient world around the 4th millennium BC, and it coincides with the invention of writing.',\n",
       "  '11The role of technology in societyThe course of human development can be grouped into three time periods separated by \"revolutions\": •The Cognitive Revolution began history about [50,000, 70, 70,000] years ago. •The Agricultural Revolution accelerated it about 12,000 years ago. •The Scientiﬁc Revolution, which began only 500 years ago, has made possible the industrial age and the world as we know it today. A Revolution is associated with a change, often of a technological nature, that causes the human species to change its way of life (organization of work, social organization, cultural practices, etc.). Concepts such as big data, machine learning, artiﬁcial intelligence and data science are making possible a new Revolution, the Digital, which can have as much or deeper consequences than the previous ones.',\n",
       "  '12\\nKranzberg’s Six Laws of Technology The role of technology in societyKranzberg’s First Law:  “Technology is neither good nor bad;  nor is it neutral.”  By which he means that, “technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”Dr. Melvin Kranzberg was a professor of the history of technology at the Georgia Institute of Technology',\n",
       "  '13Technologies are not ethically ‘neutral’, for they reﬂect the values that we ‘bake in’ to them with our design choices, as well as the values which guide our distribution and use of them.  Technologies both reveal and shape what humans value, what we think is ‘good’ in life and worth seeking.The role of technology in society',\n",
       "  '14Not only does technology greatly impact our opportunities for living a good life, but its positive and negative impacts are often distributed unevenly among individuals and groups.  Technologies can create widely disparate impacts, creating ‘winners’ and ‘losers’ in the social lottery or magnifying existing inequalities The role of technology in society',\n",
       "  '15In other cases, technologies can help to create fairer and more just social arrangements, or create new access to means of living well How do we ensure that access to the enormous beneﬁts promised by new technologies, and exposure to their risks, are distributed in the right way? This is a matter of ethics.The role of technology in society',\n",
       "  '16',\n",
       "  'Health care organizations, like many other enterprises, face steep challenges in their attempt to maximize operational efﬁciency in the face of resource constraints. Whether it is a hospital’s attempt to optimize stafﬁng or a government trying to fairly allocate and distribute limited doses of Covid-19 vaccines, these tasks can be formidable. A promising way to manage the complexity is to enlist data-driven analytics and artiﬁcial intelligence (AI).However, such techniques, while powerful, can also mask problematic underlying ethical assumptions or lead to morally questionable outcomes. Consider a recently published study about models used by some of the most technologically advanced hospitals in the world to help prioritize which patients with chronic kidney disease should receive kidney transplants. It found that the models discriminated against black patients: “One-third of Black patients … would have been placed into a more severe category of kidney disease if their kidney',\n",
       "  'can also mask problematic underlying ethical assumptions or lead to morally questionable outcomes. Consider a recently published study about models used by some of the most technologically advanced hospitals in the world to help prioritize which patients with chronic kidney disease should receive kidney transplants. It found that the models discriminated against black patients: “One-third of Black patients … would have been placed into a more severe category of kidney disease if their kidney function had been estimated using the same formula as for white patients.” While it is just the latest of many studies to show the deﬁciencies of such models, it is unlikely to be the last.Cases',\n",
       "  '17Cases\\nhttps://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/Public services have to be efﬁcient, consistent, objective, etc.  Automatic decision systems can help to this challenge. But…',\n",
       "  '18Ethics and AlgorithmsWe will specially consider the case of algorithms that are used to  1.turn data into evidence for a given outcome, which is used to,  2.trigger and motivate an action that may have ethical consequences.  Actions (1) and (2) may be performed by data-driven automatic algorithms —such as machine  learning (ML) algorithms— and this complicates the attribution of responsibility for the effects of actions that  an algorithm may trigger. Why?',\n",
       "  '19Ethics and AlgorithmsThere are, at least, 5 types of ethical concerns: 1.Inconclusive evidence. 2.Inescrutable evidence. 3.Misguided evidence. 4.Unfair outcomes. 5.Transformative effects.\\nEpistemic factors\\nNormative concernsThe epistemic factors in the map highlight the relevance of the quality and accuracy of the data for the justiﬁability of the conclusions that algorithms reach and which, in  turn, may shape morally-loaded decisions affecting individuals, societies, and the environment.  The normative concerns identiﬁed in the map refer explicitly to the ethical impact of algorithmically-driven actions and decisions,  including lack of transparency (opacity) of algorithmic processes, unfair outcomes, and unintended consequences.https://link.springer.com/content/pdf/10.1007/s00146-021-01154-8.pdf',\n",
       "  '20From another point of view, ethical concerns can be divided in three different time frames/areas: • Short-term/person, organization: What is the impact of [privacy, transparency, fairness] in my application? • Medium-term/society: How the use [military use, medical care, justice, education] of these applications will change the way we are organized as a society? • Long-temr/humans: What are the ethical goals of these technologies?GDPR…Autonomous weapons, pre-pol, AI justice,…Singularity, convergence…Applied Ethics Problems',\n",
       "  '21Laws are coming in 2024, including the EU AI Act!',\n",
       "  '22What is Ethics?',\n",
       "  'Definitions\\n23Ethics is the process of questioning, discovering and defending your values, principles and purposes in order to be able of deciding what is right and what is wrong. \\nEthics seeks to answer questions like “what is good or bad”, “what is right or what is wrong”, or “what is justice, well-being or equality”. Applied ethics concerns what a moral agent is obligated or permitted to do in a speciﬁc situation or a particular domain of action.\\nPhilosophySciencePurpose\\nValuesPrinciplesSocietyYour KnowledgeYour Beliefs',\n",
       "  '24How do we make decisions?\\nDesires NeedsPurpose Principles ValuesKnowledge',\n",
       "  '25Purpose\\nValuesPrinciplesYour reason for being. Leaving the world better than I found it.\\nThe things that are goodJustice, knowledge, equality,…Lines I’ll never cross.Beliefs, the necessary ingredients of a good individual decision.\\nTreat other people the way you would like to be treated.PhilosophySocietyScienceYour Beliefs',\n",
       "  '26Beliefs, the necessary ingredients of a good individual decision.\\nNowadays, the most common approach is to consider that, rather than grounding our beliefs on a solid foundation, we assemble a collection of beliefs that hold together under a mutual (maybe unstable) knowledge attraction. PurposeValuesPrinciples\\nTraditional approaches consider that person’s beliefs must be grounded in a solid foundation that cannot be discussed (God, rationality and harm, etc).',\n",
       "  '27Purpose\\nValuesPrinciplesKnowledge, our vision of the world\\nLaw\\nMoralPhilosophySocietyBeliefs about RealityYour KnowledgeYour Environment',\n",
       "  '28How do we make decisions?\\nDesires NeedsPurpose Principles ValuesLaw Moral',\n",
       "  '29LawLaws are formal rules that govern how we behave as members of a society. They specify what we must do, and more frequently, what we must not do. They create an enforceable standard of behavior.  Laws can be just or unjust, because they are subject to ethical assessment. Law cannot be applied to every decision: it cannot say anything about what to do when you hear a friend to make a racist joke…',\n",
       "  '30In an ideal world, our ethical beliefs shape law and moral systems. We need a toolkit to run our reﬂections!The role of ethics is not to be a soft version of the law, even if laws are based on ethical principles.  The  real  application  of  ethics  lies  in  challenging the status quo, seeking its deﬁcits  and blind spots. N.Kluge Corrêa, Good AI for the Present of Humanity. Democratizing AI Governance How do we take decisions?',\n",
       "  '31Morality refers to an informal social framework of values, beliefs, principles, customs and ways of living. Examples: christianity, stoicism, buddhism… Moral systems provide a set of answers to general ethical questions.  Morality is, in most of the cases, inherited (unconsciously) from family, community or culture. Morality is applied as a matter of habbit, without having to think.  In most cases, there are moral authorities.How do we take decisions?',\n",
       "  '32You can take decisions exclusively based on laws and morality, but this should not be enough. Ethics is a process of reﬂection that aims to answer this question: What should I do? The answer is based on our values, principles and purposes rather than social conventions.  An ethical decision is based on conscious, rational reﬂection. How do we take decisions?',\n",
       "  '33Traditional Normative EthicsThere are three traditional theories of what it means to be ethical: •Utilitarianism (J.Bentham): Does an action maximize happiness and well-being for all affected individuals? (consequences) •Deontology (I.Kant): Does an action follow a moral rule (e.g. the Golden Rule: ‘Treat others how you want to be treated’)? An action should be based on whether that action itself is right or wrong under a series of rules, rather than based on the consequences of the action. (beliefs) •Virtue Ethics (Aristotle): Does an action contribute to virtue? (justice, honesty, responsibility, care, etc.)',\n",
       "  '34Traditional Normative Ethics\\nAsimov’s Three Laws of Robotics are an example of deontological approach to AI ethics.1.A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2.A robot must obey orders given it by human beings except where such orders would conﬂict with the First Law. 3.A robot must protect its own existence as long as such protection does not conﬂict with the First or Second Law.',\n",
       "  '35Traditional EthicsSuppose it is obvious that someone in need should be helped.  •A utilitarian will point to the fact that the consequences of doing so will maximize well-being.  •A deontologist will point to the fact that, in doing so the agent will be acting in accordance with a moral rule such as “Do unto others as you would be done by”.   •A virtue ethicist will point to the fact that helping the person would be charitable or benevolent. \\nhttps:/ /plato.stanford.edu/entries/ethics-virtue/',\n",
       "  '36(Political) Philosophy\\n4 theories about what is right and what is wrong in society',\n",
       "  '37\\nJohn RawlsJohn Rawls tried to work out how people would construct their society if the choice had to be made behind what he called a “veil of ignorance” about whether they will be rich, poor or somewhere in-between.  Faced with the risk of being the worst off, Rawls posited, humans would not demand total equality, but would need to be assured of the trappings of a modern welfare state. The assurance of basic necessities and the opportunity to do better would form the foundation for social and political justice and provide the ability for people to assert themselves.\\nRawlsians(Political) Philosophy',\n",
       "  \"38\\nJohn LockeA man had a right to live for himself and an individual’s happiness cannot be prescribed by another man or any number of other men. Libertarianism holds that the basic moral concepts are individual rights and that the rights to be respected are noninterference rights. These generally fall under the heading of rights to life, to liberty or to property.  For libertarianism, the only proper limit to one person's enjoyment of these rights is his or her duty to respect the similar rights of others.\\nLibertarians(Political) Philosophy\",\n",
       "  '39\\nJohn Stuart MillRulers must be guided to the total happiness, or “utility,” of all the people, and should aim to secure “the greatest good for the greatest number.” Utilitarian calculus opens up the possibility that in situations such as a pandemic, some people might justly be sacriﬁced for the greater good. It would beneﬁt society to accept casualties.\\nUtilitarians\\n(Political) Philosophy',\n",
       "  '40\\nMichael SandelEveryone derives their identify from the broader community.  Individual rights count, but not more than community norms. Justice cannot be determined in a vacuum or behind a veil of ignorance, but must be rooted in society (common good).\\nCommunitarians(Political) Philosophy',\n",
       "  '41Only west-centric values?\\nThat most AI ethics guidelines are being written in Western countries means that the ﬁeld is dominated by Western values such as respect for autonomy and the rights of individuals, especially since the few guidelines issued in other countries mostly reﬂect those in the West.Buddhism proposes a way of thinking about ethics based on the assumption that all sentient beings want to avoid pain. Thus, the Buddha teaches that an action is good if it leads to freedom from suffering.Another key concept in Buddhism is compassion, or the desire and commitment to eliminate suffering in others.',\n",
       "  '42Canonical views of AI ethics?\\nValue diversity & Pragmatism',\n",
       "  '43Ethics approachesThe normative approach to ethics focuses on how the world should be.\\nThe positive approach to ethics describes the world as it is.  It is about how humans judge situations and decisions in different scenarios.  \\nhttps://existentialcomics.com/comic/424',\n",
       "  '44An alternative approach to ethicsIt is about how humans judge situations and decisions in different scenarios.  This is done by focusing our understanding of the world on empirically veriﬁable effects that we can later explore through normative approaches. For instance, empirical work has shown that people exhibit algorithmic aversion, a bias where people tend to reject algorithms even when they are more accurate than humans.Dietvorst BJ, Simmons JP, Massey C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. Journal of Experimental psychology. General. 2015 Feb;144(1):114-126. DOI: 10.1037/xge0000033.',\n",
       "  '45Ethics: positive approach',\n",
       "  '46Ethics: positive approachIn recent decades, psychologists have discovered ﬁve moral dimensions that humans consider when judging situations: •Harm, which can be both physical or psychological •Fairness/liberty, which is about biases in processes and procedures •Loyalty, which ranges from supporting a group to betraying a country •Authority, which involves disrespecting elders or superiors, or breaking rules •Purity, which involves concepts as varied as the sanctity of religion or personal hygiene. These ﬁve dimensions deﬁne a space where we, humans, decide what is right and what is wrong.',\n",
       "  '47Ethics: positive approachJudgments depend on the intention of agents, not only on the moral dimension, or the outcome, of an action. In which situation would you blame Bob?',\n",
       "  '48Ethics: positive approachJudging machines/algorithms is not equivalent to judging humans. Humans are judged more positively than machines in autonomous driving scenarios. Humans were judged more harshly (plagiarism). Etc.\\nFindings suggest that people judge machines based on the observed outcome, but judge humans based on a combination of outcome and intention.',\n",
       "  'Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science  Decision-Making, Values and Legitimacy  2',\n",
       "  '2',\n",
       "  '3LegitimacyWhen considering a possible use case of AI, we can ask this question:  Does it make sense to use AI?\\nThe mother of all ethical AI questions',\n",
       "  '4LegitimacyAccording to institutional theory (politics), legitimacy refers to the congruence between organizational activities and their cultural environment.  The legitimacy of a political system depends on various factors: how well it achieves its goals, whether the subjects of the political system are involved in developing the rules, and whether the decision subject has the ability to challenge decisions. It is the central problem of politics.',\n",
       "  '5LegitimacyLegitimacy represents an important form of social evaluation, as it is indispensable for the acceptance and diffusion of novel technologies. The legitimacy question should precede other ethical questions such as discrimination or privacy.',\n",
       "  '6LegitimacyIn these scenarios there are “legitimacy issues”: •A student is proud of the creative essay she wrote for a standardized test. She receives a perfect score, but is disappointed to learn that the test had in fact been graded by a computer. •A defendant ﬁnds that a criminal risk prediction system categorized him as high risk for failure to appear in court, based on the behavior of others like him, despite he had every intention of appearing in court on the scheduled date. •An automated system locked out a social media user for violating the platform’s policy on acceptable behavior. The user insists that they did nothing wrong, but the platform won’t provide further details nor any appeal process. Which are the issues?',\n",
       "  '7LegitimacyHow do we evaluate the legitimacy of AI for taking high-stake decisions? We need to understand which are the critical issues of a high-stake decision and how do they interact with a set of values.',\n",
       "  'Values & Technology\\n8',\n",
       "  \"9Values/Principles and technologyOrganizations deﬁne their values regarding AI through a multifaceted process.  Here's a general outline of how this process might unfold: •Mission and Vision Alignment: Companies begin by ensuring that their AI values align with their broader mission and vision. This means considering how AI can help achieve their goals while adhering to the ethical standards they've set for themselves. •Stakeholder Engagement: They engage with various stakeholders, including employees, customers, partners, and potentially affected communities, to gather insights and perspectives on the ethical use of AI. This inclusive approach helps ensure that the company's AI values are considerate of diverse viewpoints and concerns. •Ethical Standards and Principles: Many companies adopt ethical frameworks or principles speciﬁc to AI. These often include commitments to transparency, fairness, accountability, privacy, and ensuring that AI technologies do not cause harm. These\",\n",
       "  \"employees, customers, partners, and potentially affected communities, to gather insights and perspectives on the ethical use of AI. This inclusive approach helps ensure that the company's AI values are considerate of diverse viewpoints and concerns. •Ethical Standards and Principles: Many companies adopt ethical frameworks or principles speciﬁc to AI. These often include commitments to transparency, fairness, accountability, privacy, and ensuring that AI technologies do not cause harm. These principles guide the development and deployment of AI systems. •Regulatory and Industry Standards Compliance: Companies also consider existing and anticipated regulations governing AI in their jurisdictions, as well as industry best practices and standards. This helps ensure that their AI values and practices are not only ethical but also legally compliant.\",\n",
       "  '10Example\\nIt is based on ﬁve core principles: justice, autonomy, beneﬁcence, non-maleﬁcence, and transparency.',\n",
       "  '11Example• Justice: • Impartiality • Equality • Proportionality • Autonomy: • Explainability • Privacy • Literacy • Non-Maleﬁcence: • Reliability • Controllability • Accountability • Transparency: • Comprehensibility • Interactivity • Traceability • Beneﬁcence: • Security • Sustainability • Responsibility',\n",
       "  'Decision-making automation\\n12Decision-making automation refers to the use of technology, particularly software and algorithms, to automate the process of making decisions that were traditionally made by humans.',\n",
       "  'Kinds of automatic decision-making systemsIn the context of decision-making, automation can be categorized into three distinct types.\\n13',\n",
       "  'Kinds of automatic decision-making systemsConverting human-designed decision-making rules into software. \\n14\\nThe doctors still decided who was medically eligible for dialysis. But then, they established a second committee, a group of seven laypeople\\xa0chosen by the local medical society, who would make the non-medical decision of how to allocate the few available slots among the many eligible patients. The committee members were given some basic education about kidney medicine, but weren’t told how to make their moral choices.1962Rules that have been set down by hand.',\n",
       "  'Kinds of automatic decision-making systemsConverting human-designed decision-making rules into software.\\n15\\nRight now, about 100,000 people in the U.S. are waiting for a kidney transplant.The new algorithm’s logic, and the factors that determine each patient’s fate within it, are transparent—not only publicly disclosed, but explained in simple terms. And the system’s performance is subject to annual audits, by an organization that publishes detailed reports.2022',\n",
       "  'Kinds of automatic decision-making systemsLeveraging machine learning to emulate the informal decision-making processes of humans.\\n16\\nDecision makers have primarily reliedon informal judgment rather than formally speciﬁed rules.',\n",
       "  'Kinds of automatic decision-making systemsLearning decision-making rules from labeled data (based on a loss function that is a proxy of the policy).\\n17\\nUncovering  patterns in a dataset that predict an outcome or property of policy interest (such as risks of cardiovascular disease, life expectancy, etc.)— and then bases decisions (such as transplant priority) on those predictions.',\n",
       "  'Kinds of automatic decision-making systems\\n18\\nLearning decision-making rules from labeled data (and using a proxy loss function).',\n",
       "  'Kinds of automatic decision-making systems\\n19Learning decision-making rules from labeled data (and using a proxy loss function). Example:  To apply a policy for selecting “the student who will beneﬁt the most from studying at your university” you can employ several proxy concepts. Some key proxy concepts include: •Academic Performance (best predicted scores). •Engagement and Participation (best predicted engagement). •Socio-Economic Impact: (best predicted socio-economic impact, either personally (e.g., ﬁrst-generation college students) or on their communities.',\n",
       "  'Legitimacy & Decision Making\\n20',\n",
       "  '21About the processLesson of History Up to now, in our society, critical decisions were often made by bureaucratic systems.  Bureaucracies arose, in part, to counteract the subjectivity, randomness, and inconsistency inherent in human decision-making, that can result in arbitrary decisions.  Its established rules and procedures are designed to reduce the impact of weaknesses found in individual decision makers.',\n",
       "  '22About the processArbitrariness has two sides.  The ﬁrst view of arbitrariness is primarily concerned with procedural regularity: whether a decision-making scheme is executed consistently.  When decision-making is arbitrary in this sense of the term, individuals may ﬁnd that they are subject to different decision-making schemes and receive different decisions simply because they go through the decision-making process at different times.  This principle is based on the belief that people are entitled to similar decisions unless there are reasons to treat them differently.',\n",
       "  '23About the processThe second view of arbitrariness refers to the basis for making decisions without reasoning, even if decisions are consistently made on that basis.  This principle is based two beliefs:   •the belief  that random decision-making (in general) shows a lack of respect for people &  •the belief that subjective decision-making can lead to unfairness, errors, and lack of quality.',\n",
       "  '24About the decisionsThe results of the decisions must be: •Accurate (the system must provide results that are “correct” in most cases or very close to the ideal result). Correctness of the results must be deﬁned in a way that is compatible with the values of those affected by the decisions. •Reliable (the system offers stable and consistent results in different scenarios, is invariant to some kinds of changes in the environment). •Effective (the system delivers results that affect/impact the real world in the expected way).',\n",
       "  '25Legitimacy & Decision MakingWe can consider the application of automated decision-making processes in a speciﬁc scenario to be legitimate if it meets several criteria at two different levels: 1. The results of the decisions are: •Accurate & Aligned •Reliable •Effective 2. The decision-making process is: •Well executed •Well justiﬁed',\n",
       "  '26Legitimacy & Decision Making',\n",
       "  '27Legitimacy & Decision Making\\nThe ﬁrst form of automation is a direct response to arbitrariness as inconsistency. It allows procedural regularity. However, several problems can arise: policies intended to be automated may lack clarity or speciﬁcity, leading programmers to make subjective decisions and thus overstep their bounds in policy deﬁnition. Also, the software may be prone to errors. Automation also requires an institution to pre-determine all decision-making criteria, leaving no ﬂexibility for unforeseen or unforeseen details. In addition, automation poses a signiﬁcant risk as it potentially reduces accountability and intensiﬁes the impersonal nature of bureaucratic interactions.',\n",
       "  '28Legitimacy & Decision Making\\nIn the second case, this form of automation could help solve problems of arbitrariness in human decision-making by formalizing and ﬁxing a decision-making scheme similar to what humans might have used in the past .  In this sense, machine learning can be desirable because it can help smooth out any inconsistencies or subjectivities in human decisions.',\n",
       "  '29Legitimacy & Decision MakingThese decision-making schemes can be considered equivalent to those employed by humans, and are therefore likely to perform similarly, even though the model may make its decisions differently and produce quite different error patterns.  Worse, the models could also learn to base themselves on criteria in ways that humans would ﬁnd troubling or objectionable, even if doing so still produces a set of decisions similar to what humans would make.',\n",
       "  \"30Legitimacy & Decision Making\\nThe third form of automation, which we'll call predictive optimization, speaks directly to concerns with reasoned decision making.  Predictive optimization attempts to provide a more rigorous basis for decision-making based only on criteria to the extent that they demonstrably predict the outcome or quality of interest. (Consistently execute a pre-existing policy through automation does not ensure that the policy itself is reasoned. Nor does relying on past human decisions to induce a decision-making rule guarantee that the basis of automation decision-making will reﬂect reasoned judgments).\",\n",
       "  \"31Legitimacy & Decision Making\\nBut it has ﬂaws:  •Good predictions may not lead to good decisions (causality),  •It's hard to measure what we really care about (proxy loss functions),  •Training data rarely matches the deployment conﬁguration (drift of distribution),  •Social outcomes are not predictable with precision (predictability), with or without machine learning.\",\n",
       "  '32ConclusionsIn consequential applications of AI, to establish legitimacy, the decision-makers must be able to afﬁrmatively justify their scheme according to the dimensions we have set out: the level of accuracy, reliability and effectiveness of their predictions, of their potential ethical problems, and that is also well executed and well justiﬁed.  To these properties we could add a condition of prudence, irreducibility: that there is no comparable solution based on human-designed algorithms and that therefore the decision system cannot be based on converting human-designed decision-making rules into software.',\n",
       "  'Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science  Fundamental and Practical Limits of ML3',\n",
       "  '2ML Aim:  If we model a phenomenon/system as a process by which some input state  is transformed into some output state , we can hope to learn an approximate transformation function  from observed past examples using machine learning/statistics. XYy=f(x)Limits to prediction',\n",
       "  '3If we model a phenomenon as a process by which some input state  is transformed into some output state , we can hope to learn a transformation function  from observed past examples using machine learning/statistics.  Method: We observe  (i.i.d data) and model  by maximizing the empirical risk of the model (accuracy, likelihood). The interpretation of  is: “given that I have observed , what can I say about ?” XYy=f(x)P(X,Y)𝔼(Y|X)𝔼(Y|X)XYLimits to prediction',\n",
       "  '4The term \"i.i.d data\" refers to \"Independent and Identically Distributed\" data. In the context of statistics and machine learning, i.i.d is an assumption about the random variables that make up the dataset being used for analysis or modeling. Here\\'s a breakdown of what this means: Independent: Each data point (or random variable) in the dataset is assumed to be independent of the others. This means that the occurrence of one data point does not inﬂuence or change the probability of occurrence of another data point. For example, in a coin toss, each toss is independent of the previous ones. Identically Distributed: This means that each data point in the dataset is drawn from the same probability distribution and has the same statistical properties (such as mean, variance, etc.). It does not mean that all data points are the same, but rather that they share the same underlying distribution.Limits to prediction',\n",
       "  '5Limits to prediction\\nA seven-day forecast can accurately predict the weather about 80 percent of the time and a ﬁve-day forecast can accurately predict the weather approximately 90 percent of the time.  However, a 10-day—or longer—forecast is only right about half the time.',\n",
       "  '6Weather data is typically not i.i.d because it often violates both the independence and identical distribution assumptions for several reasons: •Temporal Dependence: Weather observations are strongly dependent on preceding conditions. For example, the weather today is likely to be similar to the weather yesterday to some extent, especially in terms of temperature, precipitation, and atmospheric pressure. This sequential dependence means weather data points are not independent. •Seasonal Variations: Weather data exhibits seasonal patterns, which means that its distribution can change over different times of the year. For instance, temperatures are generally higher in summer and lower in winter in many places. This seasonal effect means that the distribution of weather data is not identical throughout the year. Limits to prediction',\n",
       "  '7Getting truly i.i.d weather data is challenging due to the inherent temporal and spatial dependencies in weather phenomena. However, you can approximate i.i.d conditions in weather data for speciﬁc types of analyses: •If your analysis can tolerate it, randomly sampling weather data from a wide range of locations and times might reduce dependencies. •Instead of using raw weather data, you can use anomalies or deviations from a long-term mean. For example, calculating the deviation of daily temperatures from the 30-year average for that day can help to remove some of the seasonal and longer-term trends, making the data more homogenous. •Etc.Limits to prediction',\n",
       "  '8Let’s suppose we get iid data. Is everything predictable given enough data and powerful algorithms?  • Are there fundamental limits? • Which are the practical limits?Limits to prediction',\n",
       "  'Fundamental Limits of ML\\n9',\n",
       "  '10Fundamental limits to prediction•The nondeterminism of phenomena of interest  •Inscrutability of of the world •Computational limits. •Limits to collecting sufﬁcient training examples (volume, independence, etc.) •Etc.impossible to know, to understandIf there were a vast intelligence — Laplace’s Demon — that knew the exact state of the universe at any one moment, and knew all the laws of physics, and had arbitrarily large computational capacity, it could both predict the future and reconstruct the past with perfect accuracy.',\n",
       "  '11Fundamental limits to predictionNote that: •Determinism of the universe at the most fundamental level is compatible with non-determinism at higher levels of description (chemistry, biology, psychology, sociology, etc.)! •The cause of this paradox is that higher levels of description are deﬁned by states that correspond to multiple fundamental level states.The laws of physics (Core Theory, QFT) are sufﬁcient to predict the future state of the universe (at least the part of the universe that matters for humans) at any one moment given a complete representation of the current state.Carroll, Sean M. \"The Quantum Field Theory on Which the Everyday World Supervenes.\" arXiv preprint arXiv:2101.07884 (2021).',\n",
       "  'Practical Limits of ML\\n12',\n",
       "  '13Practical limits: •Sensitive dependence on inputs (butterﬂy’s effect, ill-posed problems). This is possible even in linear models. •Effects of unexpected/unpredictable events (a lottery jackpot; an accident). This corresponds to variables that interact with very low probability (the real problem of autonomous driving). •Feedback loops (predicting  causes changes in ). •Drift: the statistical relationship between the input variables and the target may change over time. •Unobservable/latent input features (intelligence, people’s thoughts).  All these issues can cause failures..YXPractical limits to prediction',\n",
       "  '14Sometimes, what is incorrectly framed as a prediction problem can be better understood as a problem of explanation, intervention, or decision making. •Explanation is about generating scientiﬁc insight into how a process works rather than simply predicting its input-output behavior. We need a generative model of , their statistical relationships are not sufﬁcient (causality). •Intervention is about ﬁguring out how to change a process for the better rather than treating it as a given and conﬁning oneself to making predictions. We need a generative model of  (causality). •Decision making recognizes that many considerations go into making good decisions beyond maximizing predictive accuracy (fairness, diversity, etc.).P(X,Y)P(X,Y)When shouldn’t be used prediction?',\n",
       "  '15Explanation is about generating scientiﬁc insight into how a process works rather than simply predicting its input-output behavior.  Example: the multicollinearity problem. Take the ﬁctional toy example of predicting a child’s reading ability (y) as a function of its age (a) and height (h). Let’s assume age and height are perfectly correlated in our data, as in the example below. Now we can express y equivalently as: When shouldn’t be used prediction?\\nThis model is not identiﬁable (existence of one unique value for each parameter) DataModels',\n",
       "  '16Intervention is about ﬁguring out how to change a process for the better rather than treating it as a given and conﬁning oneself to making predictions. When shouldn’t be used prediction?\\nObserving  does not determine the effect of an intervention . In general .P(Wet,Rain,Sprinkler)P(Rain|do(Wet))P(Rain|do(Wet))≠P(Rain|Wet)P(Wet,Rain,Sprinkler)Generative ModelDataSprinklerRainWetTFTTTTTTFFFFTFTFTFFTTTFT………',\n",
       "  '17Decision making recognizes that many considerations go into making good decisions beyond maximizing predictive accuracy, especially because the decisions themselves have causal effects. When shouldn’t be used prediction?When training a model: •I want to minimize the Empirical Risk. •I want to maximize robustness against changes in data distribution. •I want to be able of explaining my predictions. •I want to measure and mitigate unwanted biases (discrimination). •Etc.',\n",
       "  'ML failures from a  data-centric point of view\\n18',\n",
       "  'ML failures\\n19ML fails when we are dealing with a predictive problem but, at inference time,  does not correspond to what happens in the real world.𝔼(Y|X)',\n",
       "  'ML failures\\n20• Data distribution shifts: the model learns from a distribution that does not represent the world at inference time.',\n",
       "  'ML failures\\n21• Data distribution shifts: the model learns from a distribution that does not represent the world at inference time.   Causes: •External changes in the  data generation process. •Degenerate feedback loops:  system’s outputs cause  changes in the inputs.',\n",
       "  'ML failures\\n22•Edge Cases: a ML learning can fail in a number of edge cases, making catastrophic mistakes.',\n",
       "  'Data Distribution Shifts\\n23• The distribution of the data the model is trained on, , is called source distribution.  • The distribution of the data the model runs inference on is called the target distribution. • can be decomposed in two ways: •  •P(X,Y)P(X,Y)P(X,Y)=P(X)P(Y|X)P(X,Y)=P(Y)P(X|Y)',\n",
       "  'Data Distribution Shifts\\n24Data distribution shifts are: • Covariate shift is when  changes, but  remains the same.  • Label Shift is when  changes, but  remains the same. • Concep drift is when  changes, but  remains the same.P(X)P(Y|X)P(Y)P(X|Y)P(Y|X)P(X)',\n",
       "  'Covariate Shift\\n25Statistics: a covariate is a variable that can inﬂuence the outcome of a given statistical trial. Supervised ML: input features are covariates. Covariate shift: Input distribution changes, but for a given input, output is the same.',\n",
       "  'Covariate Shift\\n26 changes, but for a given input,  is the same. P(X)P(Y|X)\\nNew incoming data can invalidate the current model.',\n",
       "  'Covariate Shift\\n27 changes, but for a given input,  is the same. P(X)P(Y|X)Training\\nTest',\n",
       "  'Covariate Shift\\n28 changes, but for a given input,  is the same. Example: • Predicts (cancer | patient_data) • (age > 40) > (age > 40) • (cancer | age > 40) = (cancer | age > 40) There are several causes. E.g. women >40 are encouraged by doctors to get check-ups.P(X)P(Y|X)PPtrainingPinferencePtrainingPinference',\n",
       "  'Covariate Shift\\n29 changes, but for a given input,  is the same. Example: • Predicts (cancer | patient_data) • (age > 40) > (age > 40) • (cancer | age > 40) = (cancer | age > 40) Training: If knowing in advance how the production data will differ from training data, use importance weighting. Production: unlikely to know how a distribution will change in advance.P(X)P(Y|X)PPtrainingPinferencePtrainingPinference',\n",
       "  'Importace weighting\\n30In supervised machine learning, it is important to train an estimator on balanced data so the model is equally informed on all classes. To balance the classes, we can inform the estimator to adjust how it calculates loss. Using weights, we can force as estimator to learn based on more or less importance (‘weight’) given to a particular class. Weights scale the loss function. As the model trains on each point, the error will be multiplied by the weight of the point. The estimator will try to minimize error on the more heavily weighted classes, because they will have a greater effect on error, sending a stronger signal. Without weights set, the model treats each point as equally important. Example: Logistic regression Loss=1NN∑i=1(−(yilog(̂yi))+(1−yi)log(1−̂yi))WeightedLoss=1NN∑i=1(−w0(yilog(̂yi))+w1(1−yi)log(1−̂yi))',\n",
       "  'Importace weighting\\n31\\nIn supervised machine learning, it is important to train an estimator on balanced data so the model is equally informed on all classes. To balance the classes, we can inform the estimator to adjust how it calculates loss. Using weights, we can force as estimator to learn based on more or less importance (‘weight’) given to a particular class. Weights scale the loss function. As the model trains on each point, the error will be multiplied by the weight of the point. The estimator will try to minimize error on the more heavily weighted classes, because they will have a greater effect on error, sending a stronger signal. Without weights set, the model treats each point as equally important. Example: Multiclass',\n",
       "  'Importace weighting\\n32',\n",
       "  'Label Shift\\n33 changes, but for a given output,  is the same. Output distribution changes but for a given output, input distribution stays the same.P(Y)P(X|Y)',\n",
       "  'Label Shift\\n34 changes, but for a given output,  is the same. Output distribution changes but for a given output, input distribution stays the same. Exemple: •Predicts  •The prevalence of diseases,  , are changing over time.P(Y)P(X|Y)P(Y=disease|X=symptoms)P(Y)P(Y|X)=P(X|Y)P(Y)P(X)',\n",
       "  'Concept Drift\\n35 remains the same, but  changes. Same input, expecting different output.P(X)P(Y|X)\\nConcept shift on soft drink names in the United States.',\n",
       "  'Concept Drift\\n36 remains the same, but  changes. Same input, expecting different output. Example (non stationary distribution): •Predicts P(€|features of a house in BCN) •P(features of a house in BCN) remains the same. •Covid causes people to leave BCN, housing prices drop. •P(€1M | features of a house in BCN):  •Pre-covid: high •During-covid: lowP(X)P(Y|X)',\n",
       "  'Other drifts: Bergson’s paradox\\n37US Universities pick students based on a number of attributes.  Two commonly considered attributes are high school GPA and SAT scores.  We want to measure the correlation GPA-SAT by using data from a random school. The prior hypothesis is that there is a positive correlation… The SAT is a standardized test widely used for college admissions in the United States.',\n",
       "  'Other: Bergson’s paradox\\n38The admissions committee accepts students who have either a sufﬁciently high GPA, a sufﬁciently high SAT score, or some combination of the two.  However, applicants who have both high GPAs and high SAT scores will likely get into a higher-tier school and not attend, even if they are accepted.',\n",
       "  'Other: Bergson’s paradox\\n39Data show a downward trend (negative correlation) even though the overall population (red and blue dots) show an upward trend (positive correlation). This trend reversal is the \"paradox,\" though there is nothing truly paradoxical about it.',\n",
       "  'Other: Bergson’s paradox\\n40 change!P(X),P(Y),P(Y|X),P(X|Y)',\n",
       "  'How to detect Data Distribution Shifts?\\n41Data distribution shifts are only a problem if they cause your model’s performance to degrade. You have to monitor your model’s accuracy related metrics!',\n",
       "  'How to detect Data Distribution Shifts?\\n42How to determine that two distributions are different? 1.Compare statistics: mean, median, variance, quantiles, skewness, kurtosis,…  How: Compute mean & variance of a feature during training and compare them to the same values computed in production. Not universal: only useful for distributions where these statistics are meaningful. Inconclusive:  if statistics differ, distributions differ. If statistics are the same, distributions can still differ. 2.Two-sample hypothesis test.  How: Determine whether the difference between two populations is statistically signiﬁcant (using the Kolmogorov-Smirnov test). Doesn’t make assumptions about distribution. Only works with one-dimensional data.',\n",
       "  'How to address Data Distribution Shifts?\\n431.Train model using a massive dataset (hopefully including diverse data distributions). 2.Retrain model with new data from new distribution (ﬁne-tuning).  Need to ﬁgure out not just when to retrain models, but also how and what data.',\n",
       "  'Jordi Vitrià\\nEthical Data Science    MSc in Fundamental Principles of Data Science   Bias and Discrimination I 4',\n",
       "  'Index 1. Bias and Discrimination. 2. The human factor. 3. Automated Discrimination. 4. Case Analysis: Recividism risk.\\n2',\n",
       "  'Bias and discrimination\\n3',\n",
       "  'What do we mean by “data bias”? \\n4The common deﬁnition of data bias is that the available data is not representative of the population or phenomenon of study.  But bias also denotes: •Data includes content which may contain bias against speciﬁc groups of people.Except for data acquired by a carefully designed randomized sampling process, most organically produced datasets are biased.\\nEthical Issue!',\n",
       "  'What do we mean by “algorithmic bias”? \\n5Algorithmic bias describes systematic deviation in output/performance or impact, relative to some norm or standard.  Example: many universities use data from past students to build models for predicting student success, where those models can support informed changes in policies and practices.World observationOutputImpact',\n",
       "  'What do we mean by “algorithmic bias”? \\n6',\n",
       "  'What do we mean by “algorithmic bias”? \\n7Algorithmic bias describes systematic deviation in output/performance or impact, relative to some norm or standard.  An algorithm can be statistically XOR ethically biased.  Example:  - Our algorithm will be statistically biased if predictions differ systematically from previously observed data. - Our algorithm will be ethically biased if predictions depend on the gender of the student.',\n",
       "  'What do we mean by “algorithmic bias”? \\n8Not all statistically biased behaviors are ethically problematic, while not all statistically unbiased behaviors are ethically acceptable. Example: An AI system for hiring, trained with historical data, can be statistically unbiased and ethically problematic.',\n",
       "  'Example\\n9https://pair.withgoogle.com/explorables/hidden-bias/\\nX\\nYThe Dataset (x,y)',\n",
       "  'Example\\n10https://pair.withgoogle.com/explorables/hidden-bias/\\nNaive Predictor  ̂y=x\\nŶY',\n",
       "  'Example\\n11https://pair.withgoogle.com/explorables/hidden-bias/\\nY\\nPredictor  ̂y=𝔼(y|x)=ax+b\\n̂Y',\n",
       "  'Example\\n12https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x1,x2,…,xK)\\n̂Y\\nY',\n",
       "  'Example\\n13https://pair.withgoogle.com/explorables/hidden-bias/\\n̂YPredictor  ̂y=𝔼(y|x1,x2,…,xK,…,xn)\\nY',\n",
       "  'Example\\n14https://pair.withgoogle.com/explorables/hidden-bias/\\nOverpredicting outcomes for men and underpredicting for women can have signiﬁcant ethical implications, but it is not the only unethical bias we can found. Higher outcome variance for women can also be an issue from the perspective of “quality of service”.̂Y\\nY',\n",
       "  'Example\\n15https://pair.withgoogle.com/explorables/hidden-bias/',\n",
       "  'Example\\n16https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x)',\n",
       "  'Example\\n17https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x)',\n",
       "  'Example\\n18https://pair.withgoogle.com/explorables/hidden-bias/\\nPredictor  ̂y=𝔼(y|x1,x2,…,xK)',\n",
       "  'Sources of Bias\\n19\\nData Generation\\nSampleWorld as it isSampleDataset\\nModelLearningEvaluationModel deﬁnition\\nDecisionsML model life cycle\\nIdeal and Possible World\\nMeasurement\\nWorld as it isFeedback',\n",
       "  '20Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDecisionsTraining & Test\\nThe features we use do not represent the phenomenon we are studying,Sources of Bias',\n",
       "  \"The data we use do not represent the population,Inductive bias describes the tendency for a system to prefer a certain set of generalizations over others that are equally consistent with the observed data.Aggregation bias occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group.Simpson’s ParadoxStructural bias refers to the social/institutional patterns and practices that confer advantage to some and disadvantage to others based on identity.When evaluating a model, metrics calculated against an entire test or validation set don't always give an accurate picture of how the model works. Deployment bias refers, generally, to any bias that arises during deployment, where a system is used or interpreted in inappropriate ways, perhaps not intended by the designers or developers.This type of bias occurs when a model itself inﬂuences the generation of data that is used to train it.\",\n",
       "  '21Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasSexism, racism, socio-economic status, etc.',\n",
       "  '22Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasSexism, racism, socio-economic status, etc.An optimal predictor can be unfair!',\n",
       "  '23Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIndirect effect: Unbalanced Dataset\\nAggregation  BiasSexism, racism, socio-economic status, etc.',\n",
       "  '24Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasDirect effect:  If there is systemic racism in a university that impacts student success,  there is label bias.Sexism, racism, socio-economic status, etc.',\n",
       "  '25Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasAggregation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias',\n",
       "  '26What about applications that aren’t about people?  Consider “Street Bump,” a project by the city of Boston to crowdsource data on potholes.  The smartphone app automatically detects pot holes using data from the smartphone’s sensors and sends the data to the city. Infrastructure seems like a comfortably boring application of data-driven decision-making, far removed from the ethical quandaries we’ve been discussing. But the data reﬂects terms of smartphone ownership, which are higher in wealthier parts of the city compared to lower-income areas and areas with large elderly populations.',\n",
       "  '27Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasStudent success can be speciﬁed in terms of many different variables that do not represent in a fair way all groups: grades, employer prestige, post-graduate salary, etc.\\nAggregation  Bias',\n",
       "  '28Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasStudent success can be speciﬁed in terms of many different variables that do not represent in a fair way all groups: grades, employer prestige, post-graduate salary, etc.\\nAggregation  BiasReality Gap: “real” data are not necessarily equivalent to “reality”The features we use determine what real patterns can be detected!',\n",
       "  '29Recommended Reading:  Measurement and Fairness, by Abigail Z. Jacobs, Hanna Wallach https:/ /arxiv.org/abs/1912.05511Measuring almost any attribute about people is similarly subjective and challenging: teacher effectiveness, economic status, etc.',\n",
       "  '30Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasThe choice of the objective function is not ethically neutral: we could minimize “prediction errors” or minimize “predictions of no-help-needed for students who truly need help”.\\nAggregation  Bias',\n",
       "  '31Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasSometimes, there is no “ground truth” to label data. In many applications the system can do little more than mimic the agreement amongst domain “experts”.',\n",
       "  '32Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIf users care about students who are most likely to have low grades but the algorithm is optimized to identify those likely to drop-out, the output will not provide the right information. \\nAggregation  Bias',\n",
       "  '33Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of BiasIf users care about students who are most likely to have low grades but the algorithm is optimized to identify those likely to drop-out, the output will not provide the right information. \\nAggregation  BiasDistribution Shifts',\n",
       "  '34Structural BiasRepresentation  BiasMeasurement  BiasInductive  BiasEvaluation  BiasDeployment  BiasFeedback  Bias\\nData Generation\\nPopulation SamplingWorldSampleDataset\\nMeasurement\\nModelLearningEvaluationModel deﬁnition\\nWorldDeploymentTraining & TestSources of Bias\\nAggregation  BiasPopularity bias is an undesirable phenomenon associated with recommendation algorithms where popular items tend to be over-recommended over long-tail ones.',\n",
       "  '35We have seen bad biases, biases that are problematic from an ethical point of view because they conﬁgure the distribution of goods, services, risks, and opportunities, or even access to information in ways that are problematic. But there are biases that are inevitable, that enable ML.Every Bias Is Not a Bad Bias',\n",
       "  'Every Bias Is Not a Bad Bias\\n36\\n1980:  Bias in ML does help us generalize better and make our model less sensitive to some single data point.Bias is a need to generalize!',\n",
       "  'Every Bias Is Not a Bad Bias\\n37Deﬁnition: a hypothesis space is the set of mathematical functions  (hypotheses) that are tested against the training data, based on the assumption that relevant (real) patters can be expressed by way of a mathematical function, called the target function. The learning algorithm cannot uncover patterns that are not described in one of the hypotheses. fW',\n",
       "  'Every Bias Is Not Necessarily a Bad Bias\\n38\\nA quadratic pattern cannot be seen by a linear model.',\n",
       "  '39The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions (in terms of the hypothesis space, ) that the learner uses to predict outputs of given inputs that it has not encountered.  fW\\nEvery Bias Is Not Necessarily a Bad Bias',\n",
       "  '40The inductive bias  is inevitable and, though neither good nor bad in itself, but it is not neutral in real world settings: pattern blindness can result in winners and losers!Every Bias Is Not Necessarily a Bad Bias\\nAgeOverpredictionOverpredictionUnderpredictionModel',\n",
       "  '41We’ve seen that training data reﬂects the disparities, distortions, and biases from the real world and the measurement process.  Some patterns in the training data (“smoking is associated with cancer”) represent knowledge that we wish to mine using machine learning, while other patterns (“girls like pink and boys like blue”) represent stereotypes or bad habits that we might wish to avoid learning. Algorithm ethical assessment',\n",
       "  '42But learning algorithms have no general way to distinguish between these two types of patterns, because they are the result of social norms and moral judgments.  This leads to an obvious question: when we learn a model from such data, are these disparities preserved, mitigated, or exacerbated?Algorithm ethical assessment',\n",
       "  'Impact of bad biases\\n43',\n",
       "  'We ﬁnd that lenders charge Latin/African-American borrowers 7.9 and 3.6 basis points more for purchase and reﬁnance mortgages respectively, costing them $765M in aggregate per year in extra interest.   FinTech algorithms also discriminate, but 40% less than face-to-face lenders.  The lower levels of price discrimination by algorithms suggests that removing face-to-face interactions can reduce discrimination.  44\\nDiscrimination: Unjustiﬁed basis of differentiation between individuals\\nBad News!Bias ampliﬁcationImpact of bad biases',\n",
       "  '45Impact of bad biases',\n",
       "  '46X-ray image datasets used to diagnose various thoracic diseasesImpact of bad biases',\n",
       "  'Measuing bias is difficult\\n47\\nProblem: police discrimination analysisData:and sometimes impossible',\n",
       "  'Measuing bias is difficult\\n48\\nProblem: police discrimination analysisData:If the rate of using force against stopped Black people and the rate of using force against stopped white people are the same, can we conclude that we are observing a fair behavior?',\n",
       "  '49Measuing bias is difficult',\n",
       "  '50Measuing bias is difficult',\n",
       "  '51Measuing bias is difficult',\n",
       "  '52Measuing bias is difficult',\n",
       "  '53Measuing bias is difficult',\n",
       "  'Discrimination\\n54',\n",
       "  '55Under the most advanced law systems, everyone is protected from unlawful behavior (discrimination) when the cause of this behavior is that they have or are perceived to have a “protected  characteristic” or are associated with someone who has a protected characteristic: •Age •Disability •Gender •Civil state •Pregnancy and maternity •Race •Religion and belief •Sex •Sexual orientationLaw & Discrimination',\n",
       "  '56https://fairmlbook.org/Be careful! In many classiﬁcation tasks, available data contain protected characteristics of an individual.  Some have hoped that removing or ignoring protected attributes would somehow ensure the impartiality of the resulting classiﬁer. Unfortunately, this practice is usually somewhere on the spectrum between ineffective and harmful. In a typical data set, we have many features that are slightly correlated with the sensitive attribute.  However, if numerous such features are available, as is the case in a typical browsing history, the task of predicting gender becomes feasible at high accuracy levels.',\n",
       "  'But, isn’t discrimination the very point of machine learning? Yes, but it is not admissible when this discrimination/differentiation is based on unjustiﬁed causes, is practically irrelevant or is morally wrong.  Discrimination is not a general concept, it’s domain and feature speciﬁc! \\n57ENGINEER POINT OF VIEW: THAT’S NOT MY BUSINESS!\\nWE NEED A CASE BY CASE ANALYSIS FAIRNESS CANNOT BE AUTOMATED',\n",
       "  '58There are several types of discrimination: 1.Direct discrimination . This means treating someone less favorably than someone else because of a protected characteristic.  2.Direct discrimination by perception. This means treating one person less favorably than someone else, because you incorrectly think they have a protected characteristic. 3.Discrimination arising from disability.  This means treating a disabled person unfavorably because of something connected with their disability when this cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favorably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behavior related to a protected characteristic which has the purpose or effect of violating  someone’s dignity or which creates a hostile,',\n",
       "  'cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favorably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behavior related to a protected characteristic which has the purpose or effect of violating  someone’s dignity or which creates a hostile, degrading, humiliating or offensive environment.https://www.equalityhumanrights.com/sites/default/ﬁles/ea_legal_deﬁnitions_0.pdfLaw & Discrimination',\n",
       "  '59Disparate treatment or direct discrimination:  Treatment depends on class membershipDisparate impact or indirect discrimination: Outcome depends on class membershipLaw & Discrimination',\n",
       "  '60https://www.equalityhumanrights.com/sites/default/ﬁles/ea_legal_deﬁnitions_0.pdfAn employer does not interview a job applicant because of the  applicant’s ethnic backgroundA hair salon owner has a policy of not employing stylists who  cover their hair, believing it is important for them to exhibit their  ﬂamboyant haircuts.An employer dismisses a worker because she has had three  months’ sick leave. The employer is aware that the worker  has multiple sclerosis and most of her sick leave is disability- related.An employer offers ﬂexible working to all staff. Requests are  supposed to be considered based on business need. A manager  allows a man’s request to work ﬂexibly to train for a qualiﬁcation  but does not allow another man’s request to work ﬂexibly to care  for his disabled child.An employer has a policy that designated car parking spaces are  only offered to senior managers. A worker who is not a manager,  but has a mobility impairment is not given a designated car parking',\n",
       "  'disability- related.An employer offers ﬂexible working to all staff. Requests are  supposed to be considered based on business need. A manager  allows a man’s request to work ﬂexibly to train for a qualiﬁcation  but does not allow another man’s request to work ﬂexibly to care  for his disabled child.An employer has a policy that designated car parking spaces are  only offered to senior managers. A worker who is not a manager,  but has a mobility impairment is not given a designated car parking space. A builder addresses abusive and hostile remarks to a customer  because of her race after their business relationship has ended.1  1.Direct discrimination . This means treating someone less favourably than someone else because of a protected characteristic.  2.Direct discrimination by perception. This means treating one person less favourably than someone else, because you incorrectly think they have a protected characteristic. 3.Discrimination arising from disability.  This means treating',\n",
       "  'A builder addresses abusive and hostile remarks to a customer  because of her race after their business relationship has ended.1  1.Direct discrimination . This means treating someone less favourably than someone else because of a protected characteristic.  2.Direct discrimination by perception. This means treating one person less favourably than someone else, because you incorrectly think they have a protected characteristic. 3.Discrimination arising from disability.  This means treating a disabled person unfavourably because of something connected with their disability when this cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favourably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behaviour related to a protected characteristic which',\n",
       "  'person unfavourably because of something connected with their disability when this cannot be objectively justiﬁed. 4.Direct discrimination by association. This means treating someone less favourably than another person because they are associated with a person who has a protected characteristic. 5.Failing to make reasonable adjustments. To do this for disabled people is also a form of discrimination. 6.Harassment. Harassment is unwanted behaviour related to a protected characteristic which has the purpose or effect of violating  someone’s dignity or which creates a hostile, degrading, humiliating or offensive environment.2  3  4  5  6  Law & Discrimination',\n",
       "  '61Algorithmic discrimination scenarios: •Access to employment •Access to education •Access to government/companies beneﬁts •Access to penitentiary alternatives •Etc. Anti-discrimination legislation typically seeks equal access/treatment (mitigation of direct discrimination) to employment, working conditions, education, social protection, goods, and services, but in some cases, equal outcome is also sought (mitigation of indirect discrimination).  Law & Discrimination',\n",
       "  'In general, anti-discrimination laws aim to achieve equality of opportunity. Narrow notions of equality of opportunity are concerned with ensuring that decision-making treats similar people similarly on the basis of relevant features, given their current degree of similarity. Broader notions of equality of opportunity are concerned with organizing society in such a way that people of equal talents and ambition can achieve equal outcomes over the course of their live. Somewhere in between is a notion of equality of opportunity that forces decision-making to treat seemingly dissimilar people similarly, on the belief that their current dissimilarity is the result of past injustice.62Law & Discrimination',\n",
       "  'Example\\n63\\nhttps://www.cs.cmu.edu/~mtschant/ife/',\n",
       "  'Example\\n64',\n",
       "  'The human factor',\n",
       "  'Our brains are evolved to help us survive. That means they take a lot of shortcuts to help us get through the day. These shortcuts, or heuristics, are vital. But they come at a cost. Our world is much more complex than the world our brains developed these heuristics. Unconscious brains can be unreliable in this environment. Our unconscious can helps us in some situations, but it is not always the right tool. We must be sure that it will not hurt others.Human Biases\\n66The halo effect People who looks healthy or attractive are also competent and good. Reading:  Physiognomy’s New Clothes, by Blaise Agüera y Arcas, Margaret Mitchell and Alexander Todorov.',\n",
       "  'Unconscious Human Biases\\n67\\nhttps://www.visualcapitalist.com/wp-content/uploads/2017/09/cognitive-bias-infographic.html',\n",
       "  'We know that human decision-making is affected by: •Unconscious thoughts, biases, etc. •Unthinking custom and practice, or unconsciously absorving beliefs of our friends, family, society, etc. •Personal ethical decision making proﬁle. F.e. you prioritize relationships in your decicion-making. •Reﬂective practice, to consider context and the people who will be affected by your decisions. Human decision making\\n68The role of ethics is to have a toolkit to do reﬂective practice, and to able of making and justiﬁying our decisionsMINDPERSONAL HISTORYDEFAULT SETTING',\n",
       "  'Automated Discrimination',\n",
       "  'Algorithmic Fairness\\n70Algorithm fairness is the ﬁeld of research aimed at understanding and correcting unwanted biases. Speciﬁcally, it includes: •Researching the causes of bias in data and algorithms •Deﬁning and applying measurements of fairness •Developing data collection and modelling methodologies aimed at creating fair algorithms.',\n",
       "  '71We can distinguish between two approaches to formalizing fairness: – Individual fairness deﬁnitions are based on the premise that similar entities should be treated similarly. – Group fairness deﬁnitions are based on the deﬁnition of group entities and ask that all groups are treated similarly. To operationalize both approaches to fairness, we need to deﬁne similarity for the input and the output of an algorithm.   For group fairness, the challenge lies in determining how to partition entities into groups (protected attributes)How to measure fairness',\n",
       "  '72Individual fairness.  discriminates against  in relation to  if: • has property  and  does not have . • treats worse  than she treats  and this is because  has  and  does not have . XYZYPZPXYZYPZPHow to measure fairness',\n",
       "  '73Group fairness.  group-discriminates against someone, , in relation to another, , by -ing (e.g., hiring  rather than ), if: •There is a property, , such that  has  or  believes that  has , and  does not have  or  believes that  does not have , •  treats  worse than he treats or would treat  by  -ing. •It is because ( believes that)  has  and ( believes that)  does not have  that  treats  worse than  by -ing” and, •  is the property of being a member of a certain socially salient group (to which  does not belong). XYZΦZYPYPXYPZPXZPXYZΦXYPXZPXYZΦPZHow to measure fairness\\nLippert-Rasmussen K (2014) Born free and equal?: a philosophical inquiry into the nature of discrimination. In: Oxford university press, Oxford, New York',\n",
       "  '74One way of formulating individual fairness is a distance-based one.  Given a distance measure d between two entities and a distance measure D between the outputs of an algorithm, we would like the distance between the output of the algorithm for two entities to be small, when the entities are similar. Fairness Definitions',\n",
       "  '75Another form of individual fairness is counterfactual fairness.  An output is fair toward an entity if it is the same in both the actual world and a counterfactual world where the entity belonged to a different group.  Given that Alice did not get promoted in her job, and given that she is a woman, and given everything else we can observe about her circumstances and performance, what is the probability of her getting a promotion if she was a man instead? Causal inference is used to formalize this notion of fairness.Fairness Definitions',\n",
       "  '76For simplicity, let us assume two groups, namely the protected group  (f.e. women) and the non-protected (or, privileged) group  (f.e. men) and a binary classiﬁer.  We will start by presenting statistical approaches commonly used in classiﬁcation. Assume that  is the actual and  the predicted output of the binary classiﬁer, that is,  is the “ground truth”, and  the output of the algorithm.  Let 1 be the positive class that leads to a favorable decision, e.g., someone getting a loan, or being admitted at a competitive school, and  be the predicted probability for a certain classiﬁcation.G+G−ŶYŶYSFairness Definitions\\nThere are equivalent frameworks for regression and ranking.',\n",
       "  '77Statistical approaches to group fairness can be distinguished as: •Base rates approaches: that use only the output  of the algorithm, •Accuracy-based approaches: that use both the output  of the algorithm and the ground truth , and •Calibration approaches: that use the predicted probability  and the ground truth . ̂ŶYYSYFairness Definitions',\n",
       "  '78Base rate fairness compares (ratio or difference)  •the probability  that an entity  receives the favorable outcome when  belongs to the protected group  •with the corresponding probability  that  receives the favorable outcome when  belongs to the non-protected group.  When the probabilities of a favorable outcome are equal for the two groups, we have a special type of fairness termed demographic, or statistical parity:  P(̂Y=1|X∈G+)XXP(̂Y=1|X∈G−)XXP(̂Y=1|X∈G+)∼P(̂Y=1|X∈G−)Base rate fairness',\n",
       "  '79In a more general setting we can deﬁne demographic parity in terms of statistical independence: the protected characteristic must be statistically independent of the outcome.  independent of the protected characteristic for all groups  and all values : ̂Ya,bdBase rate fairness\\np(̂Y=d|X∈a)=p(̂Y=d|X∈b)',\n",
       "  '80Base rate fairness ignores the actual output. For example, assume that the classiﬁcation task is getting or not a job and the protected group  is based on gender.  Statistical parity asks for a speciﬁc ratio of women in the positive class, even when there are not that many women in the input who are well qualiﬁed for the job. G+Base rate fairness',\n",
       "  '81Base rate fairness\\nWhat is the 80% Rule?  The rule states that companies should be hiring protected groups at a rate that is at least 80% of that of white men. The 80% rule was created to help companies determine if they have been unwittingly discriminatory in their hiring process.',\n",
       "  'Let’s assume we’re building an application to select promising candidates for a job. Our model will aim to learn the typical proﬁle of those who can be hired. In this example we get demographic parity: We must take into account that: •Demographic parity can reject the optimal classiﬁer. Base rate fairness\\n82Dataset, Positive OutcomêY=1Acceptance rate: 6/24Class A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | NegativeAcceptance rate: 4/16Reminder: P(̂Y=1|X∈G)=P(̂Y=1,X∈G)P(X∈G)',\n",
       "  'Decisions based on a classiﬁer that satisﬁes independence can have undesirable properties (and similar arguments apply to other statistical criteria). Imagine a company that in group  hire diligently selected applicants at some rate .  In group , the company hires carelessly selected applicants at the same rate .  Eventhough the acceptance rates in both groups are identical, it is far more likely that unqualiﬁed applicants are selected in one group than in the other.  As a result, it will appear in hindsight that members of group B performed worse than members of group A, thus establishing a negative track record for group B.Ap>0BpWarning!\\n83https://fairmlbook.org/',\n",
       "  '84Accuracy-based fairness warrants that various types of classiﬁcation errors (e.g., true positives, false positives) are equal across groups. Depending on the type of classiﬁcation errors considered, the achieved type of fairness takes different names. Accuracy-based fairness',\n",
       "  '85Accuracy-based fairness',\n",
       "  '86Accuracy-based fairness',\n",
       "  '87The case in which we ask that   (same True Positive Rate) is called equal opportunity. Comparing equal opportunity with statistical parity, again the members of the two groups have the same chance of getting the favorable outcome, but only when these members qualify. P(̂Y=1|Y=1,X∈G+)=P(̂Y=1|Y=1,X∈G−)Accuracy-based fairness\\n Reminder: P(̂Y=1|Y=1,X∈G)=P(̂Y=1,Y=1,X∈G)P(Y=1,X∈G)',\n",
       "  '88In the general case, this method can be called separation:  must be independent of  the protected characteristic, conditional on . Separation acknowledges that in many scenarios, the sensitive characteristic may be correlated with the target variable.   A bank might argue that it is a matter of business necessity  to therefore have different lending rates for these groups. For example, one group might have a higher  default rate on loans than another. Roughly speaking, the separation criterion allows correlation between the score and the sensitive attribute to the extent that it is justiﬁed by the target variable. ̂YYAccuracy-based fairness',\n",
       "  '89Accuracy-based fairnessThe case in which we ask that is called equalized odds. All groups experience the same true positive rate and the same false positive rate.p(̂Y=1|Y=1,X∈G+)=p(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=0,X∈G+)=p(̂Y=1|Y=0,X∈G−)',\n",
       "  '90Equalized odds requires both the fraction of non-defaulters  that qualify for loans and the fraction of defaulters that qualify for loans to be constant across groups. Accuracy-based fairness',\n",
       "  '91Class A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | NegativeDatasetFPFPFPFPFNFNFN\\nFNFNFNFNFNFNFNTPRA=58FPRA=116Accuracy-based fairnesŝY=1̂Y=0TPRB=18FPRB=38',\n",
       "  'In many applications (e.g. hiring), people care more about the true positive rate than false positive rate so many works focus on equal of opportunity:\\n92p(̂Y=1|Y=1,X∈G+)=p(̂Y=1|Y=1,X∈G−)\\nFPFPFPFPFNFNFN\\nFNFNFNFNFNFNFNTPRA=58TPRB=18Accuracy-based fairness\\n̂Y=1̂Y=0',\n",
       "  'Separation may not help closing the gap between two groups in the real world.  For example, imagine group A has 100 applicants and 58 of them are qualiﬁed while group B also have 100 applicants but only 2 of them are qualiﬁed.  If the company decides to accept 30 applicants and satisﬁes equality of opportunities, 29 offers will be conferred to group A while only 1 offer will be conferred to group B.  If the job is a well-paid job, group A tends to have a better living condition and affords better education for their kids, and thus enable them to be qualiﬁed for such well-paid jobs when they grow up. The gap between group A and group B will tend to be enlarged over time.93Accuracy-based fairness',\n",
       "  'The criteria we reviewed constrain the joint distribution  in non-trivial ways. We should therefore suspect that imposing any two of them simultaneously over-constrains the space to the point where only degenerate solutions remain.  It can be shown that if we assume that  is binary, the protected feature is not independent of , and  is not independent of , then, independence and separation cannot both hold. It is impossible to satisfy all deﬁnitions of group fairness, meaning that the data scientists need to choose one to refer to when starting a fairness analysis.P(X,Y,̂Y)YŶYYRelationships between criteria\\n94',\n",
       "  'Relationships between criteria\\n95Incompatibility of fairness metrics doesn’t imply that fairness efforts are fruitless.  Instead, it suggests that fairness must be deﬁned contextually for a given ML problem, with the goal of preventing harms speciﬁc to its use cases.',\n",
       "  'For binary decision procedures, we can summarize a procedure with the confusion matrix, which illustrates match and mismatch between decision  and true status .̂YYFairness for decisions\\n96\\nConfusion MatrixDemographic Parity:p(̂Y=1|X∈G+)=p(̂Y=1|X∈G−)',\n",
       "  'For binary decision procedures, we can summarize a procedure with the confusion matrix, which illustrates match and mismatch between decision  and true status .̂YYFairness for decisions\\n97\\nConfusion MatrixFor any box in the confusion matrix involving the decision , we can deﬁne fairness as equality across groups. For example, Equal False Omission Rates: dp(Y=1|̂Y=0,X∈G+)=p(Y=1|̂Y=0,X∈G−)',\n",
       "  'Some machine learning systems produce scores instead of labels, f.e. probabilistic classiﬁers, recommenders, etc. Some of the measures we have seen can be generalized to scores.Fairness for scores\\n98',\n",
       "  'For score outputs, we can consider the following initial deﬁnitions of fairness based on equal metrics across groups: •Balance for the Positive Class: the average score assigned to positive members, , should be the same across groups.  •Balance for the Negative Class: the average score assigned to negative members, , should be the same across groups.  •Calibration: the fraction of those marked with a given score who are actually positive, , should be the same across groups.  •AUC (Area Under Curve) Parity: the area under the receiver operating characteristic (ROC) curve should be the same across groups. The AUC can be interpreted as the probability that a randomly chosen positive individual  is scored higher than a randomly chosen negative individual. 𝔼(S|Y=1)𝔼(S|Y=0)𝔼(Y=1|S=d)Y=1Fairness for scores\\n99',\n",
       "  '100Calibration-based fairness considers probabilistic classiﬁers that predict a probability  for each class. In general, a classiﬁcation algorithm is considered to be well-calibrated if:  when the algorithm predicts a set of individuals as having probability  of belonging to the positive class, then approximately a  fraction of this set is actual members of the positive class.  In terms of fairness, intuitively, we would like the classiﬁer to be equally well calibrated for both groups.pppFairness for scores',\n",
       "  '101CalibrationTo get an intuitive understanding of how well a speciﬁc model performs in this regard, Realiability Diagramms are often used.\\nAccuracyConﬁdence',\n",
       "  '102CalibrationThe Expected Calibration Error (ECE) simply takes a weighted average over the absolute accuracy/conﬁdence difference.',\n",
       "  '103Calibration-based fairness is asking that for any predicted probability score , the probability of positives among those with a given score is equal for both groups, i.e., p∈[0,1]Calibration-based fairnessP(Y=1|S=p,X∈G+)=P(Y=1|S=p,X∈G−)',\n",
       "  '104P(Y=1|S=p,X∈G+)=P(Y=1|S=p,X∈G−)\\nCalibration by gender on UCI adult data. A straight diagonal line would correspond to perfect calibration.\\nCalibration by race on UCI adult data.The fraction of those marked with a given score who are actually positive should be the same across groups.Calibration-based fairness',\n",
       "  '105Online Example\\nhttps://research.google.com/bigpicture/attacking-discrimination-in-ml/Attacking discrimination with  smarter machine learning or why fairness is part of a multi-objective task.',\n",
       "  '106Group-based measures in general tend to ignore the merits of each individual in the group.  Some individuals in a group may be better for a given task than other individuals in the group, which is not captured by some group-based fairness deﬁnitions. Limitations',\n",
       "  '107This issue may lead to two problematic behaviors, namely,  (a) the self-fulﬁlling prophecy where by deliberately choosing the less qualiﬁed members of the protected group we aim at building a bad track record for the group, and  (b) reverse tokenism where by not choosing a well qualiﬁed member of the non-protected group we aim at creating convincing refutations for the members of the protected group that are also not selected.Limitations',\n",
       "  '‘Bias preserving’ fairness metrics seek to reproduce historic performance in the outputs of the target model with equivalent error rates for each group as reﬂected in the training data (or status quo).  F.e. Equal FPR In contrast, ‘bias transforming’ metrics do not blindly accept social bias as a given or neutral starting point that should be preserved, but instead require people to make an explicit decision as to which biases the system should exhibit.   F.e. Demographic parity  Bias preservation or transformation?\\n108p(̂Y=1|Y=0,X∈G+)=p(̂Y=1|Y=0,X∈G−)\\np(̂Y=1|X∈G+)=p(̂Y=1|X∈G−)',\n",
       "  'Bias preserving criteria are always satisﬁed by a perfect classiﬁer that exactly predicts its target labels with zero error, replicating bias present in the data. Bias transforming metrics are not necessarily satisﬁed by a  perfect classiﬁer. \\n109Bias preservation or transformation?',\n",
       "  '110\\nBias preservation or transformation?',\n",
       "  '111DEMOGRAPHIC DISPARITY (DD) Is the disadvantaged class a bigger proportion of the rejected outcomes than the proportion of accepted outcomes for the same class? For example, in the case of college admissions, if women applicants comprised 40% of the rejected applicants and comprised only 30% of the accepted applicants, we say that there is demographic disparity because the rate at which women were rejected exceeds the rate at which they were accepted. DD=P(X∈G+|̂Y=0)−P(X∈G+|̂Y=1)Bias transforming\\nThis applies to cases were we can accept different a priori preferences between groups.',\n",
       "  '112DD=P(X∈a|̂Y=0)−P(X∈a|̂Y=1)Rejected = 8 /20 = 40%  Accepted = 3/10 = 30%DD=0.4−0.3=0.1Bias transformingWomanMan',\n",
       "  '113CONDITIONAL DEMOGRAPHIC DISPARITY (CDD) We can condition DD on attributes that deﬁne a strata of subgroups on the dataset.  This is necessary to rule out Simpson’s paradox (a problem that appears when aggregating data). Example: Graduate school admissions to University of California, Berkeley.\\n44% of the male applicants were accepted compared to only 35% of female applicants… Is there discrimination?Bias transforming',\n",
       "  '114Graduate school admissions to University of California, Berkeley.\\nAccepted women: 35% of 4231 = 1481 Accepted applicants: 44% of 8442 + 35% of 4231 = 5195 Non accepted women: 65% of 4231 = 2750 Non accepted applicants: 56% of 8442 + 65% of 4231 = 7478 DD = 2750/7478 - 1481/5195 = 0.08 There is evidence of (small) demographic disparity.Bias transforming',\n",
       "  '115However, when examining the individual departments, it appeared that 6 out of 85 departments were signiﬁcantly biased against men, while 4 were signiﬁcantly biased against women. The issue was that women were much more likely to apply to more competitive departments (such as English) that were much more likely to reject graduates of any gender, whereas other departments (such as Engineering) were more lenient. In the language of demographic parity: although Berkeley’s pattern of admission exhibited evidence of demographic disparity, once we condition according to “department applied for”, the apparent bias disappears.Bias transforming',\n",
       "  '116Let’s consider these dataset: \\nDD=0.46−0.32=0.14Bias against women.Bias transforming',\n",
       "  '117Let’s consider these tables: \\nBias in favour of menBias transforming\\nBias in favour of women\\nSimpson’s Paradox  is a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations.',\n",
       "  '118The Conditional Demographic Disparity metric gives a single measure for all the disparities found in the subgroups deﬁned by an attribute (f.e. department) by averaging (each subgroup weighted in proportion to the number of observations it contains) them. \\nSmall bias in favour of women!CDD=1n∑iniDDini:Number of observations for each subgroupn:Total number of observationsBias transforming',\n",
       "  '119Individual Fairness: Distance, Counterfactual. Demographic parity: • Equal opportunity,  • Equalized odds: • • Calibration: • (Conditional) Demographic disparity: •P(̂Y=1|X∈G+)∼P(̂Y=1|X∈G−)P(̂Y=1|Y=1,X∈G+)∼P(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=1,X∈G+)∼p(̂Y=1|Y=1,X∈G−)p(̂Y=1|Y=0,X∈G+)∼p(̂Y=1|Y=0,X∈G−)P(̂Y=1|S=p,X∈G+)∼P(̂Y=1|S=p,X∈G−)P(X∈G+|̂Y=0)∼P(X∈G+|̂Y=1)Fairness Metrics Summary : Protected groupG+ : Non-protected groupG−',\n",
       "  'The criminal justice system needs to evaluate a diverse set of risks:  •The risk of committing a new crime after an arrest (recidivism), •The risk of committing a new violent crime (violent recidivism), •The risk of committing an act of violence against another inmate or penitentiary personnel in jail (intra-penitentiary violence), •The risk of committing an administrative violation such as breaking the conditions of a permit. •Etc.Case Analysis: Recidivism risk\\n120',\n",
       "  'Structured risk assessment corresponds to a family of methodologies for evaluating these risks using a systematic process, typically in which a number of different items are evaluated.  We can train a ML system to make automatic decisions based on the scores in each item, but most often, a professional makes a decision based on his/her own evaluation of a defendant and the result of a series of items.\\n121Measuring recidivism risk',\n",
       "  'COMPAS (Correctional Offender Management Proﬁling for Alternative Sanctions) is an automated tool that outputs numerical scores, which are labeled, for example, “risk of recidivism”, “risk of violent recidivism”, or “risk of failure to appear”.  These scores are then used in an unspeciﬁed way to make decisions of jail, bail, home arrest, release, etc. Measuring recidivism risk\\n122Bail: the temporary release of an accused person awaiting trial.',\n",
       "  'In May 2016, ProPublica, an investigative journal, published a piece called \"Machine Bias\", in which COMPAS, was found to be biased against blacks. \\n123\\nMeasuring recidivism risk',\n",
       "  '124\\nMeasuring recidivism risk',\n",
       "  '125Measuring recidivism risk: the debateTwo of their ﬁndings of ProPublica can be phrased in our language as follows: •COMPAS does not satisfy equal false negative rates, in fact, white defendants who did get rearrested () were nearly twice as likely to be misclassiﬁed as low risk ().  •COMPAS does not satisfy equal false positive rates, in fact, black defendants who did not get rearrested () were nearly twice as likely to be misclassiﬁed as higher risk ().  Y=1̂Y=0Y=0̂Y=1',\n",
       "  '126Accuracy-based fairness',\n",
       "  '127Measuring recidivism riskBut the developers did not agree…',\n",
       "  '128Measuring recidivism risk',\n",
       "  '129Measuring recidivism risk: the debateIn their response, Equivant/Northpointe, the developers of COMPAS, cited two articles ﬁnding that: •COMPAS satisﬁes calibration: scores mean the same thing regardless of the defendant’s race. For example, among defendants with a score of 7, 60 percent of white defendants were rearrested and 61 percent of black defendants were rearrested.  •It can be shown that calibration implies equal (positive and negative) predictive values (but not the other way around):  •among those labeled higher risk (), the proportion of defendants who got rearrested () is approximately the same regardless of race. •among those labeled lower risk (), the proportion of defendants who did not get rearrested () is approximately the same regardless of race.̂Y=1Y=1̂Y=0Y=0',\n",
       "  '130Accuracy-based fairness',\n",
       "  '131Accuracy-based fairness',\n",
       "  '132Measuring recidivism risk: analysis\\nhttps://allendowney.github.io/RecidivismCaseStudy/',\n",
       "  '133Bias MitigationThe ﬁeld of bias mitigation strategies can be categorised into three types:  •Pre-processing methods manipulate the data to eliminate bias before a machine learning (ML) model is able to incorporate these biases based on the data.  •In-processing bias mitigation strategies manipulate the model to mitigate bias that appears during the training process.  •Post-processing methods alter the outcomes of a model, preying on bias present in the output.',\n",
       "  '134Bias MitigationPre-processing techniques •Reweighing Pre-Processing: Generates weights for the training samples in each (group, label) combination  differently  to  ensure  fairness  before  classiﬁcation. It  does  not  change  any  feature  or  label  values,  so  this  is  ideal  if you are unable to make value changes. •Optimized Pre-Processing:  Learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data ﬁdelity constraints and objectives. •Learning Fair Representations:  Finds  a  latent  representation  that  encodes  the  data  well  but obfuscates information about protected attributes.',\n",
       "  '135Bias MitigationIn-processing techniques •Adversarial Debiasing: Learns a classiﬁer to maximize prediction accuracy and simultaneously  reduces  an  adversary’s  ability  to  determine  the  protected  attribute  from  the  predictions.  This  approach  leads  to  a fair  classiﬁer  because  the  predictions  can’t  carry  any  group  discrimination information that the adversary can exploit. •Prejudice Remover: Adds a discrimination-aware regularization term to the learning objective. •Meta Fair Classiﬁer: Takes the fairness metric as part of the input and returns a classiﬁer optimized for the metric.',\n",
       "  '136Bias MitigationPost-processing techniques •Equalized Odds: Solves  a  linear  program  to  ﬁnd  probabilities  with  which  to change output labels to optimize equalized odds. •Calibrated Equalized Odds: Optimizes over calibrated classiﬁer score outputs to ﬁnd probabilities  with  which  to  change  output  labels  with  an  equalized odds objective. •Reject Option Classiﬁcation: Gives favorable outcomes to unprivileged groups and unfavorable outcomes to privileged groups in a conﬁdence band aroundthe decision boundary with the highest uncertainty.',\n",
       "  '137Bias Mitigation',\n",
       "  '138Bias Mitigation',\n",
       "  '139Bias Mitigation\\nhttp://aif360.mybluemix.net/',\n",
       "  '140ReweightingSampling, massaging, reweighing and suppression are among different pre-processing bias mitigation techniques proposed from academic literature. The advantage of reweighting is, instead of modifying the labels, it assigns different weights to the examples based upon their categories of protected attribute and outcome such that bias is removed from the training dataset. The weights are based on frequency counts.',\n",
       "  '141ReweightingReweighting works by postulating that a fair data set  would show no conditional dependence of the outcome on a protected attribute.  Hence, it postulates  group membership and outcome should be statistically independent.   Reweighting adjusts the data point weights to make this so.DP(Y=a,X∈G)=P(Y=a)P(X∈G)=|{Y=a}||D|×|{X∈G}||D|',\n",
       "  '142Reweighting: Adult datasetThe binary target in our example is whether an individual has an income higher or lower than $50k.  It contains several features that are protected by the law, but for simplicity, we will focus on sex.  As can be seen in the table, Male is the privileged group with a 31% probability of having a positive outcome (>$50k) compared to an 11% probability of having a positive outcome for the Female group.\\n+p−p−np+npNegative non privilegedPositive non privilegedNegative privilegedPositive privileged',\n",
       "  '143Reweighting: Adult datasetUsing the frequency counts in the table, the reweighing technique will assign weights as follows:w+p=np×n+n×n+pw−p=np×n−n×n−pw−np=nnp×n−n×n−npw+np=nnp×n+n×n+np',\n",
       "  '144Assignment:  Recividism Analysis in CataloniaThe dataset corresponds to a set of juvenile offenders in Catalonia who were evaluated using SAVRY, a structured risk assessment tool. The data on recidivism indicates if the same people committed a new offence in 2013-2015. Objectives of the assignment: •To compare the performance of SAVRY and ML-based methods, in terms of both accuracy and fairness metrics.  •To analyze the causes of unfairness. •To explore a mitigation strategy.',\n",
       "  '145Equalized Base RatesLet’s suppose we have a binary decision problem  and my protected feature is . My dataset  is: I have Equal Base Rates if  and , which is not the case.  In this case I need to oversample class  in order to get  additional samples! Two of these samples will be oversampled from the positive pool. The other one must be sampled from the negative pool. The result is a new dataset : Now  and  .Y∈{−1,1}X∈{A,B}DPD(A)=PD(B)PD′ (Y|X)=PD(Y|X)B6−3=3D′ PD′ (A)=PD′ (B)PD′ (Y|X)=PD(Y|X)DatasetClass A  | PositiveClass B  | PositiveClass A  | NegativeClass B  | Negative\\nDatasetrandomly resample the training dataset'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.get(where={'field' : 'Theory'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "This piece is the search engine, and will look for data similar to the user query in our database. We can select how many references we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also add filters to the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs ={\"filter\":{\"field\":\"Theory\"},\"k\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA chain\n",
    "Finally, we will put all of this together to create a QA chain. It will take the users query and return an answer using the provided knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "\n",
    "#try qa with sources\n",
    "qa_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=model, \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afred\\miniconda3\\envs\\chat-edelvives\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Why is ethics important in AI?',\n",
       " 'result': 'Ethics is important in AI to ensure that the development and deployment of AI technologies are done in a responsible and considerate manner. It helps in challenging the status quo, identifying deficits and blind spots, and ensuring that AI systems are transparent, fair, and accountable. Ethical principles also guide companies in protecting privacy, avoiding harm, and complying with regulations, ultimately contributing to the well-being of employees, customers, partners, and communities affected by AI technologies.',\n",
       " 'source_documents': [Document(page_content='27Why Ethics?   in technology, data science, AI…', metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'page': 26, 'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'}),\n",
       "  Document(page_content='30In an ideal world, our ethical beliefs shape law and moral systems. We need a toolkit to run our reﬂections!The role of ethics is not to be a soft version of the law, even if laws are based on ethical principles.  The  real  application  of  ethics  lies  in  challenging the status quo, seeking its deﬁcits  and blind spots. N.Kluge Corrêa, Good AI for the Present of Humanity. Democratizing AI Governance How do we take decisions?', metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf', 'page': 29, 'source': '.\\\\documents\\\\EDS1 Ethical Foundations.pdf'}),\n",
       "  Document(page_content=\"employees, customers, partners, and potentially affected communities, to gather insights and perspectives on the ethical use of AI. This inclusive approach helps ensure that the company's AI values are considerate of diverse viewpoints and concerns. •Ethical Standards and Principles: Many companies adopt ethical frameworks or principles speciﬁc to AI. These often include commitments to transparency, fairness, accountability, privacy, and ensuring that AI technologies do not cause harm. These principles guide the development and deployment of AI systems. •Regulatory and Industry Standards Compliance: Companies also consider existing and anticipated regulations governing AI in their jurisdictions, as well as industry best practices and standards. This helps ensure that their AI values and practices are not only ethical but also legally compliant.\", metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'page': 8, 'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full example\n",
    "query = \"Why is ethics important in AI?\"\n",
    "llm_response = qa_chain(query)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What do elephants eat?',\n",
       " 'answer': \"I don't know.\\n\",\n",
       " 'sources': '',\n",
       " 'source_documents': [Document(page_content='2', metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf', 'page': 1, 'source': '.\\\\documents\\\\EDS2 Legitimacy, values and decisions.pdf'}),\n",
       "  Document(page_content='The human factor', metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf', 'page': 64, 'source': '.\\\\documents\\\\EDS4 Bias and Fairness I.pdf'}),\n",
       "  Document(page_content='36', metadata={'field': 'Theory', 'link': 'https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf', 'page': 35, 'source': '.\\\\documents\\\\EDS0 Introduction Data Science in Context.pdf'})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full example\n",
    "query = \"What do elephants eat?\"\n",
    "llm_response = qa_sources(query)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prompt Retrievers\n",
    "In some cases, we would like to modify the prompt of the QA chain. We will modify the prompt so it answers even though there is no provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "template = \"\"\"\n",
    "Answer the question based on the information from the context. If there's no information in the context, answer the question, but you must notify that the information is not in the documentation. Furthermore mark 'yes' or 'no' between the tags (at the end) depending on wether there is or not information.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:<answer>\n",
    "\n",
    "<INFO>yes/no<INFO>\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(model,\n",
    "                                       verbose=False,\n",
    "                                       # retriever=vectordb.as_retriever(),\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use regular expressions to process our final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_tag(text):\n",
    "    \"\"\"\n",
    "    Extracts if there is INFO/NOINFO tag\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to find text between <Answer> and </Answer>\n",
    "    pattern = \"<INFO>(.*?)<INFO>\"  \n",
    "    # Use re.findall to find all occurrences that match the pattern\n",
    "    matches = re.findall(pattern, text) \n",
    "    return matches[0].lower()\n",
    "\n",
    "\n",
    "def remove_last_line(text):\n",
    "    # Split the string into a list of lines\n",
    "    lines = text.split('\\n')\n",
    "    # Remove the last line\n",
    "    lines = lines[:-1]\n",
    "    # Join the list back into a string\n",
    "    modified_string = '\\n'.join(lines)\n",
    "    return(modified_string)\n",
    "\n",
    "\n",
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    text = llm_response['result']\n",
    "    try:\n",
    "        # Assuming extract_tag and remove_last_line are defined elsewhere\n",
    "        info = extract_tag(text)\n",
    "        text = remove_last_line(text)\n",
    "    except Exception as e:\n",
    "        info = 'no'\n",
    "    if info == 'yes':\n",
    "        text += '\\n\\nFurther information in:'\n",
    "    else:\n",
    "        text += '\\n\\nMay be useful:'\n",
    "\n",
    "    # Use a dictionary to group pages by document\n",
    "    documents = {}\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        source_name = source.metadata['source']\n",
    "        page = source.metadata['page']\n",
    "        link = source.metadata['link']\n",
    "        if source_name not in documents:\n",
    "            documents[source_name] = {'pages': [page], 'link': link}\n",
    "        else:\n",
    "            if page not in documents[source_name]['pages']:\n",
    "                documents[source_name]['pages'].append(page)\n",
    "\n",
    "        # Append the aggregated information to the text\n",
    "    for source_name, info in documents.items():\n",
    "        pages_text = ', '.join(str(page) for page in info['pages'])\n",
    "        text += f'\\n**Document:** {source_name}, **Pages** {pages_text} \\n{info[\"link\"]}'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, now the LLM will answer question outside of the scope of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethics is important in AI to challenge the status quo, identify deficits and blind spots, gather insights and perspectives from various stakeholders, ensure transparency, fairness, accountability, privacy, and prevent harm caused by AI technologies. It also helps companies comply with existing and anticipated regulations governing AI and industry best practices. \n",
      "\n",
      "\n",
      "Further information in:\n",
      "**Document:** .\\documents\\EDS0 Introduction Data Science in Context.pdf, **Pages** 26 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf\n",
      "**Document:** .\\documents\\EDS1 Ethical Foundations.pdf, **Pages** 29 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS1%20Ethical%20Foundations.pdf\n",
      "**Document:** .\\documents\\EDS2 Legitimacy, values and decisions.pdf, **Pages** 8 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf\n"
     ]
    }
   ],
   "source": [
    "resp = qa_chain.invoke(\"Why is ethics important in AI?\")\n",
    "print(process_llm_response(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information about what elephants eat is not provided in the context. \n",
      "\n",
      "\n",
      "May be useful:\n",
      "**Document:** .\\documents\\EDS2 Legitimacy, values and decisions.pdf, **Pages** 1 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS2%20Legitimacy%2C%20values%20and%20decisions.pdf\n",
      "**Document:** .\\documents\\EDS4 Bias and Fairness I.pdf, **Pages** 64 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS4%20Bias%20and%20Fairness%20I.pdf\n",
      "**Document:** .\\documents\\EDS0 Introduction Data Science in Context.pdf, **Pages** 35 \n",
      "https://github.com/arturofredes/Ethical_AI_RAG/blob/main/documents/EDS0%20Introduction%20Data%20Science%20in%20Context.pdf\n"
     ]
    }
   ],
   "source": [
    "resp = qa_chain.invoke(\"What do elephants eat?\")\n",
    "print(process_llm_response(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat history\n",
    "In conversation it may be important to leverage previous user messages to get a better retrieval. First, we will see an example on how to create a conversational chatbo using langchain, and the next step would be to create an agent that synthesises all previous interactions into a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola me llamo arturo\n",
      "¡Hola, Arturo! ¿Cómo puedo asistirte hoy?\n",
      "Me gusta la escalada y me gustaria que me dieras consejos para mejorar\n",
      "¡Genial, Arturo! Me alegra que te guste la escalada. Aquí te dejo algunos consejos que podrían ayudarte a mejorar:\n",
      "\n",
      "1. **Fortalecimiento de la fuerza de agarre**: Tu capacidad para sostener y mover tu cuerpo depende en gran medida de la fuerza de tu agarre. Considera ejercicios de fortalecimiento de la mano y el antebrazo.\n",
      "\n",
      "2. **Mejora tu técnica de pies**: Muchas personas se centran en sus manos y brazos cuando escalan, pero tus pies también juegan un papel importante. Trabaja en la colocación precisa de los pies y el equilibrio.\n",
      "\n",
      "3. **Acondicionamiento físico general**: La escalada es un deporte de todo el cuerpo que requiere más que solo fuerza en la parte superior del cuerpo. El entrenamiento de la fuerza central, la flexibilidad y el acondicionamiento cardiovascular pueden ser de gran ayuda.\n",
      "\n",
      "4. **Practica la visualización**: Antes de hacer un movimiento o una ruta, visualízalo primero. Esto puede ayudarte a prepararte mentalmente y trazar una ruta.\n",
      "\n",
      "5. **Descansa adecuadamente**: La recuperación es una parte importante de cualquier régimen de entrenamiento. Asegúrate de descansar lo suficiente entre las sesiones de escalada.\n",
      "\n",
      "6. **No te olvides de estirar**: Es importante estirar antes y después de cada sesión de escalada para prevenir lesiones y mantener la flexibilidad.\n",
      "\n",
      "7. **Mantén una alimentación equilibrada**: La nutrición es clave para mantener tu energía y ayudar a tu cuerpo a recuperarse después de la escalada. \n",
      "\n",
      "Recuerda siempre que la seguridad es lo más importante. Asegúrate de tener el equipo adecuado y de conocer las técnicas de seguridad. Y, por supuesto, ¡diviértete mientras escalas!\n",
      "exit\n",
      "¡Adiós, Arturo! No dudes en volver si tienes más preguntas. ¡Disfruta de tu escalada!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a helpful assistant\"),\n",
    "MessagesPlaceholder(variable_name='chat_history'),\n",
    "(\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "chat_chain = LLMChain(llm=model, prompt=prompt)\n",
    "chat_hist=[]\n",
    "user_input = ''\n",
    "\n",
    "while user_input != 'exit':\n",
    "    user_input = input()\n",
    "    print(user_input)\n",
    "    response = chat_chain.invoke({'question':user_input, \"chat_history\":chat_hist})\n",
    "    print(response['text'])\n",
    "    chat_hist.append(HumanMessage(user_input))\n",
    "    chat_hist.append(AIMessage(response['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to apply this to our RAG system. For that, I will ask the LLM to reformulate the question given a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is utilitarism?\n",
      "Could you please provide an overview of utilitarianism and its key principles?\n",
      "Utilitarianism is a theory that focuses on whether an action maximizes happiness and well-being for all affected individuals. The key principle of utilitarianism is to aim for the greatest good for the greatest number of people. Utilitarian calculus allows for the possibility of sacrificing some individuals for the greater good, as long as it benefits society as a whole.\n",
      "How can it be applied to ethics in AI?\n",
      "To what extent can utilitarianism be utilized in ethical considerations within the field of artificial intelligence (AI)?\n",
      "Utilitarianism can be utilized in ethical considerations within the field of artificial intelligence (AI) to the extent that it focuses on maximizing overall well-being and minimizing harm for the greatest number of people. \n",
      "\n",
      "<INFO>yes<INFO>\n",
      "exit\n",
      "If you have any more questions in the future, feel free to ask. Goodbye!\n",
      "There is no information provided in the context to answer the question. \n",
      "\n",
      "<INFO>no<INFO>\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "reformulating_prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"As an assistant within a Retrieval-Augmented Generation (RAG) system, your role is to interpret the conversation with the user and formulate it into a succinct question. This question should accurately capture the user's intent, leveraging specific keywords to ensure that the system's response aligns closely with what the user is seeking. It's crucial to maintain a high level of semantic similarity between the user's request and your question to the system. This approach helps in retrieving the most relevant information or answer from the database, enhancing the user experience.\"),\n",
    "MessagesPlaceholder(variable_name='chat_history'),\n",
    "(\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "chat_chain = LLMChain(llm=model, prompt=reformulating_prompt)\n",
    "chat_hist=[]\n",
    "user_input = ''\n",
    "\n",
    "while user_input != 'exit':\n",
    "    user_input = input()\n",
    "    chat_hist.append(HumanMessage(user_input))\n",
    "    print(user_input)\n",
    "    refor = chat_chain.invoke({'question':user_input, \"chat_history\":chat_hist})\n",
    "    print(refor['text'])\n",
    "    resp = qa_chain.invoke(refor['text'])['result']\n",
    "    print(resp)\n",
    "    chat_hist.append(AIMessage(resp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New splitting and retrieving strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent document retriever\n",
    "- It is possible that inside one of the chunks we talk about different things. This results in more generic embeddings and can cause problems when looking for very specific piece of information.\n",
    "- On the other hand, if we split the text into chunks too small, the LLM will not have enough context to answer the question.\n",
    "\n",
    "One way of going around this is using Parent document retriever. This allows us to use small chunks sucha as sentences for embeddings (getting better accuracy), while returning a bigger window of context to which that sentence belongs (getting a better answer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def extract_folder(doc):\n",
    "    path = Path(doc.metadata['source'])\n",
    "    return path.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def extract_folder(doc):\n",
    "    path = Path(doc.metadata['source'])\n",
    "    return path.parent.name\n",
    "#load multiple documents\n",
    "loaders = [\n",
    "PyPDFLoader(\".\\documents\\desarrollos\\Documento funcional icecream 2023 v0.3.pdf\"),\n",
    "PyPDFLoader(\".\\documents\\desarrollos\\Derivación de pedidos.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load_and_split())\n",
    "\n",
    "#añadir metadata departamento\n",
    "for doc in docs:\n",
    "    dep = extract_folder(doc)\n",
    "    doc.metadata['departamento'] = dep\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the databases and retriever\n",
    "\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.storage.file_system import LocalFileStore\n",
    "fs = LocalFileStore(\"./docdb_edelvives\")\n",
    "store = create_kv_docstore(fs)\n",
    "#parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=embeddings, persist_directory=\"db_edelvives\")\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    #parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComo funciona el envío y compra?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mretrieved_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpage_content\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"Como funciona el envío y compra?\")\n",
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the retriever\n",
    "persist_directory = 'db_edelvives'\n",
    "vectorstore = Chroma(persist_directory=persist_directory, \n",
    "                embedding_function=embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    #parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfQueryingRetrieval\n",
    "Did not work very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Nombre del documento original\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"página\",\n",
    "        type=\"integer\",\n",
    "    ),    \n",
    "    AttributeInfo(\n",
    "        name=\"area\",\n",
    "        description=\"departamento/area de la empresa que usa este documento\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"link\",\n",
    "        description=\"link al documento\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"descrpcion\",\n",
    "        description=\"Descripcion del documento original\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Chunk of text from a document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    model,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a relevant query\n",
    "retriever.get_relevant_documents(\"Quiero documentos del area de desarrollo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
